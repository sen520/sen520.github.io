<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[excel相关操作]]></title>
    <url>%2F2020%2F06%2F17%2Fpython%2F03.%E8%AF%BB%E5%8F%96excel%2F</url>
    <content type="text"><![CDATA[关于excel，大部分操作应该主要用于数据分析，其中，最主流的方式应该是采用pandas库，这里主要介绍几种方式来读写excel读取excel数据处理一般的excel，尤其没有合并单元格之类的情况时，我们可以采用普通的方法这里采用xlrd库 无单元格合并的情况下 12345678910111213141516171819202122232425262728293031import xlrddef parse_list(data): """ parse a list :param data: list :return: """ data_list = [] for i in range(1, len(data)): data_dict = &#123;&#125; for index, key in enumerate(data[0]): data_dict[key] = data[i][index] data_list.append(data_dict) return data_listdef read_excel(filename, sheet=None): """ read excel and parse every line :param name: file of absolute path :return: some dict which is about everyline of list """ workbook = xlrd.open_workbook(filename) total_data = [] if sheet: sh = workbook.sheet_by_name(sheet) for row in range(0, sh.nrows): row_list = sh.row_values(row) total_data.append(row_list) return parse_list(total_data) 上述方法主要针对特定的sheet，当然，根据上述方法也也可进行改变，读取所有的sheet 关键方法在于read_excel，所以我们来改写该方法 1234567891011121314151617import xlrddef read_excel(filename): """ read excel and parse every line :param name: file of absolute path :return: some dict which is about everyline of list """ workbook = xlrd.open_workbook(filename) total_data = [] name_sheets = workbook.sheet_names() for sheet in name_sheets: sh = workbook.sheet_by_name(sheet) for row in range(0, sh.nrows): row_list = sh.row_values(row) total_data.append(row_list) return parse_list(total_data) 主要用于处理合并存在合并单元格的情况 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import xlrddef get_excel(): data_list = [] with xlrd.open_workbook(r'D:\0325.xlsx') as workbook: name_sheets = workbook.sheet_names() # 获取Excel的sheet表列表，存储是sheet表名 for index in name_sheets: # for 循环读取每一个sheet表的内容 apply_dic = [] sheet_info = workbook.sheet_by_name(index) # 根据表名获取表中的所有内容，sheet_info也是列表，列表中的值是每个单元格里值 first_line = sheet_info.row_values(0) # 获取首行，我这里的首行是表头，我打算用表头作为字典的key，每一行数据对应表头的value，每一行组成一个字典 values_merge_cell = merge_cell(sheet_info) # 这里是调用处理合并单元格的函数 for i in range(1, sheet_info.nrows): # 开始为组成字典准备数据 other_line = sheet_info.row_values(i) for key in values_merge_cell.keys(): if key[0] == i: other_line[key[1]] = values_merge_cell[key] # print(other_line) dic = list_dic(first_line, other_line) # 调用组合字典的函数，传入key和value，字典生成 apply_dic.append(dic) data_list.append(&#123;'index': index, 'data': apply_dic&#125;) return data_listdef list_dic(list1, list2): ''' two lists merge a dict,a list as key,other list as value :param list1:key :param list2:value :return:dict ''' dic = dict(map(lambda x, y: [x, y], list1, list2)) return dicdef merge_cell(sheet_info): ''' #handle Merge transverse cells and handle Merge Vertical Cells, assign empty cells, :param rlow:row, include row exclusive of row_range :param rhigh:row_range :param clow:col, include col exclusive of col_range :param chigh:col_range :param sheet_info:object of sheet :return:dic contain all of empty cells value ''' merge = &#123;&#125; merge_cells = sheet_info.merged_cells for (rlow, rhigh, clow, chigh) in merge_cells: value_mg_cell = sheet_info.cell_value(rlow, clow) if rhigh - rlow == 1: # Merge transverse cells for n in range(chigh - clow - 1): merge[(rlow, clow + n + 1)] = value_mg_cell elif chigh - clow == 1: # Merge Vertical Cells for n in range(rhigh - rlow - 1): merge[(rlow + n + 1, clow)] = value_mg_cell return mergeif __name__ == '__main__': get_excel() 写excel 生成xlsx，使用pandas 12345678import pandas as pddef write_excel(data, filename, keys=''): data_df = pd.DataFrame(data) if keys != '': data_df.columns = list(keys) writer = pd.ExcelWriter('&#123;&#125;.xlsx'.format(filename)) data_df.to_excel(writer, index=None) writer.save() 生成csv 123456789import csvdef write_csv(name, data_list): fieldnames = data_list[0] print(fieldnames) with open(name + '.csv', mode='w', newline='', encoding='utf-8-sig') as csv_file: writer = csv.DictWriter(csv_file, fieldnames=fieldnames) writer.writeheader() for data in data_list: writer.writerow(data) 当然，可以通过json去生成csv 12345import jsondef json_to_csv(name): with open(name) as f: a = json.loads(f.read()) write_csv(name, a) 这里依旧可以使用pandas 123456789101112131415161718def data_to_csv(data_list, csv_key, name): """ save data to csv :param data_list: data :param csv_key: csv column :param name: file name :return: """ final_data = [] for data in data_list: sign_key = [] for key, value in data.items(): sign_key.append(value) final_data.append(sign_key) # 将总数据转化为data frame再输出 df = pd.DataFrame(data=final_data, columns=csv_key) df.to_csv(name + '.csv', index=False, encoding='utf-8_sig')]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yeild]]></title>
    <url>%2F2020%2F03%2F26%2Fpython%2Fspider%2F01.yield%2F</url>
    <content type="text"><![CDATA[yeild之前在scrapy有所接触，但并没有深挖其意义。今天在读一篇文章的时候，发现其在scrapy外使用了yeild，所以就简单查了一下 yeild可以先把yeild当做return来使用，但是返回值是一个generator，可以根据下面的代码来理解 1234567891011121314151617def foo(): print("starting...") while True: res = yield 4 print("res:",res)g = foo()print(next(g))print("*"*20)print(next(g))# 执行结果"""starting...4********************res: None4""" 理解 程序开始执行，因为foo函数中含有yield关键字，所以foo函数并不会真的执行，而是先得到一个生成器（相当于一个对象） 直到调用next方法，foo函数才开始执行，先执行foo中的print，然后进入循环 遇到了yield后，先return了一个4,之后程序停止 下面再次执行next(g)，这个时候是从刚才next程序停止的 地方开始执行的所以res是None 然后同上 123456789101112131415161718192021222324def foo(num): print("starting...") while num&lt;10: num=num+1 yield numprint(foo(1))for n in foo(0): print(n) # 执行结果"""&lt;generator object foo at 0x0000020899ACCBA0&gt;starting...12345678910""" 简单爬虫小例子12345678910111213141516171819202122232425262728293031323334353637383940414243444546import requestsimport loggingfrom pyquery import PyQuery as pqfrom urllib.parse import urljoinlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s:%(message)s')TOTAL_PAGE = 10BASE_URL = 'https://static1.scrape.cuiqingcai.com'def parse_index(html): doc = pq(html) links = doc('.el-card .name') for link in links.items(): href = link.attr('href') detail_url = urljoin(BASE_URL, href) logging.info('get detail url %s', detail_url) yield detail_urldef scrape_index(page): index_url = f'&#123;BASE_URL&#125;/page/&#123;page&#125;' return scrape_page(index_url)def scrape_page(url): logging.info('scraping %s...', url) try: response = requests.get(url) if response.status_code == 200: return response.text logging.error('get invalid status code %s while scraping %s', response.status_code) except requests.RequestException: logging.error('error occurred while scraping %s', url, exc_info=True)def main(): for page in range(1, TOTAL_PAGE + 1): index_html = scrape_index(page) detail_urls = parse_index(index_html) logging.info('detail urls %s', list(detail_urls))if __name__ == '__main__': main()]]></content>
      <categories>
        <category>python</category>
        <category>spider</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows关闭系统线程，进程]]></title>
    <url>%2F2020%2F03%2F06%2Fpython%2F02.%E5%85%B3%E9%97%AD%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[转载：python 中杀进程 , 线程 ,杀端口 背景目前使用一个程序开启一个web server线程，但是使用thread.join()不能使这个web server结束。故有以下调查 杀线程12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import threadingimport timeimport inspectimport ctypes def _async_raise(tid, exctype): """raises the exception, performs cleanup if needed""" tid = ctypes.c_long(tid) if not inspect.isclass(exctype): exctype = type(exctype) res = ctypes.pythonapi.PyThreadState_SetAsyncExc(tid, ctypes.py_object(exctype)) if res == 0: raise ValueError("invalid thread id") elif res != 1: # """if it returns a number greater than one, you're in trouble, # and you should call it again with exc=NULL to revert the effect""" ctypes.pythonapi.PyThreadState_SetAsyncExc(tid, None) raise SystemError("PyThreadState_SetAsyncExc failed") def stop_thread(thread): _async_raise(thread.ident, SystemExit) class testClass(threading.Thread): def __init__(self,abc): super().__init__() self.out=abc def run(self): while 1: print(self.out) def print_time(): while 1: print('234') if __name__ == "__main__": t = threading.Thread(target=print_time) # t = testClass('123') t.daemon=True t.start() time.sleep(0.5) print(t.is_alive()) stop_thread(t) time.sleep(0.5) print(t.is_alive()) print(t.is_alive()) print(t.is_alive()) print("stoped") while 1: pass 根据端口号关闭进程此处如果根据web server的端口来杀线程，并不能结束web server而保留原进程。而是把整个程序杀掉 12345678910111213141516def killport(port): """ 按端口号杀进程 :param port: :return: """ # 查找端口的pid find_port = 'netstat -aon | findstr %s' % str(port) result = os.popen(find_port) text = result.read() pid = text.strip().split(' ')[-1] # 占用端口的pid find_kill = 'taskkill -f -pid %s' % pid print(find_kill) result = os.popen(find_kill) return result.read()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python录制系统声音]]></title>
    <url>%2F2020%2F02%2F26%2Fpython%2F01.%E5%BD%95%E9%9F%B3%2F</url>
    <content type="text"><![CDATA[转载：python开发的录音机（一）录制声卡播放的声音(内录)环境准备python wave pyaudio wave 可以通过pip直接install，在安装pyaudio时，通过正常的pip install 直接安装一直处于报错阶段，后来想到可以通过轮子直接安装。在pypi提供的安装包中有对应的安装包，注意，不仅仅是python2和python3的区别，python3的小版本也有点差别。可杯具的是，小主电脑里装的是python3.8，后来想到还有一个网站可以安装pythonlibs，找到对应的版本后，下载下来。直接在文件所在目录，或者在安装中指定文件目录中执行安装 pip install /c/Users/root/Downloads/PyAudio-0.2.11-cp38-cp38-win_amd64.whl 代码和运行1234567891011121314151617181920212223242526272829303132def audio_record(out_file, rec_time): CHUNK = 1024 FORMAT = pyaudio.paInt16 # 16bit编码格式 CHANNELS = 1 # 单声道 RATE = 16000 # 16000采样频率 p = pyaudio.PyAudio() # 创建音频流 dev_idx = findInternalRecordingDevice(p) stream = p.open(format=FORMAT, # 音频流wav格式 channels=CHANNELS, # 单声道 rate=RATE, # 采样率16000 input=True, input_device_index=dev_idx, # 指定内录设备的id，可以不写，使用win的默认录音设备 frames_per_buffer=CHUNK) print("Start Recording...") frames = [] # 录制的音频流 # 录制音频数据 for i in range(0, int(RATE / CHUNK * rec_time)): # 控制录音时间 data = stream.read(CHUNK) frames.append(data) # 录制完成 stream.stop_stream() stream.close() p.terminate() print("Recording Done...") # 保存音频文件 wf = wave.open(out_file, 'wb') wf.setnchannels(CHANNELS) wf.setsampwidth(p.get_sample_size(FORMAT)) wf.setframerate(RATE) wf.writeframes(b''.join(frames)) wf.close() 在使用默认录音设备时，发现是话筒录音，效果并不是太理想，所以就去查查能不能直接录系统的声音。 123456789101112def findInternalRecordingDevice(p): # 要找查的设备名称中的关键字 target = '立体声混音' # 逐一查找声音设备 for i in range(p.get_device_count()): devInfo = p.get_device_info_by_index(i) print(devInfo) if devInfo['name'].find(target) &gt;= 0 and devInfo['hostApi'] == 0: # print('已找到内录设备,序号是 ',i) return i print('无法找到内录设备!') return -1 可以使用p.get_device_info_by_index()去查看系统有关声音的设备，通过设置为立体声混音就可以录制系统声音。 保存声音123456789101112131415def save(fileName): # 创建pyAudio对象 p = pyaudio.PyAudio() # 打开用于保存数据的文件 wf = wave.open(fileName, 'wb') # 设置音频参数 wf.setnchannels(CHANNELS) wf.setsampwidth(p.get_sample_size(FORMAT)) wf.setframerate(RATE) # 写入数据 wf.writeframes(b''.join(_frames)) # 关闭文件 wf.close() # 结束pyaudio p.terminate() 保存声音是通过上述代码进行保存，此处的_frames是个list，是通过每录一个chunk（数据流块），就把这一块的数据添加进去 然后只需要重新创建PyAudio对象，把这个list转为字节串保存到文件中就可以了 问题上述一般可以录到系统声音，但在执行的时候发现，并不能。 原因是：win的输入设备中没有配置立体声混音 设置步骤： 在win的声音调节出，右击打开声音设置 找到管理声音设备 在输入设备处启用立体声混音 就此，就完成了录制系统声音的需求 注意上述操作，可以外放，可以插入3.5mm耳机，但系统静音和tpye-c耳机插入的时候不能录到声音 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117import osimport pyaudioimport threadingimport waveimport timefrom datetime import datetime# 需要系统打开立体声混音# 录音类class Recorder(): def __init__(self, chunk=1024, channels=2, rate=44100): self.CHUNK = chunk self.FORMAT = pyaudio.paInt16 self.CHANNELS = channels self.RATE = rate self._running = True self._frames = [] # 获取内录设备序号,在windows操作系统上测试通过，hostAPI = 0 表明是MME设备 def findInternalRecordingDevice(self, p): # 要找查的设备名称中的关键字 target = '立体声混音' # 逐一查找声音设备 for i in range(p.get_device_count()): devInfo = p.get_device_info_by_index(i) # print(devInfo) if devInfo['name'].find(target) &gt;= 0 and devInfo['hostApi'] == 0: # print('已找到内录设备,序号是 ',i) return i print('无法找到内录设备!') return -1 # 开始录音，开启一个新线程进行录音操作 def start(self): threading._start_new_thread(self.__record, ()) # 执行录音的线程函数 def __record(self): self._running = True self._frames = [] p = pyaudio.PyAudio() # 查找内录设备 dev_idx = self.findInternalRecordingDevice(p) if dev_idx &lt; 0: return # 在打开输入流时指定输入设备 stream = p.open(input_device_index=dev_idx, format=self.FORMAT, channels=self.CHANNELS, rate=self.RATE, input=True, frames_per_buffer=self.CHUNK) # 循环读取输入流 while (self._running): data = stream.read(self.CHUNK) self._frames.append(data) # 停止读取输入流 stream.stop_stream() # 关闭输入流 stream.close() # 结束pyaudio p.terminate() return # 停止录音 def stop(self): self._running = False # 保存到文件 def save(self, fileName): # 创建pyAudio对象 p = pyaudio.PyAudio() # 打开用于保存数据的文件 wf = wave.open(fileName, 'wb') # 设置音频参数 wf.setnchannels(self.CHANNELS) wf.setsampwidth(p.get_sample_size(self.FORMAT)) wf.setframerate(self.RATE) # 写入数据 wf.writeframes(b''.join(self._frames)) # 关闭文件 wf.close() # 结束pyaudio p.terminate()if __name__ == "__main__": # 检测当前目录下是否有record子目录 if not os.path.exists('record'): os.makedirs('record') print("\npython 录音机 ....\n") print("提示：按 r 键并回车 开始录音\n") i = input('请输入操作码:') if i == 'r': rec = Recorder() begin = time.time() print("\n开始录音,按 s 键并回车 停止录音，自动保存到 record 子目录\n") rec.start() running = True while running: i = input("请输入操作码:") if i == 's': running = False print("录音已停止") rec.stop() t = time.time() - begin print('录音时间为%ds' % t) # 以当前时间为关键字保存wav文件 rec.save("record/rec_" + datetime.now().strftime("%Y-%m-%d_%H-%M-%S") + ".wav")]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ES相关配置]]></title>
    <url>%2F2020%2F01%2F08%2Fes%2FES%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1.设置最大返回条数在ElasticSearch中一般做分页查询，通过from和size进行实现，from指定从哪一行开始，size指定一次读取多少。但是有时候需要返回较多的数据，并不知道具体的数量，所以需要将size设定的足够大，但是ES最大查询结果只能到10000，这个是因为es的配置中的index.max_result_window的最大默认值是10000。 需要修改该值（或添加） 首先查询配置项(这里我的index为platform)： 123456789101112131415161718GET /127.0.0.1:9200/platform/_settings&#123; "platform": &#123; "settings": &#123; "index": &#123; "number_of_shards": "5", "provided_name": "platform-alpha", "creation_date": "1578477702787", "number_of_replicas": "1", "uuid": "gG9VqLXVQsG9dV3e9CATKA", "version": &#123; "created": "5061699" &#125; &#125; &#125; &#125;&#125; 可以看到并没有这个配置项，所以我们需要添加该配置 123456PUT /127.0.0.1:9200/platform/_settings&#123; "index": &#123; "max_result_window": 2147483647 &#125;&#125; 注意：ES支持最大返回数目是2 ^ 31 - 1，也就是2147483647]]></content>
      <categories>
        <category>ES</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>ES</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql游标]]></title>
    <url>%2F2019%2F12%2F25%2Fdb%2Fmysql%2F09%E3%80%81%E6%B8%B8%E6%A0%87%2F</url>
    <content type="text"><![CDATA[使用游标 在能够使用游标前，必须先声明（定义）它，这个过程实际上没有检索数据，只是定义要使用的SELECT语句 一旦声明后，必须打开游标以供使用。这个过程用前面定义的SELECT语句把数据实际检索出来 对于填有数据的游标，根据需要取出（索引）各行 在结束游标使用时，必须关闭游标 创建游标游标用DECLARE语句创建，DECLARE命名游标，并定义相应的SELECT语句，根据需要带WHERE和其他子句。 下面的语句定义了名为ordernumbers的游标，使用了可以检索所有订单的SELECT语句。 123456CREATE PROCEDURE processorders()BEGIN DECLARE ordernumbers CURSOR FOR SELECT order_num FROM orders;END; 打开和关闭游标打开游标 1OPEN ordernumbers; 关闭游标 1CLOSE ordernumbers; 隐含关闭 如果你不明确关闭游标，MySQL将会在到达END语句时自动关闭它。 12345678910111213CREATE PROCEDURE processorders()BEGIN -- Declare the cursor DECLARE ordernumbers CURSOR FOR SELECT order_num FROM orders; -- Open the cursor OPEN ordernumbers; -- Close the cursor CLOSE ordernumbers;END; 使用游标数据在一个游标被打开后，可以使用FETCH语句分别访问它的每一行。FETCH指定检索什么数据（所需的列），检索出来的数据存储在什么地方。它还向前移动游标中的内部行指针，使用下一条FETCH语句检索下一行（不重复读取同一行）。 12345678910111213141516171819CREATE PROCEDURE processorders()BEGIN -- Declare local variables DECLARE o INT; -- Declare the cursor DECLARE ordernumbers CURSOR FOR SELECT order_num FROM orders; -- Open the cursor OPEN ordernumbers; -- Get order number FETCH ordernumbers INTO o; -- 检索当前行的order_num列（将自动从第一行开始）到一个名为o的局部声明的变量中。 -- Close the cursor CLOSE ordernumbers;END; 下面一个例子，循环检索数据，从第一行到最后一行 12345678910111213141516171819202122232425CREATE PROCEDURE processorders()BEGIN -- Declare local variables DECLARE done BOOLEAN DEFAULT 0; DECLARE o INT; -- Declare the cursor DECLARE ordernumbers CURSOR FOR SELECT order_num FROM orders; -- Declare continue handler DECLARE CONTINUE HANDLER FOR SQLSTATE '02000' SET done=1; -- Open the cursor OPEN ordernumbers; -- Loop through all rows REPEAT -- Get order number FETCH ordernumbers INTO o; -- End of loop UNTIL done END REPEAT; -- Close the cursor CLOSE ordernumbers;END; 这个例子中的FETCH是在REPEAT内，因此它反复执行直到done为真（由UNTIL done END REPEAT;规定）。为使它起作用，用一个DEFAULT 0（假，不结束）定义变量done。 通过这个语句将done设为1 1DECLARE CONTINUE HANDLER FOR SQLSTATE &apos;02000&apos; SET done=1; 指当SQLSTATE &#39;02000&#39;出现时，SET done=1。这里SQLSTATE &#39;02000&#39;是一个未找到条件，当REPEAT由于没有更多行供循环而不能继续时，出现这个条件。 如果调用这个存储过程，它将定义几个变量和一个CONTINUE HANDLER，定义并打开一个游标，重复读取所有行，然后关闭游标。 DECLARE语句的次序，DECLARE语句发布存在特定的次序。用DECLARE语句定义的局部变量必须在定义任意游标或句柄之前定义。 1234567891011121314151617181920212223242526272829303132333435CREATE PROCEDURE processorders()BEGIN -- Declare local variables DECLARE done BOOLEAN DEFAULT 0; DECLARE o INT; DECLARE t DECIMAL(8, 2); -- Declare the cursor DECLARE ordernumbers CURSOR FOR SELECT order_num FROM orders; -- Declare continue handler DECLARE CONTINUE HANDLER FOR SQLSTATE '02000' SET done=1; -- Create a table to store the results CREATE TABLE IF NOT EXISTS ordertotals(order_num INT, total DECIMAL(8, 2)); -- Open the cursor OPEN ordernumbers; -- Loop through all rows REPEAT -- Get order number FETCH ordernumbers INTO O; -- Get the total for this order CALL ordertotal(o, 1, t) -- Insert order and total into ordertotals INSERT INTO ordertotals(order_num, total) VALUES(o, t); -- End of loop UNTIL done END REPEAT; -- Close the cursor CLOSE ordernumbers;END; 这个例子中，增加了另一个名为t的变量，存储每个订单的合计。此存储过程还在运行中创建了一个新表（如果它不存在的话），名为ordertotals。这个表将保存存储过程生成的结果。FETCH像之前一样，取每个order_num，然后用CALL 执行另一个存储过程（在存储过程中创建的）来计算每个订单的带税合计，结果存储到t，最后用INSERT保存每个订单的订单号和合计。 1SELECT * FROM ordertotals;]]></content>
      <categories>
        <category>数据库</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>数据库</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql触发器]]></title>
    <url>%2F2019%2F12%2F25%2Fdb%2Fmysql%2F10%E3%80%81%E8%A7%A6%E5%8F%91%E5%99%A8%2F</url>
    <content type="text"><![CDATA[MySQL语句在需要时被执行，存储过程也是。但如果想要某条语句在事件发生时自动执行，怎么办呢？ 每当增加一个顾客到某个数据库表时，都检查其电话号码的格式是否正确，州的缩写是否为大写 每当订购一个产品时，都从库存数量中减去订购的数量 无论何时删除一行，都在某个存档表中保留一个副本 这些都需要在某个表发生更改时自动处理，就是触发器。 触发器是MySQL响应一下任意语句而自动执行的一条MySQL语句 DELETE INSERT UPDATE 创建触发器需要给出4条信息 唯一的触发器名 触发器关联表 触发器应该响应的活动(DELETE, INSERT, UPDATE) 触发器何时执行 1CREATE TRIGGER newproduct AFTER INSERT ON products FOR EACH ROW SELECT 'Product added'; CREATE TRIGGER用来创建名为newproduct的新触发器。触发器可在一个操作发生前或之后执行。这里给出了AFTER INSERT，所以此触发器将在INSERT语句成功执行后执行。这个触发器还指定FOR EACH ROW ，因此代码对每个插入行执行。这个例子中，文本Product added 将对每个插入的行显示一次。 只有表才支持触发器，视图不支持 触发器按每个表每个事件每次地定义，每个表每个事件每次只允许一个触发器。因此每个表最多支持6个触发器（每条INSERT、UPDATE、DELETE的之前和之后）。单一触发器不能与多个事件或多个表关联。所以，如果需要一个对INSERT和UPDATE操作执行的触发器，则应该定义两个触发器。 删除触发器1DROP TRIGGER newproduct; 使用触发器INSERT触发器INSERT触发器在INSERT语句执行之前或之后执行。 在INSERT触发器代码内，可引用一个名为NEW的虚拟表，访问被插入的行 在BEFORE INSERT触发器中，NEW中的值也可以被更新（允许更改被插入的值） 对于AUTO_INCREMENT列，NEW在INSERT执行之前包含0，在INSERT执行之后包含新的自动生成值 例： 12CREATE TRIGGER neworder AFTER INSERT ON ordersFOR EACH ROW SELECT NEW.order_num; 创建了一个名为neworder的触发器，按照AFTER INSERT ON orders执行。在插入一个新订单到orders表时，MySQL生成一个新订单号并保存到order_num中。触发器从NEW.order_num取得这个值并返回它。此触发器必须按照AFTER INSERT执行，因为在BEFORE INSERT语句执行之前，新的order_num还没有生成。对于orders的每次插入使用这个触发器将总是返回新的订单号。 1INSERT INTO orders(order_date, cust_id) VALUES(Now(), 10001); orders包含3个列，order_date和cust_id必须给出，order_num由MySQL自动生成，而现在order_num还自动被返回 DELETE触发器 在DELETE触发器代码内，可以引用一个名为OLD的虚拟表，访问被删除的行 OLD中的值全都是只读的，不能更新 12345CREATE TRIGGER deleteorder BEFORE DELETE ON ordersFOR EACH ROWBEGIN INSERT INTO archive_orders(order_num, order_date, cust_id) VALUES(OLD.order_num, OLD.order_date, OLD.cust_id);END; 在任意订单被删除前，执行此触发器，将OLD中的值（要被删除的订单）保存到一个名为archive_orders的存档表中 UPDATE触发器 在UPDATE触发器代码中，你可以引用一个名为OLD的虚拟表访问以前的（UPDATE语句前）的值，引用一个名为NEW的虚拟表访问新更新的值 在BEFORE UPDATE触发器中，NEW中的值可能也被更新（允许更改将要用于UPDATE语句中的值） OLD中的值全都是只读的，不能更新 下面的例子保证州名缩写总是大写： 12CREATE TRIGGER updatevendor BEFORE UPDATE ON vendorsFOR EACH ROW SET NEW.vend_state = Upper(NEW.vend_state);]]></content>
      <categories>
        <category>数据库</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>数据库</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql存储过程]]></title>
    <url>%2F2019%2F12%2F25%2Fdb%2Fmysql%2F08%E3%80%81%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[使用存储过程MySQL称存储过程的执行为调用，因此MySQL执行存储过程的语句为CALL。CALL接受存储过程的名字以及需要传递给它的任意参数123CALL productpricing(@pricelow, @pricehigh, @priceaverage);执行名为productpricing的存储过程，计算返回产品的最低，最高和平均价格。 123456-- 一个返回产品平均价格的存储过程CREATE PROCEDURE productpricing()BEGIN SELECT Avg(prod_price) AS priceaverage FROM products;END; 上述代码创建一个名为productpricing的存储过程。如果存储过程接受参数，他们会在()中列举出来。 使用这个存储过程 1CALL productpricing(); -- 执行刚创建的存储过程并显示返回的结果 存储过程实际上是一种函数 删除存储过程存储过程在创建之后，被保存在服务器上以供使用，直至被删除 1DROP PROCEDURE productpricing; 如果指定的过程不存在，则DROP PROCEDURE将产生一个错误。也可以使用DROP PROCEDURE IF EXISTS 使用参数1234567891011121314151617181920CREATE PROCEDURE productpricing( OUT pl DECIMAL(8, 2), OUT ph DECIMAL(8, 2), OUT pa DECIMAL(8, 2))BEGIN SELECT Min(prod_price) INTO p1 FROM products; SELECT Max(prod_price) INTO ph FROM products; SELECT Avg(prod_price) INTO pa FROM products;END;/* 此存储过程接受3个参数：pl存储产品最低价格，ph存储产品最高价格，pa存储产品平均价格。每个参数必须具有指定的类型，这里使用十进制值。关键字OUT指出相应的参数用来从存储过程传出一个值（返回给调用者）。MySQL支持IN（传递给存储过程）、OUT(从存储过程传出)和INOUT（对存储过程传入和传出）类型的参数。 这里存储过程的代码是一系列的SELECT语句，用来检索值，然后保存到响应的变量（通过INTO关键字）*/ 所有的MySQL变量都必须以@开始 1234CALL productpricing(@pricelow, @pricehigh, @priceaverage);SELECT @pricehigh, @pricelow, @priceaverage; 下面使用IN和OUT参数，ordertotal接受订单号，并返回该订单的合计 12345678910CREATE PROCEDURE ordertotal( IN onumber INT, OUT ototal DECIMAL(8, 2))BEGIN SELECT Sum(item_price*quantity) FROM orderitems WHERE order_num = onumber INTO ototal;END; 调用这个存储过程 12CALL ordertotal(20005, @total);SELECT @total; 样例这样一个场景，需要获得与以前一样的订单合计，但需要对合计增加营业税，不过只针对某些顾客（或许是你所在州中那些顾客），那么需要做下面几件事： 获得合计 把营业税有条件的添加到合计 返回合计 1234567891011121314151617181920212223242526272829-- Name: ordertotal-- Parameters: onumber = order number-- taxable = 0 if not taxable, 1 if taxable-- ototal = order total variableCREATE PROCEDURE ordertotal( IN onumber INT, IN taxable BOOLEAN, OUT ototal DECIMAL(8, 2)) COMMENT 'Obtain order total, optionally adding tax'BEGIN -- Declare variable for total DECLARE total DECIMAL(8, 2); -- Declare tax percentage DECLARE taxrate INT DEFAULT 6; -- Get the order total SELECT Sum(item_price*quantity) FROM orderitems WHERE order_num = onumber INTO total; -- Is this taxable? IF taxable THEN -- Yes, so add taxrate to the total SELECT total + (total/100*taxrate) INTO total; END IF; -- And finally, save to out variable SELECT total INTO ototal;END; COMMENT关键字是不必需的，如果给出，将在SHOW PROCEDURE STATUS 的结果中显示 12CALL ordertotal(20005, 0, @total);SELECT @total; 12CALL ordertotal(20005, 1, @total);SELECT @total; 检查存储过程为显示用来创建一个存储过程的CREATE语句 1SHOW CREATE PROCEDURE ordertotal; 为获得包括何时，由谁创建等详细信息的存储过程列表 1234SHOW PROCEDURE STATUS;-- 限制过程状态结果，可以用LIKE指定一个过滤模式SHOW PROCEDURE STATUS LIKE 'ordertotal';]]></content>
      <categories>
        <category>数据库</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>数据库</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql视图]]></title>
    <url>%2F2019%2F12%2F24%2Fdb%2Fmysql%2F07%E3%80%81%E8%A7%86%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[视图是虚拟的表，只包含使用时动态检索的查询例如：下面的语句123SELECT cust_name, cust_contact FROM customers, orders, orderitems WHERE customers.cust_id = orders.cust_id AND orderitems.order_num = orders.order_num AND prod_id = 'TNT2'; 这个查询用来检索订购了某个特定产品的客户。任何需要这个数据的人都必须理解相关表的结构，并且知道如何创建查询和对表进行联结。为了检索其他产品（或多个产品）的相同数据，必须修改最后的WHERE子句。 使用视图后，语句变成 SELECT cust_name, cust_contact FROM productcustomers WHERE prod_id = &#39;TNT2&#39;; 其中，productcustomers是一个视图 应用 重用SQL语句 简化复杂的SQL操作。在编写查询后，可以方便地重用它而不必知道它的基本查询细节 使用表的组成部分而不是整个表 保护数据。可以给用户授予表的特定部分的访问权限而不是整个表的访问权限 更改数据格式和表示。视图可以返回与底层表的表示和格式不同的数据 视图仅仅是用来查看存储在别处的数据的一种设施，视图本身不包含数据，因此它们返回的数据是从其他表中检索出来的。在添加或更改这些表中的数据时，视图将返回改变过的数据。 性能问题：因为视图不包含数据，所以每次使用视图时，都必须处理查询执行时所需的任一个检索。如果用了多个联结和过滤创建了复杂的视图，可能会发现性能下降的厉害。 视图的规则和限制 与表一样，视图必须唯一命名（不能给视图取与别的视图或表相同的名字） 对于可以创建的视图数目没有限制 为了创建视图，必须具有足够的访问权限。这些限制通常由数据库管理人员授予 视图可以嵌套，即可以利用从其他视图中检索数据的查询来构造一个视图 ORDER BY可以用在视图中，但如果从该视图检索数据的SELECT语句中也含有ORDER BY，那么该视图中的ORDER BY将被覆盖 视图不能索引，也不能有关联的触发器或默认值 视图可以和表一起使用，例如，编写一条联结表和视图的SELECT语句 使用视图 利用视图简化复杂的联结 1234CREATE VIEW productcustomers ASSELECT cust_name, cust_contact, prod_id FROM customers, orders, orderitems WHERE customers.cust_id = orders.cust_id AND orderitems.order_num = orders.order_num; 这条语句创建一个名为productcustomers的视图，它联结三个表，以返回已订购了任意产品的所有客户的列表。如果执行SELECT * FROM productcustomers;，将列出订购了任意产品的客户。 为了检索订购了产品TNT2的客户，可进行下面查询 123SELECT cust_name, cust_contact FROM productcustomersWHERE prod_id = 'TNT2'; 用视图重新格式化检索出的数据 普通sql 123SELECT Contact(RTrim(vend_name), ' (', RTrim(vend_country), ')') AS vend_titleFROM vendorsORDER BY vend_name; 如果经常需要这个格式的结果，不必在每次需要时执行联结，创建一个视图，每次需要时使用它即可 123CREATE VIEW vendorlocations AS SELECT Contact(RTrim(vend_name), ' (', RTrim(vend_country), ')') AS vend_titleFROM vendorsORDER BY vend_name; 利用视图过滤不想要的数据 1234CREATE VIEW customeremaillist AS SELECT cust_id, cust_name, cust_emailFROM customersWHERE cust_email IS NOT NULL; 如果在视图检索数据时使用了一条WHERE子句，则两组子句（一组在视图中，另一组是传递给视图的）将自动组合 使用视图与计算字段 1234567CREAET VIEW orderitemsexpanded AS SELECT order_num, prod_id, quantity, item_price, quantity * item_price AS expanded_priceFROM orderitems; 更新视图通常，视图是可更新的（可以对它们使用INSERT, UPDATE, DELETE）。更新一个视图将更新其基表，如果你对视图增加或删除行，实际上是对其基表增加或删除行。 但是，并非所有的视图都是可更新的。如果MySQL不能正确地确定被更新的基数据，则不允许更新。这意味着，当视图定义中有以下操作，则不能进行视图更新： 分组（使用GROUP BY 和 HAVING） 联结 子查询 并 聚集函数（Min(), Count(), Sum()等） DISTINCT 导出（计算）列]]></content>
      <categories>
        <category>数据库</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>数据库</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux升级node版本]]></title>
    <url>%2F2019%2F12%2F23%2Fjavascript%2Fnodejs%2F07%E3%80%81linux%E5%8D%87%E7%BA%A7node%E7%89%88%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[问题在docker容器中启动node项目，但不希望每次都要npm i，所以要把源代码采用挂载的方式，放在镜像中。需要在服务器中npm i，之后启动容器，结果报错了，原因是node版本不对。后来采用了先挂载安装包，后再启动容器。其实我们可以选择升级一下本地主机的node版本就方便多了升级nodenode -v 首先清理npm的缓存 npm cache clean -f 安装版本管理工具 npm install -g n 更新到最新的版本 n latest 12345678n 常用的命令有：n 会列出所有安装的版本供你切换n latest 安装最新版本n stable 安装最新稳定版n lts 安装最新长期支持版本n rm [版本号] 删除某一版本n -h 帮助命令n [版本号] 安装指定版本node 查看node安装路径 which node 配置环境 vim ~/.bash_profile 添加下面语句 12export N_PREFIX=/usr/local/node-v7.10.0-linux-x64 #第4步显示的路径export PATH=$N_PREFIX/bin:$PATH 执行source使修改生效 source ~/.bash_profile 检查node，如果版本已更新，那就配置结束，如果未更新，则执行以下语句 n stable 升级npmnpm i -g npm 或者指定npm 版本 npm i -g npm@5.0.0]]></content>
      <categories>
        <category>javascript</category>
        <category>nodejs</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>javascript</tag>
        <tag>nodejs</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql组合查询]]></title>
    <url>%2F2019%2F12%2F19%2Fdb%2Fmysql%2F05%E3%80%81%E7%BB%84%E5%90%88%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[使用UNION多数SQL查询都只包含一个或多个表中返回数据的单条SELECT语句。MySQL也允许执行多个查询(多条SELECT语句)，并将结果作为单个查询结果集返回。这些组合查询通常称为并(union) 有两种情况需要使用组合查询： 在单个表查询中从不同的表返回类似结构的数据； 对单个表执行多个查询，按单个查询返回数据。 组合查询和多个WHERE条件：多数情况下，组合相同表的两个查询完成的工作与具有多个WHERE子句条件的单条查询完成的工作相同。 可用UNION操作符来组合数条SQL查询，所需要做的就是给出每条SELECT语句，在各条语句之间放上关键字UNION 例如： ​ 假设需要价格小于等于5的所有物品的一个列表，而且还想包括供应商1001和1002生产的所有物品，当然可以利用WHERE子句来完成，不过这里我们使用UNION 这里是两条语句的组合 SELECT vend_id, prod_id, prod_price FROM products WHERE prod_price &lt;= 5; SELECT vend_id, prod_id, prod_price FROM products WHERE vend_id IN (1001, 1002); 可以使用下面的语句 SELECT vend_id, prod_id, prod_price FROM products WHERE prod_price &lt;= 5 UNION SELECT vend_id, prod_id, prod_price FROM products WHERE vend_id IN (1001, 1002); 同样，可以使用WHERE子句 SELECT vend_id, prod_id, prod_price FROM products WHERE prod_price &lt;= 5 OR vend_id IN (1001, 1002); UNION规则 UNION必须由两条或两条以上的SELECT语句组成，语句之间用关键字UNION分隔 UNION中的每个查询必须包含相同的列、表达式或聚集函数（各个列不需要以相同的次序列出） 列数据烈性必须兼容：类型不必完全相同，但必须是DBMS可以隐含地转换的类型（不同数值类型或不同的日期类型） 包含或取消重复的行UNION会从查询的结果集中自动去除重复的行，如果想返回所有匹配行，可使用UNION ALL SELECT vend_id, prod_id, prod_price FROM products WHERE prod_price &lt;= 5 UNION ALL SELECT vend_id, prod_id, prod_price FROM products WHERE vend_id IN (1001, 1002); 对组合查询结果排序SELECT语句的输出用ORDER BY子句排序，在用UNION组合查询时，只能使用一条ORDER BY子句。它必须出现在最后一条SELECT语句之后。不允许使用多条ORDER BY子句。 SELECT vend_id, prod_id, prod_price FROM products WHERE prod_price &lt;= 5 UNION SELECT vend_id, prod_id, prod_price FROM products WHERE vend_id IN (1001, 1002) ORDER BY vend_id, prod_price;]]></content>
      <categories>
        <category>数据库</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>数据库</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql全文本搜索]]></title>
    <url>%2F2019%2F12%2F19%2Fdb%2Fmysql%2F06%E3%80%81%E5%85%A8%E6%96%87%E6%9C%AC%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[理解全文本搜索并非所有引擎都支持全文本搜索：MySQL最常用的引擎MyISAM和InnoDB，前者支持全文本索引，后者不支持。 LIKE关键字利用通配操作符匹配文本（和部分文本），能够查找包含特殊值或部分值的行 正则表达式，可以编写查找所需行的非常复杂的匹配模式 虽然以上搜索机制非常有用，但存在几个重要的限制。 性能——通配符和正则表达式匹配通常要求MySQL尝试匹配表中所有行（而且这些搜索极少使用表索引）。因此，由于被搜索行数不断增加，这些搜索可能非常耗时。 明确控制——使用通配符和正则表达式匹配，很难（而且并不总是能）明确地控制匹配什么和不匹配什么。例如，指定一个词必须匹配，一个词必须不匹配，而一个词仅在第一个词确实匹配的情况下，才可以匹配或者才可以不匹配。 智能化的结果——虽然基于通配符和正则表达式的搜索提供了非常灵活的搜索，但他们都不能提供一种智能化选择结果的方法。 使用全文本搜索为了进行全文本搜索，必须索引被搜索的列，而且要随着数据的改变不断地重新索引，在对表列进行适当设计后，MySQL会自动进行所有的索引和重新索引。 在索引之后，SELECT可与Match()和Against()一起使用以实际执行搜索。 启用全文本搜索支持一般在创建表时启用全文本搜索。CREATE TABLE语句接受FULLTEXT子句，它给出一个被索引列的一个逗号分隔的列表 123456789CREATE TABLE productnotes( note_id int NOT NULL AUTO_INCREMENT, prod_id char(10) NOT NULL, note_date datetime NOT NULL, note_text text NULL, PRIMARY KEY(note_id), FULLTEXT(note_text)) ENGINE=MyISAM; 这里FULLTEXT索引单个列，如果需要也可以指定多个列。 在定义之后，MySQL自动维护该索引。在增加、更新或删除行时，索引随之自动更新。 可以在创建表时指定FULLTEXT,或者在稍后指定（在这种情况下所有已有数据必须立即索引） 进行全文本搜索在索引之后，使用两个函数Match()和Against()执行全文本搜索，其中Match()指定被搜索的列，Against()指定要使用的搜索表达式。 SELECT note_text FROM productnotes WHERE Match(note_text) Against(&#39;rabbit&#39;); 注意： 传递给Match()的值必须与FULLTEXT()定义中的相同。如果指定多个列，则必须列出它们（而且次序正确）。 除非使用BINARY方式，否则全文本搜索不区分大小写。 上述语句也可以用LIKE完成： SELECT note_text FROM productnotes WHERE note_text LIKE &#39;%rabbit%&#39;; 上述两条SELECT语句都不包含ORDER BY子句，后者（使用LIKE）以不特别有用的顺序返回。前者（使用全文本搜索）返回以文本匹配的良好程度排序的数据。两行都包含rabbit，但包含词rabbit作为第三个词的行的等级比作为第20个词的行高。 使用查询扩展查询扩展用来设法放宽所返回的全文本搜索结果的范围。考虑下面的情况，想找出所有提到anvils的注释。只有一个注释包含词anvils，但你还想找出可能与你的搜索有关的所有其他行，即使它们不包含词anvils 在使用查询扩展时，MySQL对数据和索引进行两遍扫描来完成搜索 首先，进行一个基本的全文本搜索，找出与搜索条件匹配的所有行 其次，MySQL检查这些匹配行并选择所有有用的词 再其次，MySQL再次进行全文本搜索，这次不仅使用原来的条件，而且还使用所有有用的词。 注意：只用于MySQL 4.1.1或更高的版本 例： 一个简单的全文本搜索 SELECT note_text FROM productnotes WHERE Match(note_text) Against(&#39;anvils&#39;); 相同的搜索，这次使用查询扩展 SELECT note_text FROM productnotes WHERE Match(note_text) Against (&#39;anvils&#39; WITH QUERY EXPANSION); 查询扩展极大地增加了返回的行数，但这样做也增加了实际上并不想要的行的数目。 布尔文本搜索 MySQL支持全文本搜索的另外一种形势，成为布尔方式。以布尔方式，可以提供关于如下内容的细节： 要匹配的词； 要排斥的词 排列提示 表达式分组 另外一些内容 布尔方式即使没有定义FULLTEXT索引，也可以使用，但这是一种非常缓慢的操作（其性能将随数据量的增加而降低） SELECT note_text FROM productnotes WHERE Match(note_text) Against(&#39;heavy&#39; IN BOOLEAN MODE); 分析：此全文搜索检索包含词heavy的所有行（有两行）。其中使用了关键字IN BOOLEAN MODE，但实际上没有指定布尔操作符，因此，其结果与没有指定布尔方式的结果相同。 匹配包含heavy但不包含任意以rope开始的词的行 SELECT note_text FROM productnotes WHERE Match(note_text) Against(&#39;heavy -rope*&#39; IN BOOLEAN MODE); 这次只返回一行。这一次仍然匹配词heavy，但-rope*明确地指示MySQL排除包含rope*(任何以rope开始的词，包括ropes)的行，这就是为什么上一个例子中的第一行被排除的原因。 附：如果使用的是MySQL 4.x则上面的例子可能不返回任何行。在MySQL 4.x使用这个例子，使用-ropes而不是-rope*(排除ropes而不是排除任何以rope开始的词)。 布尔操作符 说明 + 包含，词必须存在 - 排除，词必须不出现 &gt; 包含，而且增加等级值 &lt; 包含，且减少等级值 () 把词组成子表达式（允许这些子表达式作为一个组被包含、排除、排列等） ~ 取消一个词的排序值 * 词尾的通配符 “” 定义一个短语（与单词的列表不一样，它匹配整个短语以便包含或排除这个短语） 下面举例说明操作符如何使用 SELECT note_text FROM productnotes WHERE Match(note_text) Against(&#39;+rabbit +bait&#39; IN BOOLEAN MODE); 搜索匹配包含词rabbit和bait的行 SELECT note_text FROM productnotes WHERE Match(note_text) Against(&#39;rabbit bait&#39; IN BOOLEAN MODE); 没有指定操作符，这个搜索匹配包含rabbit和bait中至少一个词的行 SELECT note_text FROM productnotes WHERE Match(note_text) Against(&#39;&quot;rabbit bait&quot;&#39; IN BOOLEAN MODE); 搜索匹配短语rabbit bait 而不是匹配两个词 SELECT note_text FROM productnotes WHERE Match(note_text) Against(&#39;&gt;rabbit &lt;carrot&#39; IN BOOLEAN MODE); 匹配rabbit和carrot，增加前者的等级，降低后者的等级。 SELECT note_text FROM productnotes WHERE Match(note_text) Against(&#39;+safe +(&lt;combination)&#39; IN BOOLEAN MODE); 搜索匹配词safe和combination，降低后者的等级 全文本搜索的使用说明 在索引全文本数据时，短词被忽略且从索引中排除。短语定义为那些具有3个或3个一下字符的词（如果需要，这个数目可以更改）。 MySQL带有一个内建的非用词（stopword）列表，这些词在索引全文本数据时总是被忽略。如果需要，可以覆盖这个列表 许多词出现的频率很高，搜索它们没有用处（返回太多的结果）。因此，MySQL规定了一条50%规则，如果一个词出现在50%以上的行中，则将它作为一个非用词忽略。50%规则不用于IN BOOLEAN MODE 如果表中的行数少于3行，则全文本搜索不返回结果（因为每个词或者不出现，或者至少出现在50%的行中） 忽略词中的单引号，例如，don’t索引为dont 不具有词分隔符（包括日语和汉语）的语言不能恰当地返回全文本搜索结果 仅在MyISAM数据库引擎中支持全文本搜索。]]></content>
      <categories>
        <category>数据库</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>数据库</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql数据分组]]></title>
    <url>%2F2019%2F12%2F13%2Fdb%2Fmysql%2F04%E3%80%81%E6%95%B0%E6%8D%AE%E5%88%86%E7%BB%84%2F</url>
    <content type="text"><![CDATA[创建分组分组是在SELECT语句中的GROUP BY 子句中建立的。 例： SELECT vend_id, COUNT(*) AS num_prods FROM products GROUP BY vend_id; GROUP BY GROUP BY子句可以包含任意数目的列，这使得能对分组进行嵌套，为数据分组提供更细致的控制。 如果在GROUP BY子句中嵌套了分组，数据将在最后规定的分组商家进行汇总。换句话说，在建立分组时，指定的所有列都一起计算。（所有不能从个别的列取回数据）。 GROUP BY子句中列出的每个列都必须是检索列或有效的表达式（但不能是聚集函数）。如果在SELECT中使用表达式，则必须在GROUP BY子句中指定相同的表达式，不能使用别名 除聚集计算语句外，SELECT语句中的每个列都必须在GROUP BY子句中给出。 如果分组列中具有NULL值，则NULL将作为一个分组返回。如果列中有多行NULL值，他们将分为一组。 GROUP BY子句必须出现在WHERE子句之后，ORDER BY 子句之前。 过滤分组如果想要列出至少有两个订单的所有骨科，这样的数据就必须基于完整的分组而不是个别的行进行过滤。 可以使用HAVING SELECT cust_id, COUNT(*) AS orders FROM orders GROUP BY cust_id HAVING COUNT(*) &gt;= 2; HAVING和WHERE的差别，WHERE在数据分组前进行过滤，HAVING在分组后进行过滤。当然，两个也可以在同一条语句中出现。 列出具有2个（含）以上、价格为10（含）以上的产品的供应商 SELECT vend_id, COUNT(*) AS num_prods FROM products WHERE prod_price &gt;= 10 GROUP BY vend_id HAVING COUNT(*) &gt;= 2; 列出具有两个至上的产品的供应商 SELECT vend_id, COUNT(*) AS num_prods FROM products GROUP BY vend_id HAVING COUNT(*) &gt;= 2; 分组和排序 ORDER BY GROUP BY 排序产生的输出 分组行。但输出可能不是分组的顺序 任意列都可以使用（非选择的列也可以） 只可能使用选择列或表达式列，而且必须使用每个选择列表达式 不一定需要 如果与聚集函数一起使用列（表达式），则必须使用 检索总计订单价格大于等于50的订单的订到号和总计订单价格 SELECT order_num, SUM(quantity*item) AS ordertotal FROM orderitems GROUP BY order_num HAVING SUM(quantity*item_price) &gt;= 50; 如果按总计订单价格排序输出 SELECT order_num, SUM(quantity * item_price) AS ordertotal FROM orderitems GROUP BY order_num HAVING SUM(quantity * item_price) &gt;= 50 ORDER BY ordertotal; SELECT子句的顺序 子句 说明 是否必须使用 SELECT 要返回的列或表达式 是 FROM 从中检索数据的表 仅在从表选择数据时使用 GROUP BY 分组说明 仅在按组计算聚集时使用 WHERE 行级过滤 否 HAVING 组级过滤 否 ORDER BY 输出排序顺序 否 LIMIT 要检索的行数 否]]></content>
      <categories>
        <category>数据库</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>数据库</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql正则字符]]></title>
    <url>%2F2019%2F12%2F12%2Fdb%2Fmysql%2F01%E3%80%81mysql%E6%AD%A3%E5%88%99%2F</url>
    <content type="text"><![CDATA[字符类 类 说明 [:alnum:] 任意字母和数字(同[a-zA-Z0-9]) [:alpha:] 任意字符(同[a-zA-Z]) [:blank:] 空格和制表(同[\t]) [:cntrl:] ASCII控制字符(ASCII 0到32 和127) [:digit:] 任意数字(同[0-9]) [:graph:] 与[:print:]相同，但不包括空格 [:lower:] 任意小写字母(同[a-z]) [:print:] 任意可打印字符 [:punct:] 既不在[:alnum:]又不在[:cntrl:]中的任意字符 [:space:] 包括空格在内的任意空白字符(同[\f\n\r\t\v]) [:upper:] 任意大写字母(同[A-Z]) [:xdigit:] 任意十六进制数字 空白元字符 元字符 说明 \f 换页 \n 换行 \r 回车 \t 制表 \v 纵向制表 重复元字符 元字符 说明 * 0个或多个匹配 + 1个或多个匹配(等于{1, }) ? 0个或1个匹配(等于{0}) {n} 指定数目的匹配 {n, } 不少于指定数目的匹配 {n, m} 匹配数目的范围(m不超过255) 定位元字符 元字符 说明 ^ 文本的开始 $ 文本的结束 [[:&lt;:]] 词的开始 [[:&gt;:]] 词的结束]]></content>
      <categories>
        <category>数据库</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>数据库</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql数据处理函数]]></title>
    <url>%2F2019%2F12%2F12%2Fdb%2Fmysql%2F03%E3%80%81mysql%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[函数SQL支持利用函数来处理数据。函数一般是在数据上执行的，它给数据的转换和处理提供了方便。文本处理函数 函数 说明 Left() 返回串左边的字符 Length() 返回串的长度 Locate() 找出串的一个子串 Lower() 将串转换为小写 LTrim() 去掉串左边的空格 Right() 返回串右边的字符 RTrim() 去掉串右边的空格 Soundex() 返回串的SOUNDEX值 SubString() 返回子串的字符 Upper() 将串转换为大写 SELECT vend_name, Upper(vend_name) AS vend_name_upcase FROM vendors ORDER BY vend_name; 日期和时间处理函数 函数 说明 AddDate() 增加一个日期(天、周等) AddTime() 增加一个时间(时、分等) CurDate() 返回当前日期 CurTime() 返回当前时间 Date() 返回日期时间的日期部分 DateDiff() 计算两个日期的差 Date_Add() 高度灵活的日期运算函数 Date_Format() 返回一个格式化的日期或时间串 Day() 返回一个日期的天数部分 DayOfWeek() 对于一个日期，返回对应的星期几 Hour() 返回一个时间的小时部分 Minute() 返回一个时间的分钟部分 Month() 返回一个时间的月份部分 Now() 返回当前日期和时间 Second() 返回一个时间的秒部分 Time() 返回一个日期时间的时间部分 Year() 返回一个日期的年份部分 匹配订单日期在2005-09-01那天： SELECT cust_id, order_num FROM orders WHERE Date(order_date) = &#39;2005-09-01&#39;; 订单日期在2005年9月份的： SELECT cust_id, order_num FROM orders WHERE Date(order_date) BETWEEN &#39;2005-09-01&#39; AND &#39;2005-09-30&#39;; 也可以按照下面这种写法，这种写法不必考虑一个月有多少天： SELECT cust_id, order_num FROM orders WHERE Year(order_date) = 2005 AND Month(order_date) = 9; 数值处理函数 函数 说明 Abs() 返回一个数的绝对值 Cos() 返回一个角度的余弦 Exp() 返回一个数的指数值 Mod() 返回除操作的余数 Pi() 返回圆周率 Rand() 返回一个随机数 Sin() 返回一个角度的正弦 Sqrt() 返回一个数的平方根 Tan() 返回一个角度的正切 聚集函数 函数 说明 AVG() 返回某列的平均值 COUNT() 返回某列的行数 MAX() 返回某列的最大值 MIN() 返回某列的最小值 SUM() 返回某列值之和 AVG() 函数返回products表中所有产品的平均价格：SELECT AVG(prod_price) AS avg_price FROM products; 也可以用来确定特定列或行的平均值。 返回特定供应商所提供的产品的平均价格： SELECT AVG(prod_price) AS avg_price FROM products WHERE vend_id = 1003; AVG()函数只能用于单个列，且忽略列值为NULL的行 COUNT() 函数返回customers表中客户的总数： SELETE COUNT(*) AS num_cust FROM customers; 只对具有电子邮件地址的客户计数： SELECT COUNT(cust_email) AS num_cust FROM customers; NULL值，如果制定列名，则制定列的值为空的行被COUNT()函数忽略，但如果COUNT()函数中用的是*，则不忽略。 MAX() 函数返回products表中最贵的物品的价格： SELECT MAX(prod_price) AS max_price FROM products; 对非数值数据使用MAX()，虽然MAX()一般用来找出最大的数值或时间，但MySQL允许将它用来返回任意列中的最大值，包括返回文本列中的最大值。用于文本数据时，如果数据按相应的列排序，则返回最后一行 MAX() 函数忽略列值为NULL的行 MIN() 函数正好与MAX()功能相反 SELECT MIN(prod_price) AS min_price FROM product; SUM() 函数orderitems表包含订单中实际的物品，每个物品有相应的数量(quantity)。检索所订购物品的总数（所有quantity值之和）: SELECT SUM(quantity) AS items_ordered FROM orderitems WHERE order_num = 20005; SUM()也可以用来合计计算值。 合计每项物品的item_price*quantity，得出总的订单金额(where 保证了只统计某个物品订单中的物品)： SELECT SUM(item_price*quantity) AS total_price FROM orderitems WHERE order_num = 20005; 聚类不同值 对所有行执行计算，指定ALL参数或者不给参数。(某人是ALL) 只包含不同的值，指定DISTINCT参数。 SELECT AVG(DISTINCT prod_price) AS avg_price FROM products WHERE vend_id = 1003; 注意：如果指定列名，则DISTINCT只能用于COUNT()。DISTINCT 不能用于COUNT(*)，因此，不允许使用COUNT(DISTINCT)。类似的，DISTINCT必须使用列名，不能用于计算或表达式。 组合聚集函数12345SELECT COUNT(*) AS num_items, MIN(prod_price) AS price_min, MAX(prod_price) AS price_max, AVG(prod_price) AS price_avg FROM products;]]></content>
      <categories>
        <category>数据库</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>数据库</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql计算字段]]></title>
    <url>%2F2019%2F12%2F12%2Fdb%2Fmysql%2F02%E3%80%81mysql%E8%AE%A1%E7%AE%97%E5%AD%97%E6%AE%B5%2F</url>
    <content type="text"><![CDATA[计算字段存储在数据库表中的数据一般不是应用程序所需要的格式。 如果想在一个字段中既显示公司名，又显示公司的地址，但是这两个信息一般包含在不同的表列中。 城市、州和邮政编码存储在不同的列中，但邮件标签打印程序却需要把它们作为一个恰当格式的字段检索出来。 列数据是大小写混合的，但报表程序需要把所有数据按大写表示出来 物品订单表存储物品的价格和数量，但不需要存储每个物品的总价格（用价格乘以数量即可）。 根据表数据进行总数，平均数计算或其他计算 1. 拼接字段举例：vendors表中包含供应商名和位置信息。假如要生成一个供应商报表，需要在供应商的名字中按照name(location)这样的格式列出供应商的位置。 此报表需要单个值，而表中数据存储在两个列vend_name和vend_country中。此外，需要用括号将vend_country括起来。 拼接可以使用Contcat()函数来拼接两个列 SELECT Concat(vend_name, &#39; (&#39;, vend_country, &#39;)&#39;) FROM vendors ORDER BY vend_name; 同时，我们需要删除数据右侧多余的空格，可以使用RTrim()来完成 SELECT Concat(RTrim(vend_name), &#39; (&#39;, RTrim(vend_country), &#39;)&#39;) FROM vendors ORDER BY vend_name; 去除空格还可以用LTrim()去掉左边的空格，Trim()去掉右边的空格 使用别名SELECT Concat(RTrim(vend_name), &#39; (&#39;, RTrim(vend_country), &#39;)&#39;) AS vend_title FROM vendors ORDER BY vend_name; 2. 执行算术计算orders表包含收到的所有订单，orderitems表包含每个订到的各项物品。下面的SQL语句检索订单号20005中的所有物品 SELECT prod_id, quantity, item_price FROM orderitems WHERE order_num = 20005; item_prict 列包好订单中每项物品的单价，如下可以汇总物品的价格 SELECT prod_id, quantity, item_price, quantity*item_price AS expanded_price FROM orderitems WHERE order_num = 20005; MySQL算术操作符 操作符 说明 + 加 - 减 * 乘 / 除 TipsSELECT Now(); 会利用Now()函数返回当前日期和时间]]></content>
      <categories>
        <category>数据库</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>数据库</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树]]></title>
    <url>%2F2019%2F11%2F20%2Falgorithm%5B%E7%AE%97%E6%B3%95%5D%2F5.%E6%A0%91%2F</url>
    <content type="text"><![CDATA[树（tree）是n（n&lt;=0）个节点的有限集。当n=0时，成为空树。在任意一个非空树中，有如下特点： 有且仅有一个特定的成为根的节点。 当n&gt;1时，其余节点可分为m（m&gt;0）个互不相交的有限集，每一个集合本身又是一个树，并称为根的子树 树的最大层级数，被称为树的高度或深度，上面这个数的高度是4 二叉树二叉树（binary tree）是树的一种特殊形式。这种树的每个节点最多有两个孩子节点。 二叉树有两种特殊的形式，一个叫做满二叉树，另一个叫完全二叉树。 满二叉树 一个二叉树的所有非叶子节点都存在左右孩子，并且所有的叶子节点都在同一层级上；简单来说，满二叉树的每一个分支都是满的 完全二叉树 对一个有n个节点的二叉树，按层级顺序编号，则所有的节点的编号为从1到n。如果这个树所有节点和同样深度的满二叉树的编号为从1到n的节点位置相同，则这个二叉树为完全二叉树。 上图中，二叉树编号从1到12的12个节点，和前面满二叉树从1到12的节点位置完全对应，因此这个树是完全二叉树。 完全二叉树的条件没有满二叉树那么苛刻：满二叉树要求所有分支都是满的；而完全二叉树只需保证最后一个节点之前的节点都齐全即可。 存储结构： 链式存储结构 链式存储是二叉树最直观的存储方式。 一个节点最多可以指向左右两个孩子节点，所以二叉树的每一个节点包含3部分 存储数据的data变量 指向左孩子的left指针 指向右孩子的right指针 数组 使用数组存储时，会按照层级顺序把二叉树的节点放到数组中对应的位置。如果某一个节点的左孩子或者右孩子空缺，则数组的相应位置也空出来。 这样设计可以更方便的在数组中定位二叉树的孩子节点和父节点。 假设一个父节点的下标是parent，那么它的左孩子节点下标就是2 × parent + 1，右孩子节点的下标就是2 × parent + 2 反过来，一个左孩子节点下标是leftChild，那么它的父节点的下标就是(leftChild - 1) / 2 对于一个稀疏的二叉树来说，用数组表示法是非常浪费空间的。 二叉树的应用1. 查找二叉查找树 如果左子树不为空，则左子树上所有节点的值均小于根节点的值 如果右子树不为空，则右子树上所有节点的值均大于根节点的值 左右子树也都是二叉查找树 例如查找值为4的节点 访问根节点6，发现4 &lt; 6 访问节点6的左孩子节点3，发现4 &gt; 3 访问节点3的右孩子节点4，发现4 = 4，正是要找的节点 对于一个节点分布相对均衡的二叉查找树来说，如果节点总数是n，那么搜索节点的时间复杂度就是O(logn)，和树的深度是一样的。 2. 维持相对顺序仍然要从二叉查找树说起，二叉查找树要求左子树小于父节点，右子树大于父节点，正是这样保证了二叉树的有序性。 因此二叉查找树又叫二叉排序树 新插入的节点，同样要遵循二叉排序树的原则，例如插入新元素5，由于5 &lt; 6, 5 &gt; 3, 5 &gt; 4，所以最终为插入节点4的右孩子的位置 这样却隐藏了一个致命的问题，在二叉查找树中依次插入9、8、7、6、5、4会出现下面这种情况 这就涉及到了二叉树的自平衡，后面会说到 二叉树的遍历分为4种：前序遍历，中序遍历，后序遍历，层序遍历 从更宏观的角度来看，二叉树的遍历归结为两大类： 深度优先遍历（前序遍历，中序遍历，后序遍历） 广度优先遍历（层序遍历） 深度优先遍历 前序遍历 输出顺序是根节点，左子树，右子树 中序遍历 输出顺序是左子树，根节点，右子树 后序遍历 输出顺序是左子树，右子树，根节点 递归的方式 非递归方式（栈） 首先遍历二叉树的根节点，放入栈中。 遍历根节点1的左孩子节点2，放入栈中。 遍历节点2的左孩子节点4，放入栈中。 节点4既没有左孩子，也没有右孩子，我们需要回溯到上一个节点2。 因为栈已经存储了刚才遍历的路径。让旧的栈顶元素4出栈，就可以重新访问节点2，得到节点2的右孩子节点5. 此时节点2已经没有利用价值了，节点2出栈，节点5入栈。 节点5既没有左孩子，也没有右孩子，我们需要再次回溯，一直回溯到节点1，所以让节点5出栈，根节点1的右孩子是节点3，节点1出栈，节点3入栈。 节点3的右孩子是节点6，节点3出栈，节点6入栈 节点6既没有左孩子，也没有右孩子，所以节点6出栈。此时栈为空，遍历结束。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116import java.util.Arrays;import java.util.LinkedList;import java.util.Stack;public class helloword &#123; /** * 构建二叉树 * * @param inputList 输入序列 */ public static TreeNode createBinaryTree(LinkedList&lt;Integer&gt; inputList) &#123; TreeNode node = null; if (inputList == null || inputList.isEmpty()) &#123; return null; &#125; Integer data = inputList.removeFirst(); if (data != null) &#123; node = new TreeNode(data); node.leftChild = createBinaryTree(inputList); node.rightChild = createBinaryTree(inputList); &#125; return node; &#125; /** * 二叉树的前序遍历 * * @param node 二叉树节点 */ public static void preOrderTraveral(TreeNode node) &#123; if (node == null) &#123; return; &#125; System.out.println(node.data); preOrderTraveral(node.leftChild); preOrderTraveral(node.rightChild); &#125; /** * 二叉树的中序遍历 * * @param node 二叉树节点 */ public static void inOrderTraveral(TreeNode node) &#123; if (node == null) &#123; return; &#125; inOrderTraveral(node.leftChild); System.out.println(node.data); inOrderTraveral(node.rightChild); &#125; /** * 二叉树的后序遍历 * * @param node 二叉树节点 */ public static void postOrderTraveral(TreeNode node) &#123; if (node == null) &#123; return; &#125; postOrderTraveral(node.leftChild); postOrderTraveral(node.rightChild); System.out.println(node.data); &#125; /** * 二叉树节点 */ private static class TreeNode &#123; int data; TreeNode leftChild; TreeNode rightChild; TreeNode(int data) &#123; this.data = data; &#125; &#125; /** * 二叉树非递归前序遍历 * * @param root 二叉树根节点 */ public static void preOrderTraveralWithStack(TreeNode root) &#123; Stack&lt;TreeNode&gt; stack = new Stack&lt;TreeNode&gt;(); TreeNode treeNode = root; while(treeNode != null || !stack.isEmpty()) &#123; // 迭代访问节点的左孩子，并入栈 while (treeNode != null) &#123; System.out.println(treeNode.data); stack.push(treeNode); treeNode = treeNode.leftChild; &#125; // 如果节点没有左孩子，则弹出栈顶节点，访问节点右孩子 if (!stack.isEmpty()) &#123; treeNode = stack.pop(); treeNode = treeNode.rightChild; &#125; &#125; &#125; public static void main(String[] args) &#123; LinkedList&lt;Integer&gt; inputList = new LinkedList&lt;Integer&gt;(Arrays.asList(new Integer[]&#123;3, 2, 9, null, null, 10, null, null, 8, null, 4&#125;)); TreeNode treeNode = createBinaryTree(inputList); System.out.println("前序遍历: "); preOrderTraveral(treeNode); System.out.println("无递归实现前序遍历: "); preOrderTraveralWithStack(treeNode); System.out.println("中序遍历: "); inOrderTraveral(treeNode); System.out.println("后序遍历: "); postOrderTraveral(treeNode); &#125;&#125; 广度优先遍历 根节点1进入队列 节点1出队，输出节点1，并得到节点1的左孩子节点2、右孩子节点3.让节点2和节点3入队。 节点2出队，输出节点2，并得到节点2的左孩子4、右孩子节点5。让节点4和节点5入队。 节点3出队，输出节点3，并得到节点三的右孩子节点6。让节点6入队 节点4出队，输出节点4，由于节点4没有孩子节点，所以没有新节点入队 节点5出队，输出节点5，由于节点5同样没有孩子节点，所以没有新节点入队。 节点6出队，输出节点6，节点6没有孩子节点，没有新节点入队 123456789101112131415161718/** 二叉树层序遍历* @param root 二叉树根节点* */public static void levelOrderTraversal(TreeNode root)&#123; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;(); queue.offer(root); while(!queue.isEmpty())&#123; TreeNode node = queue.poll(); System.out.println(node.data); if (node.leftChild != null)&#123; queue.offer(node.leftChild); &#125; if (node.rightChild != null) &#123; queue.offer(node.rightChild); &#125; &#125;&#125; 二叉堆二叉堆本质上是一种完全二叉树，分为两个类型 最大堆 最大堆的任何一个父节点的值，都大于或等于它左、右孩子节点的值。 最小堆 最小堆的任何一个父节点的值，都小于或等于它左、右孩子节点的值。 二叉树的根节点叫做堆顶。 最大堆和最小堆的特点决定了：最大堆的堆顶是整个堆中的最大元素；最小堆的堆顶是整个堆中的最小元素。 二叉堆的自我调整堆的插入和删除操作时间复杂度是O(logn) 节点的上浮和下沉时间复杂度是O(logn) 构建的时间复杂度是O(n) 插入节点 当二叉堆插入节点时，插入位置是完全二叉树的最后一个位置。例如插入一个新节点，值是0 这时，新节点的父节点5比0大，显然不符合最小堆的性质，于是让新节点“上浮”，和父节点交换位置 继续用节点0和父节点3作比较，因为0小于3，则让新节点继续上浮 最终新节点0“上浮”到了堆顶位置 删除节点 二叉堆删除节点的过程和插入节点的过程正好相反，所删除的是处于堆顶的节点1 这时，为了继续维持完全二叉树的结构，我们把堆的最后一个节点10临时补到原本的堆顶位置。 接下来，让暂处栈顶位置的节点10和它的左、右孩子节点中最小的一个，显然是节点2，那么让节点10“下沉”。 继续让节点10和它的左右孩子作比较，左右孩子中最小的节点7，由于10大于7，让节点10继续“下沉” 这样，二叉堆重新得到了调整 构建二叉堆 构建二叉堆就是把一个完全无序的二叉树调整为二叉堆，本质就是让所有的非叶子节点依次下沉。 首先，从最后一个非叶子节点开始，也就是从节点10开始。如果节点10大于它左、右孩子节点中最小的一个，则节点10下沉 接下来轮到节点3，如果节点3,大于它左、右孩子节点中最小的一个，则节点3下沉 然后轮到节点1，如果节点1大于它左、右孩子节点中最小的一个，则节点1下沉。 接下来轮到节点7，如果节点7大于它左、右孩子节点中最小的一个，则节点7下沉 节点7继续比较，继续下沉 经过上述的几轮比较和下沉，最终每一节点都小于它的左右孩子节点，一个无序的完全二叉树就被构建成了一个最小堆。 代码实现二叉堆虽然是一个完全二叉树，但它的存储方式并不是链式的，而是顺序存储。二叉堆的所有节点都存储在数组中 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import java.util.Arrays;public class test &#123; /* * “上浮”调整 * @param array 待调整的堆 * */ public static void upAdjust(int[] array) &#123; int childIndex = array.length - 1; int parentIndex = (childIndex - 1) / 2; // temp 保存插入的叶子节点值，用于最后的赋值 int temp = array[childIndex]; while (childIndex &gt; 0 &amp;&amp; temp &lt; array[parentIndex]) &#123; // 无须真正交换，单向赋值即可 array[childIndex] = array[parentIndex]; childIndex = parentIndex; parentIndex = (parentIndex - 1) / 2; &#125; array[childIndex] = temp; &#125; /* * “下沉”调整 * @param array 待调整的堆 * @param parentIndex 要“下沉”的父节点 * @param length 堆的有效大小 * */ public static void downAdjust(int[] array, int parentIndex, int length) &#123; // temp 保存父节点值，用于最后的赋值 int temp = array[parentIndex]; int childIndex = 2 * parentIndex + 1; while (childIndex &lt; length) &#123; // 如果有右孩子，且右孩子小于左孩子的值，则定位到右孩子 if (childIndex + 1 &lt; length &amp;&amp; array[childIndex + 1] &lt; array[childIndex]) &#123; childIndex++; &#125; // 如果父节点小于任何一个孩子的值，则直接跳出 if (temp &lt;= array[childIndex]) break; // 无需真正的交换，单向赋值即可 array[parentIndex] = array[childIndex]; parentIndex = childIndex; childIndex = 2 * childIndex + 1; &#125; array[parentIndex] = temp; &#125; /* * 构建堆 * @param array 待调整的堆 * */ public static void buildHeap(int[] array) &#123; // 从最后一个非叶子节点开始，依次下沉调整 for (int i = (array.length - 2) / 2; i &gt;= 0; i--) &#123; downAdjust(array, i, array.length); &#125; &#125; public static void main(String[] args) &#123; int[] array = new int[]&#123;1, 3, 2, 6, 5, 7, 8, 9, 10, 0&#125;; upAdjust(array); System.out.println(Arrays.toString(array)); array = new int[]&#123;7, 1, 3, 10, 5, 2, 8, 9, 6&#125;; buildHeap(array); System.out.println(Arrays.toString(array)); &#125;&#125; 优先队列优先队列不再遵循先入先出的原则，而是分为两种情况 最大优先队列，无论入队顺序如何，都是当前最大的元素优先出队 最小优先队列，无论入队顺序如何，都是当前最小的元素优先出队 对列的入队和出队与二叉堆的插入和删除一致，所以时间复杂度也是O(logn)]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构]]></title>
    <url>%2F2019%2F11%2F19%2Falgorithm%5B%E7%AE%97%E6%B3%95%5D%2F4.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[1. 数组和链表1.1 数组数组是有限个相同类型的变量所组成的有序集合，数组中的每一个变量被称为元素。 数组中的每一个元素，都存储在内存单元中，并且元素之间紧密排列，既不能打乱元素的存储顺序，也不能跳过某个存储单元进行存储。 1.1.1 数组的基本操作更新和读取的时间复杂度是O(1) 插入和删除的时间复杂度是O(n) 还有一种删除操作：如果数组无序，把最后一个元素复制到要删除元素的位置，然后删除最后一个元素。这样时间复杂度为O(1) 读取元素 由于数组在内存中顺序存储，所以只要给出一个数组下标，就可以读取到对应的数组元素。 根据下标读取元素的方式叫做随机读取 123int[] array = new int[]&#123;3,1,2,5,4,9,7,2&#125;;// 输出数组中下标为3的元素System.out.println(array[3]); 更新元素 123int[] array = new int[]&#123;3,1,2,5,4,9,7,2&#125;;array[5] = 10;System.out.println(array[5]); 插入元素 尾部插入 中间插入 超范围插入（需要创建一个新数组） 删除元素 数组的删除操作和插入操作的过程相反，如果删除的元素位于数组中间，其后的元素都需要向前挪动一位。 1.1.2 数组的优势和劣势数组具有非常高效的随机访问能力，只要给出下标，就可以用常量时间找到对应的元素。比如：二分查找。 劣势体现在插入和删除方面，由于数组元素连续紧密地存储在内存中，插入和删除元素都会导致大量的元素被迫移动，影响效率。 总体来说，数组适合的是读操作多，写操作少的场景。 1.2 链表链表是一种在物理上非连续、非顺序的数据结构，由若干节点组成。 单项链表的每一个节点又包含两部分，一部分是存放数据的变量data，另一部分是指向下一个节点的指针next。 双向链表每个节点除了拥有data和next指针，还有指向前置节点的prev指针。 如果说数组在内存中存储方式是顺序存储，那么链表在内存中的存储方式是随机存储。 1.2.1 链表的基本操作查找的时间复杂度是O(n) 不考虑查找过程，插入和删除的时间复杂度是O(1) 查找节点 链表只能从头节点开始向后一个一个节点逐一查找。 更新节点 忽略查找的过程，和数组类似 插入 尾部插入 把最后一个节点的next指针指向新插入的节点即可 头部插入 把新节点的next指针指向原先的头结点 把新节点变为链表的头结点（指针） 中间插入 新节点的next指针，指向插入位置的节点 插入位置的前置节点的next指针指向新节点 只要内存空间允许，能够插入链表的元素时无穷无尽的，不需要考虑扩容的问题。 删除元素 尾部删除 把倒数第二个节点的next指针指向空即可 头部删除 把链表的头节点设为原先头节点的next指针即可 中间删除 把要删除节点的前置节点的next指针，指向要删除的元素的下一个节点即可 1.3 数组VS链表 查找 更新 插入 删除 数组 O(1) O(1) O(n) O(n) 链表 O(n) O(1) O(1) O(1) 数组的优势在于能够快速的定位元素，对于读操作多、写操作少的场景，用数组更合适。 链表的优势在于能够灵活的进行插入和删除操作，如果需要在尾部频繁插入，删除元素，用链表更合适一些。、 2. 栈和队列物理结构、逻辑结构： 2.1 栈栈是一种线性数据结构，栈内元素只能先入后出。最早进入的元素存放的位置叫做栈底，最后进入的元素存放的位置叫做栈顶。 2.1.1 栈的基本操作入栈和出栈的时间复杂度都是O(1) 入栈（push） 只允许从栈顶一侧放入数据，新元素会成为栈顶 出栈（pop） 只有栈顶元素才允许出栈，出栈元素的前一个元素将会成为新的栈顶 2.2 队列对列是一种线性数据结构，队列中的元素只能先入先出，队列的出口端叫做队头，队列的入口端叫做队尾 2.2.1 队列的基本操作入队和出队的时间复杂度为O(1) 入队（enqueue） 出队（dequeue） 用数组实现的队列可以采用循环队列的方式来维持队列容量的恒定。 假设一个队列经过反复的入队和出队操作，还剩下两个元素，在物理上 分布于数组的末尾位置。这是又有一个新元素要入队 在数组不做扩容的前提下，我们可以利用已出队元素留下的空间，让队尾指针重新指回数组的首位。 这样一来整个队列的元素就”循环”起来了。在物理存储上，队尾的位置也可以在对头之前。当再有元素入队时，将其放在数组首位，队尾指针继续后移即可。 一直到(队尾下标 + 1) % 数组长度 = 对头下标 时，代表队列真的已经满了，需要注意的是，队尾指针指向的位置永远空出一位，所以队列最大容量比数组长度小1。 2.3 栈和队列的应用2.3.1 栈的应用栈的输出顺序和输入顺序相反，所以栈通常用于对历史的回溯 例如实现递归的逻辑，就可以用栈来代替，因为栈可以回溯方法的调用链 栈还有一个著名的应用场景是面包屑导航。 2.3.2 队列的应用队列的输出顺序和输入顺序相同，所以队列通常用于对历史的回放。 在多线程中，争夺公平锁的等待队列 网络爬虫实现网站抓取，把待抓取的url存入队列 2.3.3 双端队列双端队列这种数据结构，综合了栈和队列的优点，对双端队列来说，从对头的一端可以入队或出队，从队尾的一端也可以入队或出队 2.3.4 优先队列遵循谁的优先级最高，谁先出队。 3. 散列表散列表也叫做哈希表，这种数据结构提供了key和value的映射关系，只要给出一个key，就可以高效的查找它所匹配的value，时间复杂度接近于O(1)。 3.1 哈希函数通过某种方式，把key和数组下标进行转换。 3.2 散列表的读写操作 写操作 就是在散列表中插入新的键值对。 如调用hashMap.put(&quot;002931&quot;, &quot;王五&quot;)，意思就是插入一组key为002931、value为王五的键值对。 具体步骤： 通过哈希函数，把key转化成数组下标5 如果数组下标5对应的位置没有元素，就把这个Entry填充到数组下标5的位置。 但是，由于数组的长度是有限的，当插入的Entry越来越多时，不同的key通过哈希函数获得的下标有可能是相同的。例如002936这个key对应的数组下标是2； 002947这个key对应的数组下标也是2。这就产生了哈希冲突。 解决哈希冲突的方法有两种，一种开放寻址法，一种是链表法。 开放寻址法 当一个key通过哈希函数获得对应的数组下标已被占用时，可以去寻找下一个空挡位置。 以上面的情况为例，Entry6通过哈希函数得到下标2，该下标在数组中已经有了其他元素，那么就向后移动1位，看看数组下标3的位置是否有空 很不巧，下标3也已经被占用，那么久再向后移动一位，看看数组下标4的位置是否有空。 幸运的是，数组下标4的位置还没有被占用，因此把Entry6存入下标4的位置 这就是开放寻址法的基本思路。 链表法 这种方法被应用在java的集合类HashMap中 HashMap数组的每一个元素不仅是一个Entry对象，还是一个链表的头节点。每一个Entry对象通过next指针指向它的下一个Entry节点。当新来的Entry映射到与之冲突的数组位置时，只需要插入到对应的链表中即可。 读操作 读操作就是通过给定的key，在散列表中查找对应的value。 例如：调用hashMap.get(&quot;002936&quot;)，意思是查找key为002936的Entry在散列表中所对应的值。以链表法为例 通过哈希函数，把key转化成数组的下标2 找到数组下标2所对应的元素，如果这个元素的key是002936那么久找到了；如果不是，由于数组的每个元素都与一个链表对应，我们可以顺着链表慢慢往下找。 上图中，首先查到的节点Entry6的key是002974，和待查找的key002936不符，接着定位到列表下一个节点Entry1，发现Entry1的key002936正是我们要寻找的，所以返回Entry1的value即可。 扩容（resize） 当经过多次元素插入，散列表达到一定饱和时，key映射位置发生冲突的概率会逐渐提高。这样一来，大量的元素拥挤在相同的数组下标位置，形成很长的链表，对后续插入操作和查询操作的性能都有很大影响。 这时，散列表就需要扩展它的长度，也就是进行扩容。 对于JDK中的散列表实现类HashMap来说，影响其扩容的因素有两个。 Capacity，即HashMap的当前长度 LoadFactor，即HashMap的负载因子，默认值为0.75f 衡量HashMap需要进行扩容的条件如下。 HashMap.Size &gt;= Capacity × LoadFactor 扩容的步骤： 扩容，创建一个新的Entry空数组，长度是原数组的2倍。 重新Hash，遍历原Entry数组，把所有的Entry重新hash到新数组中。为什么要重新Hash呢？因为长度扩大以后，Hash的规则也随之改变。 经过扩容，原本拥挤的散列表重新变得稀疏，原有的Entry也重新得到了尽可能均匀的分配。 注python中的list在内存中是连续的，之所以不需要考虑扩容的问题，是因为list是动态扩容。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[复杂度]]></title>
    <url>%2F2019%2F11%2F19%2Falgorithm%5B%E7%AE%97%E6%B3%95%5D%2F3.%E5%A4%8D%E6%9D%82%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[简单来说，时间复杂度是执行算法的时间成本，空间复杂度是执行算法的空间成本。 时间复杂度定义：若存在函数f(n)，使得当n趋近于无穷大时，T(n)/f(n)的极限值为不等于零的常数，则称f(n)是T(n)的同数量级函数。记作T(n)=O(f(n))，O为算法的渐进时间复杂度，简称为时间复杂度。 注意： 如果运行时间是常数量级，则用常数1表示。 只保留时间函数中的最高阶项。 如果最高阶项存在，则省去最高阶项前面的系数。 空间复杂度空间复杂度是对一个算法在运行过程中临时占用存储空间大小的量度。 公式记作：S(n)=O(f(n))，其中n为问题的规模，f(n)为算法所占存储空间的函数。 常见的空间复杂度 常量空间 当算法的存储空间大小固定，和输入规模完全没有直接的关系时，空间复杂度记作O(1) 1234viod fun1(int n)&#123; int var = 3; ...&#125; 线性空间 当算法分配的空间是一个线性的集合（如数组），并且集合大小和输入规模n成正比时，空间复杂度记作O(n) 1234void fun2(int n)&#123; int[] array = new int[n]; ...&#125; 二维空间 当算法分配的空间是一个二维数组集合，并且集合的长度和宽度都与输入规模n成正比时，空间复杂度记作O(n^2) 1234void fun3(int n) &#123; int[][] matrix = new int[n][n]; ...&#125; 递归空间 计算机在执行这类程序时，会专门分配一块内存，用来存储“方法调用栈”。 “方法调用栈”包括进栈和出栈两个行为。 当进入一个新方法时，执行入栈操作，把调用的方法和参数信息压入栈中。 当方法返回时，执行出栈操作，把调用的方法和参数信息从栈中弹出。 1234567void fun4(int n)&#123; if(n&lt;=1)&#123; return; &#125; fun4(n-1); ...&#125; 如果初始值传入的参数值为n=5，下图为最终所需要的内存空间，递归结束，全部元素一一出栈。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[javascript 函数]]></title>
    <url>%2F2019%2F11%2F11%2Fjavascript%2F01-%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[构造器函数在一个函数前面带上new来调用，那么将创建一个隐藏的连接到该函数的prototype成员的新对象，同时this将会被绑定到那个新对象上。new 前缀也会改变return语句的行为1234567891011121314// 创建一个名为Quo的构造器函数，它构造一个带有status属性的对象var Quo = function (string) &#123; this.status = string;&#125;// 给Quo的所有实例提供一个名为get_status的公共方法Quo.prototype.get_status = function() &#123; return this.status;&#125;// 构造一个Quo实例var myQuo = new Quo("confused")console.log(myQuo.get_status());// confused 其目的就是结合new前缀调用的函数被称为构造器函数。按照约定，他们保存在以大写格式命名的变量里。如果调用构造器函数时没有在前面加上new，不会有任何报错和警告，但是在调用的时候会出现问题。 Apply 调用模式apply 方法让我们构建一个参数数组并用其去调用函数。他也允许我们选择this的值，apply方法接收两个参数，第一个是将被绑定给this的值，第二个就是一个参数数组。 123456789101112131415// 构造一个包含两个数字的数组，并将它们相加var add = function (a, b) &#123; return a + b;&#125;var array = [3, 4];var sum = add.apply(null, array);// 构造一个包含status成员的对象var statusObject = &#123; status: 'A-OK'&#125;// statusObject并没有继承自 Quo.prototype，但我们可以在statusObject上调用get_status方法，尽管statusObject并没有一个名为get_status的方法var status = Quo.prototype.get_status.apply(statusObject);// status 值为 'A-OK' 参数当函数被调用时，会得到一个免费奉送的参数，那就是arguments数组，通过它函数可以访问所有它被调用时传递给它的参数列表，包括那些没有被分配给函数声明时定义的形式参数的多余参数。 123456789var sum = function() &#123; var i, sum = 0; for (i = 0; i &lt; arguments.length; i+= 1) &#123; sum += arguments[i]; &#125; return sum;&#125;console.log(sum(1,2,3,4,5))// 15 arguments 并不是一个真正的数组。他只是一个类似数组(array-like)的对象。arguments拥有一个length属性，但它缺少所有的数组方法。 给类型增加方法javascript允许给语言的基本类型增加方法。 我们可以通过给Function.prototype增加方法来使得该方法对所有函数可用 1234Function.prototype.method = function (name, func) &#123; this.prototype[name] = func; return this;&#125; 通过给Function.prototype增加一个method方法，我们就不必键入prototype这个属性名。 javascript并没有单独的整数类型，因此有时候只提取数字中的整数部分是有必要的。我们可以通过给Number.prototype添加一个integer方法来改善它。他会根据数字的正负来判断是使用Math.ceiling还是Math.floor。 12345Number.method('integer', function()&#123; return Math[this &lt; 0 ? 'ceil' : 'floor'](this);&#125;)console.log((-10/3).integer())// -3 注意：基础类型的原型是公共的结构，所以在类库混用时务必小心，这种操作需要在确定没有该方法的时候才能添加 闭包123456789101112// 创键一个名为quo的构造函数// 它构造出带有get_status方法和status私有属性的一个对象var quo = function(status) &#123; return &#123; get_status: function ()&#123; return status; &#125; &#125;;&#125;;var myQuo = quo('amazed');console.log(myQuo.get_status);// amazed 这个quo函数被设计成无须在前面加上new来使用，所以名字也没有首字母大写。当我们调用quo时，它返回包含get_status方法的一个新对象。该对象的一个引用保存在myQuo中。即使quo已经返回了，但get_status方法仍然享有访问quo对象的status属性的特权。get_status方法并不是访问该参数的一个拷贝；它访问的就是该参数本身。这是可能的，因为该函数可以访问它被创建时所处的上下文环境，这被称为闭包。 下面一个例子： 12345678910111213var fade = function (node) &#123; var level = 1; var step = function () &#123; var hex = level.toString(16); node.style.backgroundColor = '#FFFF' + hex + hex; if (level &lt;15) &#123; level += 1; setTimeout(step, 100); &#125; &#125;; setTimeout(step, 100);&#125;;fade(document.body) 我们调用fade，把document.body作为参数传递给它（HTML\标签所创建的节点）。fade函数设置level为1。它定义了一个step函数，接着调用setTimeout，并传递step函数和一个时间（100毫秒）给它，然后返回。 在大约十分之一秒后step函数被调用，它把fade函数的level变量转化为16位字符。接着，它修改fade函数得到的节点的背景颜色。然后查看fade函数的level变量。如果背景色尚未变成白色，那么它增大fade函数的level变量和用setTimeout预订让它自己再次运行。 step函数很快再次被调用，但这次，fade函数的level变量值变成2，fade函数在之前已经返回了，但只要fade的内部函数需要，它的变量就会持续保留。]]></content>
      <categories>
        <category>javascript</category>
      </categories>
      <tags>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB修改集群IP]]></title>
    <url>%2F2019%2F11%2F05%2Fdb%2Fmongodb%2F06%E3%80%81mongodb%E4%BF%AE%E6%94%B9%E9%9B%86%E7%BE%A4ip%2F</url>
    <content type="text"><![CDATA[修改集群ip以单节点模式启动数据库mongo 192.168.1.35:27018123456&gt; use local&gt; db.system.replset.find()&#123; "_id" : "lcc2", "version" : 1, "protocolVersion" : NumberLong(1), "members" : [ &#123; "_id" : 1, "host" : "xx.xx.xx.xx:29012", "arbiterOnly" : false, "buildIndexes" : true, "hidden" : false, "priority" : 3, "tags" : &#123; &#125;, "slaveDelay" : NumberLong(0), "votes" : 1 &#125;, &#123; "_id" : 2, "host" : "xx.xx.xx.xx:29012", "arbiterOnly" : false, "buildIndexes" : true, "hidden" : false, "priority" : 2, "tags" : &#123; &#125;, "slaveDelay" : NumberLong(0), "votes" : 1 &#125;, &#123; "_id" : 3, "host" : "xx.xx.xx.xx:29012", "arbiterOnly" : false, "buildIndexes" : true, "hidden" : false, "priority" : 1, "tags" : &#123; &#125;, "slaveDelay" : NumberLong(0), "votes" : 1 &#125; ], "settings" : &#123; "chainingAllowed" : true, "heartbeatIntervalMillis" : 2000, "heartbeatTimeoutSecs" : 10, "electionTimeoutMillis" : 10000, "getLastErrorModes" : &#123; &#125;, "getLastErrorDefaults" : &#123; "w" : 1, "wtimeout" : 0 &#125;, "replicaSetId" : ObjectId("5c24cbaa2045a8fe36840702") &#125; &#125;// 修改为新集群IP&gt; cfg=&#123; "_id" : "lcc2", "version" : 1, "protocolVersion" : NumberLong(1), "members" : [ &#123; "_id" : 1, "host" : "xx.xx.xx.xx:29012", "arbiterOnly" : false, "buildIndexes" : true, "hidden" : false, "priority" : 3, "tags" : &#123; &#125;, "slaveDelay" : NumberLong(0), "votes" : 1 &#125;, &#123; "_id" : 2, "host" : "xx.xx.xx.xx:29012", "arbiterOnly" : false, "buildIndexes" : true, "hidden" : false, "priority" : 2, "tags" : &#123; &#125;, "slaveDelay" : NumberLong(0), "votes" : 1 &#125;, &#123; "_id" : 3, "host" : "xx.xx.xx.xx:29012", "arbiterOnly" : false, "buildIndexes" : true, "hidden" : false, "priority" : 1, "tags" : &#123; &#125;, "slaveDelay" : NumberLong(0), "votes" : 1 &#125; ], "settings" : &#123; "chainingAllowed" : true, "heartbeatIntervalMillis" : 2000, "heartbeatTimeoutSecs" : 10, "electionTimeoutMillis" : 10000, "getLastErrorModes" : &#123; &#125;, "getLastErrorDefaults" : &#123; "w" : 1, "wtimeout" : 0 &#125;, "replicaSetId" : ObjectId("5c24cbaa2045a8fe36840702") &#125; &#125;&gt; db.system.replset.update(&#123;"_id":"lcc2”&#125;,cfg) 然后重启 修改副本集ip添加副本，在登录到主节点下输入 1rs.add(&quot;ip:port&quot;); 删除副本 1rs.remove(&quot;ip:port&quot;); 新增仲裁节点 1rs.addArb(&quot;ip:port&quot;); 修改副本host 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869shard1:PRIMARY&gt; cfg = rs.conf()&#123; "_id" : "shard1", "version" : 5, "protocolVersion" : NumberLong(1), "members" : [ &#123; "_id" : 0, "host" : "127.0.0.1:2777", "arbiterOnly" : false, "buildIndexes" : true, "hidden" : false, "priority" : 1, "tags" : &#123; &#125;, "slaveDelay" : NumberLong(0), "votes" : 1 &#125; ], "settings" : &#123; "chainingAllowed" : true, "heartbeatIntervalMillis" : 2000, "heartbeatTimeoutSecs" : 10, "electionTimeoutMillis" : 10000, "getLastErrorModes" : &#123; &#125;, "getLastErrorDefaults" : &#123; "w" : 1, "wtimeout" : 0 &#125;, "replicaSetId" : ObjectId("5d9c7a7e76695600e03e231f") &#125;&#125;shard1:PRIMARY&gt; cfg.members[0].host = "10.13.10.2:2777"10.130.10.72:2777shard1:PRIMARY&gt; rs.reconfig(cfg)&#123; "ok" : 1 &#125;shard1:PRIMARY&gt; rs.status()&#123; "set" : "shard1", "date" : ISODate("2019-10-09T02:59:26.916Z"), "myState" : 1, "term" : NumberLong(1), "heartbeatIntervalMillis" : NumberLong(2000), "members" : [ &#123; "_id" : 0, "name" : "10.130.10.72:2777", "health" : 1, "state" : 1, "stateStr" : "PRIMARY", "uptime" : 54711, "optime" : &#123; "ts" : Timestamp(1570589961, 1), "t" : NumberLong(1) &#125;, "optimeDate" : ISODate("2019-10-09T02:59:21Z"), "electionTime" : Timestamp(1570536062, 2), "electionDate" : ISODate("2019-10-08T12:01:02Z"), "configVersion" : 6, "self" : true &#125; ], "ok" : 1&#125;]]></content>
      <categories>
        <category>数据库</category>
        <category>mongoDB</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>服务器</tag>
        <tag>数据库</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB小技巧]]></title>
    <url>%2F2019%2F11%2F04%2Fdb%2Fmongodb%2F05%E3%80%81mongodb%E5%B0%8F%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[update $push 数组指定位置插入数据 1test.update(&#123;'_id': '0'&#125;, &#123;'$push': &#123;'opId': &#123;'$each': ['1'], '$position': 0&#125;&#125;&#125;) 关联查询 unwind：list做分割处理，每有一个元素，就多一个独立的整体 lookup：管道 facet：多个管道 格式： { $facet: { test1: [...], test2: [...] } } 12345678910user.aggregate([ &#123;$match: ''&#125;, &#123;$lookup: &#123; from: user, localField: postId, foreignField: _id, as: posts &#125;&#125;, &#123;$unwind: $posts&#125;])]]></content>
      <categories>
        <category>数据库</category>
        <category>mongoDB</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>数据库</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R语言基础]]></title>
    <url>%2F2019%2F10%2F20%2FR%2F01_R-%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[R作为一种统计分析软件，是集统计分析与图形显示于一体的。 它可以运行于UNIX,Windows和Macintosh的操作系统上，而且嵌入了一个非常方便实用的帮助系统，相比于其他统计分析软件，R还有以下特点： R是自由软件。 这意味着它是完全免费,开放源代码的。可以在它的网站及其镜像中下载任何有关的安装程序、源代码、程序包及其源代码、文档资料。标准的安装文件身自身就带有许多模块和内嵌统计函数,安装好后可以直接实现许多常用的统计功能。 R是一种可编程的语言。 作为一个开放的统计编程环境,语法通俗易懂,很容易学会和掌握语言的语法。而且学会之后,我们可以编制自己的函数来扩展现有的语言。这也就是为什么它的更新速度比一般统计软件,如,SPSS,SAS等快得多。大多数最新的统计方法和技术都可以在R中直接得到。 所有R的函数和数据集是保存在程序包里面的。 只有当一个包被载入时,它的内容才可以被访问。一些常用、基本的程序包已经被收入了标准安装文件中,随着新的统计分析方法的出现,标准安装文件中所包含的程序包也随着版本的更新而不断变化。在另外版安装文件中,已经包含的程序包有：base一R的基础模块、mle一极大似然估计模块、ts一时间序列分析模块、mva一多元统计分析模块、survival一生存分析模块等等. R具有很强的互动性。 除了图形输出是在另外的窗口处，它的输入输出窗口都是在同一个窗口进行的，输入语法中如果出现错误会马上在窗口口中得到提示,对以前输入过的命令有记忆功能,可以随时再现、编辑修改以满足用户的需要。输出的图形可以直接保存为JPG,BMP,PNG等图片格式,还可以直接保存为PDF文件。另外,和其他编程语言和数据库之间有很好的接口。 数学运算 加 12&gt; 5+5[1] 10 减 12&gt; 5-5[1] 0 乘 12&gt; 3*5[1] 15 除 12&gt; (3+4)/2[1] 3.5 乘方 12&gt; 2^3[1] 8 取余 12&gt; (3+4)%%2[1] 1 赋值使用&lt;-作为赋值号 c()可以把多个对象放在一起，组成向量 123456&gt; a&lt;-123&gt; a[1] 123&gt; l &lt;- c(1,2)&gt; l[1] 1 2 使用帮助文档 函数 功能 help.start() 打开帮助文档首页 help(“foo”)或?foo 查看函数foo的帮助 help.search(“foo”)或??foo 以foo为关键词搜索本地帮助文档 example(“foo”) 函数foo的使用示例 RSiteSearch(“foo”) 以foo为关键词搜索在线文档和邮件列表存档 apropos(“foo”, mode=”function”) 列出名称中含有foo的所有可用函数 data() 列出当前已加载包中所含的所有可用示例数据集 vignette() 列出当前已安装包中所有可用的vignette文档 vignette(“foo”) 为主题foo显示指定的vignette文档 功能包 search()：查看当前已加载的包 install.packages(&quot;dplyr&quot;) 安装包 update.packages() 更新包 library(&quot;dplyr&quot;)载入包 数据类型 数值型 numerics 逻辑判断 logical 字符型 characters 因子 factors 注：不同类型不能相加减 使用class()来判断数据类型 常用函数 length(object)：显示对象中元素/成分的数量 12345678&gt; a &lt;- "My First List"&gt; b &lt;- c(25, 26, 18, 39)&gt; c &lt;- matrix(1:10, nrow=5)&gt; d &lt;- c("one", "two", "three")&gt; length(a)[1] 1&gt; length(d)[1] 3 dim(object)：显示某个对象的维度 str(object)：显示某个对象的结构 123456&gt; str(mylist)List of 4 $ title: chr "My First List" $ : num [1:4] 25 26 18 39 $ : int [1:5, 1:2] 1 2 3 4 5 6 7 8 9 10 $ : chr [1:3] "one" "two" "three" class(object)：显示某个对象的类或类型 mode：显示某个对象的模式 names(object)：显示某对象中各成分的名称 12&gt; names(mylist)[1] "title" "" "" "" c(object, object...)：将对象合并入一个向量 cbind(object, object...)：按列合对象 12345678910&gt; students[1] "A" "B" "C" "D"&gt; math_score[1] 100 80 70 95&gt; cbind(students, math_score) students math_score[1,] "A" "100" [2,] "B" "80" [3,] "C" "70" [4,] "D" "95" rbind(object, object...)：按行合并对象 1234&gt; rbind(students, math_score) [,1] [,2] [,3] [,4]students "A" "B" "C" "D" math_score "100" "80" "70" "95" head(object)：列出某个对象的开始部分 tail(object)：列出某个对象的最后部分 ls()：显示当前的对象列表 rm(object, object...)：删除一个或更多个对象 删除当前工作环境的所有对象 123&gt; rm(list=ls())&gt; ls()character(0) newobject &lt;- edit(object)：编辑对象并另存为newobject fix(object)：直接编辑对象 统计 mean()：均值 median()：中位数 众数 R中未提供直接调用的函数 names(table(mtcars$mpg))[which.max(table(mtcars$mpg))] quantile()百分位数 var()：样本方差 range()：极差 summary()： 可以提供最小值、最大值、四分位数和数值型变量的均值，以及因子向量和逻辑型向量的频数统计 sd()：标准差 cor(object, object)：线性关系，相关度 工作空间 函数 功能 getwd() 显示当前的工作目录 setwd(“myditectory”) 修改当前的工作目录为mydirectory ls() 列出当前工作空间中的对象 rm(objectlist) 移除（删除）一个或多个对象 help(options) 显示可用选项的说明 options() 显示或设置当前选项 history(#) 显示最近使用过的#个命令（默认是25个） savehistory(“myfile”) 保存命令历史到文件myfile中（默认值为.Rhistory） loadhistory(“myfile”) 载入一个命令历史文件（默认值为.Rhistory） save.image(“myfile”) 保存工作空间到文件myfile中（默认是.RData） save(objectlist, file=”myfile”) 保存指定对象到一个文件中 load(“myfile”) 读取一个工作空间到当前会话中（默认是.RData） q() 退出R。将会询问你是否保存工作空间。 注意： setwd()命令的路径中使用正斜杠（/），R将反斜杠作为一个转义符。即使在windows平台上，路径也要使用正斜杠。 setwd()不会自动创建一个不存在的目录，必要的话，使用dir.create()来创建新目录，然后使用setwd()将工作目录指向这个新目录 输入和输出使用键盘输入数据123456789&gt; mydata &lt;- data.frame(age=numeric(0), gender=character(0), weight=numeric(0))&gt; mydata &lt;- edit(mydata)Warning message:In edit.data.frame(mydata) : 在&apos;gender&apos;里加上了因子水准&gt; mydata age gender weight1 2 3 22 2 2 23 1 2 1 从带分隔符的文本文件中导入数据可以使用read.table() mydata &lt;- read.table(file, header=logical_value, sep=&quot;delimiter&quot;, row.names=&quot;name&quot;) 12&gt; grades &lt;- read.table("studentsgrades.csv", header=TRUE, sep=",", row.names="STUDENTID")&gt; #从当前目录中读入了一个名为studentsgrades.csv的逗号分隔文件，从文件的第一行取得了各变量名称，将变量STUDENTID指定为行标识符，最后将结果保存到了名为grades的数据框中 注意： 参数sep可以为一个或多个的空格，制表符，换行符或回车符 默认情况下，字符型变量转换为因子。我们并不是总是希望程序这样做，可以设置选项stringsAsFactors=FALSE，这样将停止对所有字符型变量的此种转换；另一种方法是使用选项colClasses为每一列指定一个类。 导入excel可以使用RODBC包来访问Excel文件 12345install.packages("RODBC")library(RODBC)channel &lt;- odbcConnectExcel("myfile.xls")mydataframe &lt;- sqlFetch(channel, "mysheet")odbcClose(channel) xlsx包可以用来读取xlsx文件 123library(xlsx)workbook &lt;- &quot;c:/myworkbook.xlsx&quot;mydataframe &lt;- read.xlsx(workbook, 1) 访问数据库管理系统 ODBC接口 install.packages(&quot;RODBC&quot;) | 函数 | 描述 || ————————————————————————————— | ——————————————————————————- || odbcConnect(dsn, uid=&quot;&quot;, pwd=&quot;&quot;) | 建立一个到ODBC数据库的连接 || sqlFetch(channel, sqltable) | 读取ODBC数据库中的表到一个数据框中 || sqlQuery(channel, query) | 向ODBC数据库提交一个查询并返回结果 || sqlSave(channel, mydf, tablename=sqltable, append=FALSE) | 将数据框写入或更新(append=TRUE)到ODBC数据库的某个表中 || sqlDrop(channel, sqltable) | 删除ODBC数据库中的某个表 || close(channel) | 关闭连接 | RODBC包允许R和一个通过ODBC连接的SQL数据库之间进行双向通信。 例：将数据库的两个表（Crime和Punishment）分别导入为R中的两个名为crimedat和pundat的数据框 123456&gt; library(RODBC)&gt; myconn &lt;- odbcConnect("mydsn", uid="Rob", pwd="aardvark")&gt; crimedat &lt;- sqlFetch(myconn, Crime)&gt; pundat &lt;- sqlQuery(myconn, "select * from Punish")&gt; close(myconn)&gt; # 这里首先通过一个已注册的数据源名称(mydsn)和用户名(rob)以及密码(aardvark)打开一个ODBC数据库连接。连接字符串并传递给sqlFetch，它将Crime表复制到R数据框crimedat中，然后我们对Punishment执行了SQL语句select并将结果保存到了数据框pundat中，最后关闭连接]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R-数据结构]]></title>
    <url>%2F2019%2F10%2F20%2FR%2F02_R-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[1. vector向量向量是用于存储数值型、字符型或逻辑性数据的一维数组。执行组合功能函数c()可用来创建向量。123&gt; a &lt;- c(1,2,5,3,6,-2, 4)&gt; b &lt;- c("apple", "pear", "orange")&gt; c &lt;- c(TRUE, FALSE, TRUE, FALSE, TRUE) 这里，a是数值型向量，b是字符型向量，c是逻辑型向量 注意：单个向量中的数据必须拥有相同的类型或模式（数值型，字符型，逻辑型）。同一向量中无法混杂不同模式的数据。 123456&gt; b[1][1] "apple"&gt; b[c(1,3)][1] "apple" "orange"&gt; c[c(2:4)][1] FALSE TRUE FALSE 2. matrix 矩阵矩阵是一个二维数组，只是每个元素都拥有相同的模式（数值型，字符型或逻辑型）。可以通过函数matrix()创建矩阵。 一般格式为mymatrix &lt;- matrix(vector, nrow=number_of_rows, ncol=number_of_columns, byrow=logical_value) 其中vector包含了矩阵的元素，nrow和ncol用以指定行和列的维数，dimnames包含了可选的、以字符型向量表示的行名和列名。选项byrow则表明矩阵应当按行填充（byrow=TRUE）还是按列填充（byrow=FALSE）,默认情况下按列填充。 123456789101112131415161718192021222324252627&gt; myMatrix &lt;- matrix(1:15, nrow=3, ncol=5)&gt; myMatrix [,1] [,2] [,3] [,4] [,5][1,] 1 4 7 10 13[2,] 2 5 8 11 14[3,] 3 6 9 12 15&gt; y &lt;- matrix(1:20, nrow = 5, ncol=4)&gt; y [,1] [,2] [,3] [,4][1,] 1 6 11 16[2,] 2 7 12 17[3,] 3 8 13 18[4,] 4 9 14 19[5,] 5 10 15 20&gt; cells &lt;- c(1, 26, 24, 68)&gt; rnames &lt;- c("R1", "R2")&gt; cnames &lt;- c("C1", "C2")&gt; mymatrix &lt;- matrix(cells, nrow = 2, ncol = 2, byrow=TRUE, dimnames=list(rnames, cnames)) # 按行填充&gt; mymatrix C1 C2R1 1 26R2 24 68&gt; mymatrix &lt;- matrix(cells, nrow=2, ncol=2, byrow=FALSE, dimnames = list(rnames, cnames))# 按列填充&gt; mymatrix C1 C2R1 1 24R2 26 68 我们可以使用下标和方括号来选择矩阵中的行、列或元素。r X[i,]指矩阵X中的第i行，r X[,j]指第j列，r X[i, j]指第i行第j个元素。选择多行或多列时，下标i或j可为数值型向量 123456&gt; myMatrix[2,][1] 2 5 8 11 14&gt; myMatrix[,3][1] 7 8 9&gt; myMatrix[2,3][1] 8 矩阵都是二维的，和向量类似，矩阵中也仅能包含一种数据类型。 3. dataframe数据框不同的列可以包含不同模式的数据。 mydata &lt;- data.frame(col1, col2, col3) 注意：列与列之间数据类型可以不一样，但是每一列的数据类型必须一致。我们在讨论数据框时将交替使用数据列和变量。选取数据框中元素的方式：可以使用前术（如矩阵中的）下标记号，也可直接使用执行列名 1234567891011121314151617181920&gt; students &lt;- c("A", "B", "C", "D")&gt; math_score &lt;- c(100, 80, 70, 95)&gt; english_score &lt;- c(96, 86, 77, 99)&gt; students_scores &lt;- data.frame(students, math_score, english_score)&gt; students_scores students math_score english_score1 A 100 962 B 80 863 C 70 774 D 95 99&gt; students_scores[1,] students math_score english_score1 A 100 96&gt; students_scores[,2][1] 100 80 70 95&gt; students_scores[,"math_score"][1] 100 80 70 95&gt; students_scores$math_score[1] 100 80 70 95 $用来选取一个给定数据框中的某个特定的变量。 1234567&gt; math &lt;- data.frame(students_scores$students, students_scores$math_score)&gt; math students_scores.students students_scores.math_score1 A 1002 B 803 C 704 D 95 attach(), detach(), with() 函数attch()可以将数据框添加到R的搜索路径中，R在遇到一个变量名以后，将检查搜索路径中的数据框，以定位到这个变量。 12345&gt; attach(students_scores) summary(math_score) plot(math_score) plot(math_score) detach(students_scores) detach()将数据框从搜索路径中移除。 1234&gt; with(students_scores, &#123; summary(math_score, english_score) plot(students, math_score)&#125;) with()的局限性在于，赋值仅在此函数的括号内生效 如果需要创建在with()结构以外存在的对象，使用特殊赋值符&lt;&lt;-代替标准赋值符&lt;-。 4. 因子factor因子factor又叫分类变量，有两种：名义型、有序型 名义型：没有顺序 有序型：有顺序关系但是没有数量关系，得不出来不同级别之间的差异具体是多少。 1234567891011&gt; excellence &lt;- c("excellent", "bad", "good", "okay", "bad")&gt; excellence &lt;- factor(excellence)&gt; excellence[1] excellent bad good okay bad Levels: bad excellent good okay&gt; excellence &lt;- factor(excellence, order=TRUE, levels=c("bad", "okay", "good", "excellent"))&gt; excellence[1] excellent bad good okay bad Levels: bad &lt; okay &lt; good &lt; excellent&gt; str(excellence) # 显示对象结构 Factor w/ 4 levels "bad","excellent",..: 2 1 3 4 1 这里我们成功的把字符型变量excellence，先转换成了无序因子变量，再转换成了有顺序的因子变量。 数值型变量可以用levels和labels参数来编码成因子。如果男性被编码成1，女性被编码成2 1234567&gt; sex &lt;- c(1, 2, 2, 1, 2, 1, 1, 3)&gt; sex[1] 1 2 2 1 2 1 1 3&gt; sex &lt;- factor(sex, levels=c(1, 2), labels=c("Male", "Female"))&gt; sex[1] Male Female Female Male Female Male Male &lt;NA&gt; Levels: Male Female 5. list列表列表是R数据类型中最为复杂的一种。 一般来说，列表就是一些对象（或成分，component）的有序集合。列表允许你整合若干（可能无关的）对象到单个对象名下。例如：某个列表中可能是若干向量、矩阵、数据框，甚至其他列表的组合。可以使用list()创建列表 1234567891011121314151617181920212223242526&gt; a &lt;- "My First List"&gt; b &lt;- c(25, 26, 18, 39)&gt; c &lt;- matrix(1:10, nrow=5)&gt; d &lt;- c("one", "two", "three")&gt; mylist &lt;- list(title=a, b, c, d)&gt; mylist$title[1] "My First List"[[2]][1] 25 26 18 39[[3]] [,1] [,2][1,] 1 6[2,] 2 7[3,] 3 8[4,] 4 9[5,] 5 10[[4]][1] "one" "two" "three"&gt; mylist[[2]][1] 25 26 18 39&gt; mylist[["title"]][1] "My First List" 上述创建了一个列表，其中有四个成：一个字符串，一个数据型向量，一个矩阵以及一个字符串向量。可以组合任意多的对象，并将它们保存为一个列表。 可以通过在双重方括号中指明代表某个成分的数字或名称来访问列表中的元素。 6. 数组数组与矩阵类似，但是维度可以大于2。 myarray &lt;- array(vector, dimensions, dimnames) vector：数据 dimensions：数值型向量，给出了各个维度下标的最大值 dimnames：可选，各维度名称标签的列表 1234567891011121314151617181920212223242526272829303132&gt; dim1 &lt;- c("A1", "A2")&gt; dim2 &lt;- c("B1", "B2", "B3")&gt; dim3 &lt;- c("C1", "C2", "C3", "C4")&gt; z &lt;- array(1:24, c(2,3,4), dimnames = list(dim1, dim2, dim3))&gt; a[1] 1 2 5 3 6 -2 4&gt; z, , C1 B1 B2 B3A1 1 3 5A2 2 4 6, , C2 B1 B2 B3A1 7 9 11A2 8 10 12, , C3 B1 B2 B3A1 13 15 17A2 14 16 18, , C4 B1 B2 B3A1 19 21 23A2 20 22 24&gt; z[1,2,3][1] 15 注意 将一个值赋给某个变量、矩阵、数组或列表中一个不存在的元素时，R将自动扩展这个数据结构以容纳新值 1234&gt; x &lt;- c(8, 6, 4)&gt; x[7] &lt;- 10&gt; x[1] 8 6 4 NA NA NA 10 R中没有标量。标量以单元素向量的形式出现 R中的下标不从0开始，从1开始 变量无法被声明，他们在首次被赋值时生成]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R-图形初阶]]></title>
    <url>%2F2019%2F10%2F20%2FR%2F03_R-%E5%9B%BE%E5%BD%A2%2F</url>
    <content type="text"><![CDATA[一个简单的例子1234&gt; dose &lt;- c(20, 30, 40, 45, 60)&gt; drugA &lt;- c(16, 20, 27, 40, 60)&gt; drugB &lt;- c(15, 18, 25, 31, 40)&gt; plot(dose, drugA, type="b") plot()是R中为对象作图的一个泛型函数（它的输出将根据所绘制对象类型的不同而变化）。 上例中，plot(x, y, type=&quot;b&quot;)将x置于横轴，将y置于纵轴，绘制点集(x, y)，然后使用线段将其链接。选项type=&quot;b&quot;表示同时绘制点和线。 图形参数我们可以通过修改称为图形参数的选项来自定义一幅图形的多个特性（字体、颜色、坐标轴、标题）。 （修改图形参数的）一种方法是通过函数par()来指定这些选项。以这种方式设定的参数值，除非再次修改，否则将在会话结束前一直有效。 格式为：par(optionname=value, optionname=name,...)。不加参数地执行par()将生成一个含有当前图形参数设置的列表。添加参数no.readonly=TRUE可以生成一个可以修改的当前图形参数的列表。 12345&gt; # 使用实心三角作为点的符号，虚线代替实线&gt; opar &lt;- par(no.readonly = TRUE) # 复制一份当前参数&gt; par(lty=2, pch=17) # lty=2 把实线改为虚线，pch=17把点改为实心三角&gt; plot(dose, drugA, type="b")&gt; par(opar) # 还原初始设置 指定图形参数的第二种方法是为高级绘图函数直接提供optionname=value的键值对。这种情况下，指定的选项仅对这幅图形本身有效。 plot(dose, drugA, type=&quot;b&quot;, lty=2, pch=17) 符号和线条 参数 描述 pch 指定绘制点时使用的符号 cex 指定符号的大小。cex是一个数值，表示绘图符号相对于默认大小的缩放倍数。默认大小是1, 1.5表示放大为默认值的1.5倍，0.5表示缩小为默认值的50% lty 指定线条类型 lwd 指定线条宽度。lwd是以默认值的相对大小来表示的（默认值为1） pch可指定的绘图符号如下图： lty可指定的线条类型： 颜色用于指定颜色的参数 参数 描述 col 默认的绘图颜色。某些函数（line和pie）可以接受一个含有颜色值的向量，并自动循环使用。例如，如果设定col=c(“red”, “blue”)并需要绘制三条线，则第一条将为红色，第二条为蓝色，第三条为红色 col.axis 坐标轴刻度文字的颜色 col.lab 坐标轴标签(名称)的颜色 col.main 标题的颜色 col.sub 副标题的颜色 fg 图形的前景色 bg 图形的背景色 在R中，可以通过颜色下标、颜色名称、十六进制的颜色值、RGB值或HSV值来指定颜色。举例来说，col=1、col=&quot;white&quot;、col=&quot;#FFFFFF、col=rgb(1, 1, 1)和col=hsv(0, 0, 1)都是表示白色的等价方式。函数rgb()可基于红-绿-蓝三色值生成颜色，而hsv()则基于色相-饱和度-亮度来生成颜色。 colors()可返回所有可用颜色的名称。 R中也有多种用于创建连续型颜色向量的函数，包括rainbow()，heat.colors()，terrain.colors()，topo.colors()，cm.colors()。举例来说，rainbow(10)可以生成10中连续的”彩虹型“颜色。多阶灰度可使用gray()函数生成。这时要通过一个元素值为0和1之间的向量来指定各颜色的灰度。gray(0:10/10)将生成10阶灰度色 12345&gt; n &lt;- 10&gt; mycolors &lt;- rainbow(n)&gt; pie(rep(1,n), labels=mycolors,col=mycolors)&gt; mygrays &lt;- gray(0:n/n)&gt; pie(rep(1,n), labels=mygrays, col=mygrays) 上述代码运行结果： 文本属性用于指定文本大小的参数： 参数 描述 cex 表示相对于默认大小缩放倍数的数值，默认大小为1，1.5表示放大为默认值的1.5倍，0.5表示缩小为默认值的50% cex.axis 坐标轴刻度文字的缩放倍数，类似cex cex.lab 坐标轴标签（名称）的缩放倍数，类似cex cex.main 标题的缩放倍数，类似cex cex.sub 副标题的缩放倍数，类似cex 用于指定字体族、字号和字样的参数 参数 描述 font 整数。用于指定绘图使用的字体样式。1=常规，2=粗体，3=斜体，4=粗斜体，5=符号字体 font.axis 坐标轴刻度文字的字体样式 font.lab 坐标轴标签（名称）的字体样式 font.main 标题的字体样式 font.sub 副标题的字体样式 ps 字体磅值。文本的最终大小为ps*cex family 绘制文本时使用的字体族。标准的取值为serif(衬线)，sans(无衬线) 举例：执行 par(font.lab=3, cex.lab=1.5, font.main=4, cex.main=2) 之后创建的所有图形都将拥有斜体、1.5倍于默认文本大小的坐标轴标签（名称），以及粗斜体、2倍于默认文本大小的标题。 在Windows中，可以通过函数windowsFont()来创建字体的映射 12345&gt; windowsFonts(+ A=windowsFont("Arial Black"),+ B=windowsFont("Bookman Old Style"),+ C=windowsFont("Comic Sans MS")+ ) 之后，即可使用A、B、C作为family的取值。 图形尺寸和边界尺寸用于控制图形尺寸和边界大小的参数 参数 描述 pin 以英寸表示的图形尺寸（宽和高） mai 以数值向量表示的边界大小，顺序为”下左上右”，单位为英寸 mar 以数值向量表示的边界大小，顺序为”下左上右”，单位为英寸。默认值为c(5, 4, 4, 2) + 0.1 代码：par(pin=c(4, 3), mai=c(1, .5, 1, .2))可生成一幅4英寸宽、3英寸高、上下边界为1英寸、左边界为0.5英寸、右边界为0.2英寸的图形。 123456789&gt; dose &lt;- c(20, 30, 40, 45, 60)&gt; drugA &lt;- c(16, 20, 27, 40, 60)&gt; drugB &lt;- c(15, 18, 25, 31, 40)&gt; opar &lt;- par(no.readonly = TRUE)&gt; par(pin=c(2, 3))&gt; par(lwd=2, cex=1.5)&gt; par(cex.axis=.75, font.axis=3)&gt; plot(dose, drugA, type="b", pch=19, lty=2, col="red")&gt; plot(dose, drugB, type="b", pch=23, lty=6, col="blue", bg="green") 添加文本、自定义坐标和图例以下代码在图形上添加了标题(main)，副标题(sub)，坐标轴标签(xlab, ylab)并指定了坐标轴范围(xlim, ylim) 1&gt; plot(dose, drugA, type="b", col="red", lty=2, pch=2, lwd=2, main="Clinical Trials for DrugA", sub="This is hypothetical data", xlab="Dosage", ylab="Drug Response", xlim=c(0, 60), ylim=c(0, 70)) 标题title(main=&quot;main title&quot;, sub=&quot;sub-title&quot;, xlab=&quot;x-axix label&quot;, ylab=&quot;y-axis label&quot;)函数中也可以指定其他图形参数。例如下面的代码将生成红色的标题，蓝色的副标题以及叫默认大小小25%的绿色x轴、y轴标签： titile(main=&quot;My Title&quot;, col.main=&quot;red&quot;, sub=&quot;My Sub-title&quot;,col.sub=&quot;blue&quot;, xlab=&quot;My X label&quot;, ylab=&quot;My Y label&quot;, col.lab=&quot;green&quot;, cex.lab=0.75) 坐标轴axis(side, at=, labels=, pos=, lty=, col=, las=, tck=, ...) 坐标轴选项 选项 描述 side 一个整数，表示在图形的哪边绘制坐标轴(1=下，2=左，3=上，4=右) at 一个数值型向量，表示需要绘制刻度线的位置 labels 一个字符型向量，表示置于刻度线旁边的文字标签（如果为NULL，则将直接使用at中的值） pos 坐标轴线绘制位置的坐标（即与另一条坐标轴相交位置的值） lty 线条类型 col 线条和刻度线颜色 las 标签是否平行于（=0）或者垂直于（=2）坐标轴 tck 刻度线的长度，以相对于绘图区域大小的分数表示表示（负值表示在图形外侧，正值表示在图形内侧，0表示禁用可读，1表示绘制网格线），默认是-0.01 (…) 123456789101112131415&gt; x &lt;- c(1:10)&gt; y &lt;- x&gt; z &lt;- 10/x&gt; opar &lt;- par(no.readonly = TRUE)&gt; par(mar=c(5, 4, 4, 8) + 0.1)&gt; plot(x, y, type = "b", pch=21, col='red', yaxt='n', lty=3, ann=FALSE)Hit &lt;Return&gt; to see next plot: Error in plot.new() : figure margins too large&gt; plot(x, y, type = "b", pch=21, col='red', yaxt='n', lty=3, ann=FALSE)Hit &lt;Return&gt; to see next plot: return()&gt; lines(x, z, type="b", pch=22, col="blue", lty=2)&gt; axis(2, at=x, labels=x, col.axis="red", las=2)&gt; axis(4, at=z, labels=round(z, digits=2), col.axis="blue", cex.axis=0.7, tck=-.01)&gt; mtext("y=1/x", side=4, line=3, cex.lab=1, las=2, col="blue")&gt; title("An Example of Creative Axes", xlab="X values", ylab="Y=X") 参考线abline(h=yvalues, v=xvalues) 图例当图形中包含的数据不止一组时，图例可以帮助你辨别出每个条形、扇形或折线各代表哪一类数据。 legend(location, title, legend, ...) location：指定图例的位置 title：图例标题的字符串（可选） legend：图例标签组成的字符型向量 123456789101112&gt; dose &lt;- c(20, 30, 40, 45, 60)&gt; drugA &lt;- c(16, 20, 27, 40, 60)&gt; drugB &lt;- c(15, 18, 25, 31, 40)&gt; opar &lt;- par(no.readonly=TRUE)&gt; par(lwd=2, cex=1.5, font.lab=2) # 增加线条，文本，符号，标签的宽度或大小&gt; plot(dose, drugA, type="b", pch=15, lty=1, col="red", ylim=c(0, 60), main="Drug A VS. DrugB", xlab="Drug Dosage", ylab="Drug Response") # 绘制图形&gt; lines(dose, drugB, type="b", pch=17, lty=2, col="blue")&gt; abline(h=c(30), lwd=1.5, lty=2, col="gray") # 增加次要刻度线&gt; library(Hmisc)&gt; minor.tick(nx=3, ny=3, tick.ratio=0.5)&gt; legend("topleft", inset=.05, title="Drug Type", c("A", "B"), lty=c(1, 2), pch=c(15, 17), col=c("red", "blue")) # 添加图例 文本标注可以通过text()和mtext()将文本添加到图形上。 text()可向绘图区域内部添加文本，而mtext()则向图形的四个边界之一添加文本。 格式： text(location, &quot;text to place&quot;, pos, ...) mtext(&quot;text to place&quot;, side, line=n, ...) 12345678910&gt; attach(mtcars)&gt; plot(wt, mpg, main="Mileage vs. Car Weight", xlab="Weight", ylab="Mileage", pch=18, col="blue")&gt; text(wt, mpg, row.names(mtcars), cex=0.6, pos=4, col="red" )&gt; detach(mtcars) 这里我们针对数据框mtcars提供的32中车型的车重和每加仑汽油行驶英里数绘制了散点图。函数text()被用来在各个数据点右侧添加车辆型号。各点的标签大小被缩小了 40%，颜色为红色。 下面展示不同字体族的代码（需要注意的是，不同平台显示的字体效果不同）： 123456&gt; opar &lt;- par(no.readonly=TRUE)&gt; par(cex=1.5)&gt; plot(1:7,, 1:7, type="n")&gt; text(3, 3, "Example of default text")&gt; text(4, 4, family="mono", "Example of mono-spaced text")&gt; text(5, 5, family="serif", "Example of serif text") 图形的组合在R中使用函数par()或layout()可以容易地组合多幅图形为一幅总括图形。 可以在par()中使用图形参数mfrow=c(nrows, ncols)来创建按行填充的、行数为nrows、列数为ncols的图形矩阵。 另外可以使用nfcol=c(nrows, ncols)按列填充矩阵。 例如：下面代码创建了四副图形并排布在两行两列中： 123456789&gt; attach(mtcars)&gt; opar &lt;- par(no.readonly=TRUE)&gt; par(mfrow=c(2, 2))&gt; plot(wt, mpg, main="Scatterplot of wt vs. mpg")&gt; plot(wt, disp, main="Scatterplot of wt vs disp")&gt; hist(wt, main="Histogram of wt")&gt; boxplot(wt, main="Boxplot of wt")&gt; par(opar)&gt; detach(mtcars) 下面代码：三行一列排布三幅图形 注意：hist() 包含一个默认的标题（使用main=””可以禁用它，抑制使用ann=FALSE来禁用所有标题和标签） 12345678&gt; attach(mtcars)&gt; opar &lt;- par(no.readonly=TRUE)&gt; par(mfrow=c(3, 1))&gt; hist(wt)&gt; hist(mpg)&gt; hist(disp)&gt; par(opar)&gt; detach(mtcars) 函数layout()的调用形式为layout(mat)，其中mat是一个矩阵，它指定了所要的组合的多个图形的所在位置，以下代码中，一幅图被置于第一行，另两幅被置于第二行 123456&gt; attach(mtcars)&gt; layout(matrix(c(1, 1, 2, 3), 2, 2, byrow=TRUE))&gt; hist(wt)&gt; hist(mpg)&gt; hist(disp)&gt; detach(mtcars)]]></content>
      <categories>
        <category>R</category>
        <category>图形</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django缓存]]></title>
    <url>%2F2019%2F10%2F18%2Fpython%2Fweb%2Fdjango%2Fdjango%E7%BC%93%E5%AD%98%2F</url>
    <content type="text"><![CDATA[粒度Django中提供了各种粒度的缓存方案，大致分为以下几种： 整站缓存：简单粗暴，直接在setting的MIDDLEWARE中第一行增加django.middleware.cache.UpdateCacheMiddleware。 页面缓存，在url.py的路由配置中 1234from django.views.decorators.cache import eache_pageurlpatterns = [ url('/',cache_page(60*20, key_profix='index_cache_'))] 局部数据缓存：这个包括函数中某部分逻辑缓存，也包括模板中一部分数据的缓存。 比如model层的post.hot_posts更新频率不高，可以进行较长的缓存。 123456789from django.core.cache import cacheclass Post(models.Model): @classmethod def hot_posts(self): result = cache.get('hot_posts') if not result: result = cls.objects.filter(status=1) cache.set('hot_posts', result, 10*60) return result 如果需要缓存部分模板数据，可以这么做 1234&#123;% load cache %&#125;&#123;% cache 50 sidebar %&#125; ...sidebar...&#123;% endcache %&#125; 内置的缓存在django中，内置支持以下几种缓存配置。 local-memory caching: 内存缓存。线程安全，进程间独立，也就是每个进程一份缓存。这是Django的默认配置。 filesystem caching：把数据缓存到文件系统中。 database caching：数据库缓存，需要创建缓存用的表，这些表是用来存储缓存数据的。 memcached：这是Django推荐的缓存系统，也是分布式的。 1234567# local-memory cachingCACHES = &#123; 'default': &#123; 'BACKEND': 'django.core.cache.backends.locmem.LocMemCache', 'LOCATION': 'unique-snowflake', &#125;&#125; 1234567# filesystem cachingCACHES = &#123; 'default': &#123; 'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache', 'LOCATION': '/var/tmp/django_cache', &#125;&#125; 1234567# database cachingCACHES = &#123; 'default': &#123; 'BACKEND': 'django.core.cache.backends.db.DatabaseCache', 'LOCATION': 'my_cache_table', &#125;&#125; 12345678910# memcachedCACHES = &#123; 'default': &#123; 'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache', 'LOCATION':&#123; '172.19.26.240:11211', '172.19.26.242:11211', &#125; &#125;&#125; 配置Redis缓存pip install django-redis pip install hiredis 我们同时安装了hiredis，其作用是提升Redis解析性能。 我们在settings.py中可做如下配置 1234567891011121314REDIS_URL = '127.0.0.1:6379:1'CACHES = &#123; 'default': &#123; 'BACKEND': 'django_redis.cache.RedisCache', 'LOCATION': REDIS_URL, 'TIMEOUT': 300, 'OPTIONS': &#123; 'PASSWORD': '对应的密码'， 'CLIENT_CLASS': 'django_redis.client.DefaultClient', 'PARSER_CLASS': 'redis.connection.HiredisParser', &#125;, 'CONNECTION_POOL_CLASS': 'redis.connection.BlockingConnectionPool', &#125;&#125;]]></content>
      <categories>
        <category>python</category>
        <category>web</category>
        <category>django</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>python</tag>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tools]]></title>
    <url>%2F2019%2F10%2F18%2Fpython%2Ftools%2F01.tools%2F</url>
    <content type="text"><![CDATA[小工具缓存缓存，其作用是缓和较慢存储的高频次请求，简单来说就是加速慢存储的访问效率。我们可以通过设定一个字典来放置查询，key-value:查询条件-查询结果在查询之前，先查看该字典中有无该key的存在，如果有，不走查询，如果没有，查询之后将查询结果保存在该字典中。 设置一个dict来存储缓存 缓存的大小，过期时间，访问时间 超过大小要删除，超过过期时间要删除 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import timefrom collections import OrderedDictclass LRUCacheDict: def __init__(self, max_size=1024, expiration=60): """ 最大容量为1024个key，每个key的有效期是60s """ self.max_size = max_size self.expiration = expiration self._cache = &#123;&#125; self._access_records = OrderedDict() # 记录访问时间 self._expire_records = OrderedDict() # 记录失效时间 def __setitem__(self, key, value): now = int(time.time()) self.__delete__(key) self._cache[key] = value self._expire_records[key] = now + self.expiration self._access_records[key] = now self.cleanup() def __getitem__(self, key): now = int(time.time()) del self._access_records[key] self._access_records[key] = now self.cleanup() def __contains__(self, key): self.cleanup() return key in self._cache def __delete__(self, key): if key in self._cache: del self._cache[key] del self._expire_records[key] del self._access_records[key] def cleanup(self): """去掉无效的过期的超出大小的缓存""" if self.expiration is None: return None pending_delete_keys = [] now = int(time.time()) for k, v in self._expire_records.items(): if v &lt; now: pending_delete_keys.append(k) for del_k in pending_delete_keys: self.__delete__(del_k) while (len(self._cache) &gt; self.max_size): for k in self._access_records: self.__delete__(k) breakif __name__ == '__main__': cache_dict = LRUCacheDict(max_size=2, expiration=10) cache_dict['name']='test' cache_dict['age'] = 30 cache_dict['addr'] = 'beijing' print('name' in cache_dict) print('age' in cache_dict) time.sleep(11) print('age' in cache_dict) 实现dict：我们通过了一些内置的方法(__getitem__和__setitem__)实现了一个dict对象。 缓存淘汰算法的使用：这里实现的是LRU算法。值得注意的是，我们实现的这个缓存字典是非线程安全的。 OrderedDict的使用：用它的目的就是保证顺序，让我们每次遍历都能够从最早放进去的数据开始。 可以将调用上述代码，并封装成装饰器 123456789101112131415161718192021222324def cache_it(max_size=1024, expiration=60): CACHE = LRUCacheDict(max_size=max_size, expiration=expiration) def wrapper(func): @functools.wraps(func) def inner(*args, **kwargs): key = repr(*args, **kwargs) try: result = CACHE[key] except KeyError: result = func(*args, **kwargs) CACHE[key] = result return result return inner return wrapper@cache_it(max_size=10, expiration=3)def query(sql): time.sleep(1) result = 'execute %s' % sql return result 注意：python3中LRUCache已经是标准库中的一部分了，可以通过functools.lru_cache来使用]]></content>
      <categories>
        <category>python</category>
        <category>tools</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[express实现转发]]></title>
    <url>%2F2019%2F10%2F17%2Fjavascript%2Fnodejs%2F05%E3%80%81express%E5%AE%9E%E7%8E%B0%E8%BD%AC%E5%8F%91%2F</url>
    <content type="text"><![CDATA[1. 需求现在需要对一个整体的web后端项目做拆分，并且放在不同的docker容器【服务器中】，中间需要一个容器做路由转发，这样所有的服务不受程序语言，框架的影响。并且如果一个容器挂掉也不会影响到其他容器和项目功能的使用。2. 选择框架因为项目整体语言为nodejs-express，比较熟悉，所以这里也使用nodejs-express进行开发。 需要安装的包 express node-fetch 利用express的特性，所有的请求都会经过中间件之后才会到达路由请求的处理。这里将会采用中间件的方式直接做转发。 3. 开始 npm i express安装express 新建app.js 1234567891011const express = require('express');const app = express();const bodyParser = require('body-parser');app.use(bodyParser.json()); // for parsing application/jsonapp.use(bodyParser.urlencoded(&#123; extended: true &#125;));const route = require('./route.js');app.use(route);module.exports = app; 这里还需要一个包是body-parser，主要用于请求体解析，转码。 新建route.js 123456789101112131415161718192021222324252627282930313233343536373839404142434445const express = require('express');const http = require('http');const fetch = require('node-fetch');const router = express.Router();let _fn;const apiHost = 'your server host';router.get('/', function (req, res, next) &#123; const path = req.originalUrl; _fn.getData(path, function (data) &#123; res.send(data); &#125;)&#125;);router.post('/users/*', function (req, res, next) &#123; const path = req.originalUrl; const content = req.body; _fn.postData(path, content, function (data) &#123; res.send(data); &#125;)&#125;);_fn = &#123; getData: function (path, callback) &#123; fetch(`http://$&#123;apiHost&#125;$&#123;path&#125;`).then( async res =&gt; callback(await res.text()) ) &#125;, postData: function (path, data, callback) &#123; data = data || &#123;&#125;; const content = JSON.stringify(data); const options = &#123; method: 'POST', headers: &#123; Accept: 'application/json', 'Content-Type': 'application/json', &#125;, body: content, &#125;; fetch(`http://$&#123;apiHost&#125;$&#123;path&#125;`, options).then( async res =&gt; callback(await res.text()) ) &#125;&#125;;module.exports = router; 这里对上述代码进行一一说明 router：express的路由管理，get方法是处理GET请求，post方法是处理POST请求，第一个参数是路由，支持正则，第二个参数是一个函数，参数主要是request, response, next。 这里使用req.originalUrl来过去请求路径 req.body来获取请求体 _fn：定义一个对象，有两个key，一个用于发送get请求，一个用于发送post请求。方法参数为路由和一个回调函数，回调函数会将结果直接做为一个数据返回个这个请求的最终发送方 get：只需要将路由和路径拼接起来即可 post：将数据转化为字符串，然后将请求转发。 4. 启动新建start.js 123456789101112const app = require('./app');app.set('port', 80);// app.set('port', port);const server = app.listen(app.get('port'), () =&gt; &#123; const host = server.address().address; const &#123; port &#125; = server.address(); console.log('Example app listening at http://%s:%s', host, port);&#125;); 之后执行node start.js即可。]]></content>
      <categories>
        <category>javascript</category>
        <category>nodejs</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>javascript</tag>
        <tag>nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二分查找]]></title>
    <url>%2F2019%2F10%2F16%2Falgorithm%5B%E7%AE%97%E6%B3%95%5D%2F2.%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE%2F</url>
    <content type="text"><![CDATA[二分查找又称折半查找，优点是比较次数少，查找速度快，平均性能好；其缺点是要求待查表为有序表，且插入删除困难。因此，折半查找方法适用于不经常变动而查找频繁的有序列表。首先，假设表中元素是按升序排列，将表中间位置记录的关键字与查找关键字比较，如果两者相等，则查找成功；否则利用中间位置记录将表分成前、后两个子表，如果中间位置记录的关键字大于查找关键字，则进一步查找前一子表，否则进一步查找后一子表。重复以上过程，直到找到满足条件的记录，使查找成功，或直到子表不存在为止，此时查找不成功。 时间复杂度 二分查找是有条件的，首先是有序，其次因为二分查找操作的是下标，所以要求是顺序表 最优时间复杂度：O(1) 最坏时间复杂度：O(logn) 非递归实现12345678910111213141516def binary_search(alist, item): first = 0 last = len(alist) - 1 while first &lt;= last: midpoint = (first + last) // 2 if alist[midpoint] == item: return True elif item &lt; alist[midpoint]: last = midpoint - 1 else: first = midpoint + 1 return Falsetestlist = [0, 1, 2, 8, 13, 17, 19, 32, 42]print(binary_search(testlist, 3))print(binary_search(testlist, 13)) 递归实现12345678910111213141516def binary_search(alist, item): if len(alist) == 0: return False else: midpoint = len(alist) // 2 if alist[midpoint] == item: return True else: if item &lt; alist[midpoint]: return binary_search(alist[:midpoint], item) else: return binary_search(alist[midpoint + 1:], item)testlist = [0, 1, 2, 8, 13, 17, 19, 32, 42]print(binary_search(testlist, 3))print(binary_search(testlist, 13))]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[几种排序算法]]></title>
    <url>%2F2019%2F10%2F16%2Falgorithm%5B%E7%AE%97%E6%B3%95%5D%2F1.%E5%87%A0%E7%A7%8D%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[1. 冒泡排序它重复地走访要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端，故名冒泡排序。图解时间复杂度 最优时间复杂度：O(n) （表示遍历一次发现没有任何可以交换的元素，排序结束。） 最坏时间复杂度：O(n^2) 稳定性：稳定 代码12345678910111213141516def bubbleSort(input_list): if len(input_list) == 0: return [] sorted_list = input_list for i in range(len(sorted_list) - 1): for j in range(len(sorted_list) - 1): if sorted_list[j + 1] &lt; sorted_list[j]: sorted_list[j], sorted_list[j + 1] = sorted_list[j + 1], sorted_list[j] return sorted_listif __name__ == '__main__': input_list = [50, 123, 543, 187, 49, 30, 0, 2, 11, 100] print('排序前:', input_list) sorted_list = bubbleSort(input_list) print('排序后:', sorted_list) 12345678910111213141516171819202122import java.util.Arrays;public class uxdl &#123; public static void sort(int array[]) &#123; for (int i = 0; i &lt; array.length - 1; i++) &#123; for (int j = 0; j &lt; array.length - i - 1; j++) &#123; int tmp = 0; if (array[j] &gt; array[j + 1]) &#123; tmp = array[j]; array[j] = array[j + 1]; array[j + 1] = tmp; &#125; &#125; &#125; &#125; public static void main(String[] args) &#123; int[] array = new int[]&#123;5, 8, 6, 3, 9, 2, 1, 7&#125;; sort(array); System.out.println(Arrays.toString(array)); &#125;&#125; 算法稳定性假定在待排序的记录序列中，存在多个具有相同的关键字的记录，若经过排序，这些记录的相对次序保持不变，即在原序列中，r[i]=r[j]，且r[i]在r[j]之前，而在排序后的序列中，r[i]仍在r[j]之前，则称这种排序算法是稳定的；否则称为不稳定的。 冒泡排序就是把小的元素往前调或者把大的元素往后调。是相邻的两个元素的比较，交换也发生在这两个元素之间。所以相同元素的前后顺序并没有改变，所以冒泡排序是一种稳定排序算法。 冒泡排序的优化 可以看出来，经过第6轮排序后，数列就已经是有序的了 1234567891011121314151617181920212223242526272829import java.util.Arrays;public class uxdl &#123; public static void sort(int array[]) &#123; for (int i = 0; i &lt; array.length - 1; i++) &#123; // 有序标记，每一轮的初始值都是true boolean isSorted = true; for (int j = 0; j &lt; array.length - i - 1; j++) &#123; int tmp = 0; if (array[j] &gt; array[j + 1]) &#123; tmp = array[j]; array[j] = array[j + 1]; array[j + 1] = tmp; // 因为有数据进行交换，所以不是有序的，标记变为false isSorted = false; &#125; &#125; if (isSorted) &#123; break; &#125; &#125; &#125; public static void main(String[] args) &#123; int[] array = new int[]&#123;5, 8, 6, 3, 9, 2, 1, 7&#125;; sort(array); System.out.println(Arrays.toString(array)); &#125;&#125; 如果数列出现有序区 123456789101112131415161718192021222324252627282930313233343536import java.util.Arrays;public class uxdl &#123; public static void sort(int array[]) &#123; // 记录最后一次交换的位置 int lastExchangeIndex = 0; // 无序数列边界，每次比较只需要比到这里为止 int sortBorder = array.length - 1; for (int i = 0; i&lt;array.length-1; i++)&#123; // 有序标记，每一轮的初始值都是true boolean isSorted = true; for (int j = 0; j &lt; sortBorder; j ++) &#123; int tmp = 0; if (array[j] &gt; array[j+1]) &#123; tmp = array[j]; array[j] = array[j +1]; array[j + 1] = tmp; // 因为有元素进行交换，所以不是有序的，标记变为false isSorted = false; // 更新为最后一次交换元素的位置 lastExchangeIndex = j; &#125; &#125; sortBorder = lastExchangeIndex; if (isSorted)&#123; break; &#125; &#125; &#125; public static void main(String[] args) &#123; int[] array = new int[]&#123;5, 8, 6, 3, 9, 2, 1, 7&#125;; sort(array); System.out.println(Arrays.toString(array)); &#125;&#125; 2. 选择排序选择排序（Selection sort）是一种简单直观的排序算法。它的工作原理如下。首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。 选择排序的主要优点与数据移动有关。如果某个元素位于正确的最终位置上，则它不会被移动。选择排序每次交换一对元素，它们当中至少有一个将被移到其最终位置上，因此对n个元素的表进行排序总共进行至多n-1次交换。在所有的完全依靠交换去移动元素的排序方法中，选择排序属于非常好的一种。 图解 时间复杂度 最优时间复杂度：O(n^2) 最坏时间复杂度：O(n^2) 稳定性：不稳定（考虑升序每次选择最大的情况） 代码1234567891011121314151617def selection_sort(alist): n = len(alist) # 需要进行n-1次选择操作 for i in range(n - 1): # 记录最小位置 min_index = i # 从i+1位置到末尾选择出最小数据 for j in range(i + 1, n): if alist[j] &lt; alist[min_index]: min_index = j # 如果选择出的数据不在正确位置，进行交换 if min_index != i: alist[i], alist[min_index] = alist[min_index], alist[i]alist = [54, 226, 93, 17, 77, 31, 44, 55, 20]selection_sort(alist)print(alist) 3. 插入排序 插入排序（英语：Insertion Sort）是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。插入排序在实现上，在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。 分析 从第一个元素开始, 该元素可以被认为是已经排序的; 取出下一个元素, 在已经排序的数组中从后向前扫描; 如果该元素(已经排序)大于新元素(取出来的下一个元素), 将该元素移到下一个位置; 重复步骤3, 直到找到已排序的元素小于或者等于新元素的位置; 将新元素插入该位置; 重复2~5. 图解 时间复杂度 最优时间复杂度：O(n) （升序排列，序列已经处于升序状态） 最坏时间复杂度：O(n^2) 稳定性：稳定 代码123456789101112def insert_sort(alist): # 从第二个位置，即下标为1的元素开始向前插入 for i in range(1, len(alist)): # 从第i个元素开始向前比较，如果小于前一个元素，交换位置 for j in range(i, 0, -1): if alist[j] &lt; alist[j - 1]: alist[j], alist[j - 1] = alist[j - 1], alist[j] print(alist)alist = [54, 26, 93, 17, 77, 31, 44, 55, 20]insert_sort(alist)print(alist) 4. 快速排序快速排序（英语：Quicksort），又称划分交换排序（partition-exchange sort），通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。 步骤为： 从数列中挑出一个元素，称为”基准”（pivot）， 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区（partition）操作。 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。 递归的最底部情形，是数列的大小是零或一，也就是永远都已经被排序好了。虽然一直递归下去，但是这个算法总会结束，因为在每次的迭代（iteration）中，它至少会把一个元素摆到它最后的位置去。 图解 时间复杂度 最优时间复杂度：O(nlogn) 最坏时间复杂度：O(n^2) 稳定性：不稳定 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445def quick_sort(alist, start, end): """快速排序""" # 递归的退出条件 if start &gt;= end: return # 设定起始元素为要寻找位置的基准元素 mid = alist[start] # low为序列左边的由左向右移动的游标 low = start # high为序列右边的由右向左移动的游标 high = end while low &lt; high: # 如果low与high未重合，high指向的元素不比基准元素小，则high向左移动 while low &lt; high and alist[high] &gt;= mid: high -= 1 # 将high指向的元素放到low的位置上 alist[low] = alist[high] # 如果low与high未重合，low指向的元素比基准元素小，则low向右移动 while low &lt; high and alist[low] &lt; mid: low += 1 # 将low指向的元素放到high的位置上 alist[high] = alist[low] # 退出循环后，low与high重合，此时所指位置为基准元素的正确位置 # 将基准元素放到该位置 alist[low] = mid # 对基准元素左边的子序列进行快速排序 quick_sort(alist, start, low - 1) # 对基准元素右边的子序列进行快速排序 quick_sort(alist, low + 1, end)alist = [54, 26, 93, 17, 77, 31, 44, 55, 20]quick_sort(alist, 0, len(alist) - 1)print(alist) def quicksort(array): if len(array) &lt; 2: return array else: pivot = array[0] less = [i for i in array[1:] if i &lt;= pivot] greater = [i for i in array[1:] if i &gt; pivot] return quicksort(less) + [pivot] + quicksort(greater)print(quicksort([12, 54, 3, 5, 356, 5, 564, 52, 85, 8, 2])) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import java.util.Arrays;public class uxdl &#123; public static void quickSort(int[] arr, int startIndex, int endIndex) &#123; // 递归条件结束：startIndex大于或等于endIndex时 if (startIndex &gt;= endIndex) &#123; return; &#125; // 得到基准元素位置 int pivotIndex = partition(arr, startIndex, endIndex); // 根据基准元素，分成两部分进行递归排序 quickSort(arr, startIndex, pivotIndex - 1); quickSort(arr, pivotIndex + 1, endIndex); &#125; /* * 分治（双边循环法） * @param arr 待交换的数组 * @param startIndex 起始下标 * @param endIndex 结束下标 * */ private static int partition(int[] arr, int startIndex, int endIndex) &#123; // 取第一个位置（也可以选择随机位置）的元素作为基准元素 int pivot = arr[startIndex]; int left = startIndex; int right = endIndex; while (left != right) &#123; // 控制right指针比较并左移 while (left &lt; right &amp;&amp; arr[right] &gt; pivot) &#123; right--; &#125; // 控制left指针比较并右移 while (left &lt; right &amp;&amp; arr[left] &lt;= pivot) &#123; left++; &#125; // 交换left和right指针所指向的元素 if (left &lt; right) &#123; int p = arr[left]; arr[left] = arr[right]; arr[right] = p; &#125; &#125; // pivot 和指针重合点交换 arr[startIndex] = arr[left]; arr[left] = pivot; return left; &#125; public static void main(String[] args) &#123; int[] array = new int[]&#123;5, 8, 6, 3, 10, 11, 12, 12&#125;; quickSort(array, 0, array.length - 1); System.out.println(Arrays.toString(array)); &#125;&#125; 单边循环法 123456789101112131415161718192021222324 /* * 分治（单边循环法） * @param arr 待交换的数组 * @param startIndex 起始下标 * @param endIndex 结束下标 * */private static int partition(int[] arr, int startIndex, int endIndex) &#123; // 取第一个位置（也可以选择随机位置）的元素作为基准元素 int pivot = arr[startIndex]; int mark = startIndex; for (int i=startIndex + 1; i&lt; endIndex; i ++)&#123; if (arr[i]&lt; pivot)&#123; mark ++; int p = arr[mark]; arr[mark] = arr[i]; arr[i] = p; &#125; &#125; arr[startIndex] = arr[mark]; arr[mark] = pivot; return mark;&#125; 非递归实现 123456789101112131415161718192021222324252627282930public static void quickSort(int[] arr, int startIndex, int endIndex)&#123; // 用一个集合栈来代替递归的函数栈 Stack&lt;Map&lt;String, Integer&gt;&gt; quickSortStack = new Stack&lt;Map&lt;String, Integer&gt;&gt;(); // 整个数列的起止下标，以哈希的形式入栈 Map rootParam = new HashMap(); rootParam.put("startIndex", startIndex); rootParam.put("endIndex", endIndex); quickSortStack.push(rootParam); // 循环结束条件：栈为空时 while (!quickSortStack.isEmpty()) &#123; // 栈顶元素出栈，得到起止下标 Map&lt;String, Integer&gt; param = quickSortStack.pop(); // 得到基准元素位置 int pivotIndex = partition(arr, param.get("startIndex"), param.get("endIndex")); // 根据基准元素分成两部分，把每一部分的起止下标入栈 if(param.get("startIndex") &lt; pivotIndex -1)&#123; Map&lt;String, Integer&gt; leftParam = new HashMap&lt;String, Integer&gt;(); leftParam.put("startIndex", param.get("startIndex")); leftParam.put("endIndex", pivotIndex-1); quickSortStack.push(leftParam); &#125; if (pivotIndex + 1&lt; param.get("endIndex")) &#123; Map&lt;String, Integer&gt; rightParam = new HashMap&lt;String, Integer&gt;(); rightParam.put("startIndex", pivotIndex + 1); rightParam.put("endIndex", param.get("endIndex")); quickSortStack.push(rightParam); &#125; &#125;&#125; 5. 希尔排序 希尔排序(Shell Sort)是插入排序的一种。也称缩小增量排序，是直接插入排序算法的一种更高效的改进版本。 希尔排序的基本思想是：将数组列在一个表中并对列分别进行插入排序，重复这过程，不过每次用更长的列（步长更长了，列数更少了）来进行。最后整个表就只有一列了。将数组转换至表是为了更好地理解这算法，算法本身还是使用数组进行排序。 图解 时间复杂度 最优时间复杂度：根据步长序列的不同而不同 最坏时间复杂度：O(n^2) 稳定想：不稳定 代码12345678910111213141516171819def shell_sort(alist): n = len(alist) # 初始步长 gap = n // 2 while gap &gt; 0: # 按步长进行插入排序 for i in range(gap, n): j = i # 插入排序 while j &gt;= gap and alist[j - gap] &gt; alist[j]: alist[j - gap], alist[j] = alist[j], alist[j - gap] j -= gap # 得到新的步长 gap = gap // 2alist = [54, 26, 93, 17, 77, 31, 44, 55, 20]shell_sort(alist)print(alist) 6. 归并排序归并排序是采用分治法的一个非常典型的应用。归并排序的思想就是先递归分解数组，再合并数组。 将数组分解最小之后，然后合并两个有序数组，基本思路是比较两个数组的最前面的数，谁小就先取谁，取了后相应的指针就往后移一位。然后再比较，直至一个数组为空，最后把另一个数组的剩余部分复制过来即可。 图解 时间复杂度 最优时间复杂度：O(nlogn) 最坏时间复杂度：O(nlogn) 稳定性：稳定 代码1234567891011121314151617181920212223242526272829303132def merge_sort(alist): if len(alist) &lt;= 1: return alist # 二分分解 num = len(alist) // 2 left = merge_sort(alist[:num]) right = merge_sort(alist[num:]) # 合并 return merge(left, right)def merge(left, right): '''合并操作，将两个有序数组left[]和right[]合并成一个大的有序数组''' # left与right的下标指针 l, r = 0, 0 result = [] while l &lt; len(left) and r &lt; len(right): if left[l] &lt; right[r]: result.append(left[l]) l += 1 else: result.append(right[r]) r += 1 result += left[l:] result += right[r:] return resultalist = [54, 26, 93, 17, 77, 31, 44, 55, 20]sorted_alist = merge_sort(alist)print(sorted_alist) 7. 其他排序鸡尾酒排序鸡尾酒排序就是在冒泡排序的基础上，做了些优化，它的元素比较和交换过程是双向的。 例如：这里有一个由8个数字组成的一个无序数列[2, 3, 4, 5, 6, 7, 8, 1]，希望对其进行从小到大的排序。 冒泡排序的思想： 这里的问题在于只有1个元素的位置不对，缺进行了7轮排序 鸡尾酒排序： 第一轮，和冒泡排序一样，8和1交换 第二轮，反过来从右往左比较并进行交换。 第三轮，虽然实际上已经有序，但流程并没有结束，需要重新从左向右比较并进行交换。 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445import java.util.Arrays;public class uxdl &#123; public static void sort(int array[]) &#123; int tmp = 0; for (int i=0; i&lt; array.length/2; i++) &#123; // 有序标记，每一轮的初始值都是true boolean isSorted = true; // 奇数轮，从左向右比较和交换 for (int j = 1; j&lt; array.length-i-1; j++) &#123; if (array[i] &gt; array[j+1])&#123; tmp = array[j]; array[j] = array[j+1]; array[j+1] = tmp; // 有元素交换，所以不是有序的，标记变为false isSorted = false; &#125; &#125; if (isSorted) &#123; break; &#125; // 在偶数轮之前，将isSorted重新标记为true isSorted = true; // 偶数轮， 从右向左比较和交换 for (int j = array.length-i-1; j&gt;i; j--)&#123; if (array[j] &lt; array[j-1])&#123; tmp = array[j]; array[j] = array[j-1]; array[j-1] = tmp; // 因为有元素进行交换，所以不是有序的，标记变为false isSorted = false; &#125; &#125; if (isSorted)&#123; break; &#125; &#125; &#125; public static void main(String[] args) &#123; int[] array = new int[]&#123;5, 8, 6, 3, 10, 11, 12, 12&#125;; sort(array); System.out.println(Arrays.toString(array)); &#125;&#125; 优缺点在特定条件下，减少排序的回合数，但代码量几乎增加了1倍 堆排序二叉堆的两种类型： 最大堆，堆顶是整个堆中的最大元素 最小堆，堆顶是整个堆中的最小元素 以最大堆为例，如果删除最大堆的堆顶（并不是完全删除，而是跟末尾的节点交换位置），经过自我调整，第2大的元素就会被交换上来，成为最大堆的新堆顶。 由于二叉堆的这个特性，每一次删除旧堆顶，调整后的新堆顶都是大小仅次于旧堆顶的节点。 步骤 把无序数组构建成二叉堆，需要从小到大排序，则构建成最大堆；需要从大到小排序，则构建成最小堆。 循环删除栈顶元素，替换到二叉堆的末尾，调整堆产生新的堆顶 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import java.util.Arrays;public class uxdl &#123; /* * “下沉调整” * @param array 待调整的堆 * @param parentIndex 要“下沉”的父节点 * @param length 堆的有效大小 * */ public static void downAdjust(int[] array, int parentIndex, int length) &#123; // temp 保存父节点值，用于最后的赋值 int temp = array[parentIndex]; int childIndex = 2 * parentIndex + 1; while (childIndex &lt; length) &#123; // 如果有右孩子，且右孩子大于左孩子的值，则定位到右孩子 if (childIndex + 1 &lt; length &amp;&amp; array[childIndex + 1] &gt; array[childIndex]) &#123; childIndex++; &#125; // 如果父节点大于任何一个孩子的值，则直接跳出 if (temp &gt;= array[childIndex]) break; // 无须真正交换，单向赋值即可 array[parentIndex] = array[childIndex]; parentIndex = childIndex; childIndex = 2 * childIndex + 1; &#125; array[parentIndex] = temp; &#125; /* * 堆排序(升序) * @param array 待调整的堆 * */ public static void heapSort(int[] array) &#123; // 1. 把无序数组构建成最大堆 for (int i = (array.length - 2) / 2; i &gt;= 0; i--) &#123; downAdjust(array, i, array.length); &#125; System.out.println(Arrays.toString(array)); // 2. 循环删除堆顶元素，移到集合尾部，调整堆产生新的堆顶 for (int i = array.length - 1; i &gt; 0; i--) &#123; // 最后一个元素和第一个元素进行交换 int temp = array[i]; array[i] = array[0]; array[0] = temp; // "下沉"调整最大堆 downAdjust(array, 0, i); &#125; &#125; public static void main(String[] args) &#123; int[] array = new int[]&#123;5, 8, 6, 3, 10, 11, 12, 58&#125;;// quickSort(array, 0, array.length - 1); heapSort(array); System.out.println(Arrays.toString(array)); &#125;&#125; 空间复杂度是O(1) 分析一下时间复杂度 把无序数组构建成二叉堆O(n) 循环删除堆顶元素，并将该元素移到集合尾部，调整堆产生新的堆顶O(nlogn) 所以整体的时间复杂度是O(nlogn) 线性时间排序不基于元素比较，利用数组下标来确定元素的正确位置 计数排序假设数组中有20个随机整数，取值范围是0~10 因为取值范围有限，可以创建一个长度为11的数组，数组下标从1到10，元素初始值为0 遍历数组，每一个整数按照值对号入座，同时对应数组下标的元素进行加1操作 最后遍历这个数组，下标是几就输出几，值是几就输出几次。 代码实现1234567891011121314151617181920212223242526272829303132333435import java.util.Arrays;public class uxdl &#123; public static int[] countSort(int[] array) &#123; // 1.得到数列的最大值 int max = array[0]; for (int i = 0; i &lt; array.length; i++) &#123; if (array[i] &gt; max) &#123; max = array[i]; &#125; &#125; // 2.根据最大值来确定统计数组长度 int[] countArray = new int[max+1]; // 3.遍历数列，填充统计数组 for (int i=0; i&lt;array.length; i++)&#123; countArray[array[i]] ++; &#125; // 4.遍历统计数组，输出结果 int index = 0; int[] sortedArray = new int[array.length]; for (int i =0; i&lt;countArray.length; i++)&#123; for (int j=0; j&lt;countArray[i]; j++)&#123; sortedArray[index++] = i; &#125; &#125; return sortedArray; &#125; public static void main(String[] args) &#123; int[] array = new int[]&#123;5, 8, 6, 3, 10, 11, 12, 58&#125;; int[] sortedArray = countSort(array); System.out.println(Arrays.toString(sortedArray)); &#125;&#125; 优化由于统计数组的长度是由最大值决定的，当数组较大时，就会造成不必要的浪费 还有其余的优化暂时不理解，以后奉上 优化代码原始数列的规模是n，最大和最小整数的差值是m 时间复杂度是O(n+m)，空间复杂度是O(m) 12345678910111213141516171819202122232425262728293031323334353637383940import java.util.Arrays;public class uxdl &#123; public static int[] countSort(int[] array) &#123; // 1.得到数列的最大值和最小值，并计算出差值 int max = array[0]; int min = array[0]; for (int i = 1; i &lt; array.length; i++) &#123; if (array[i] &gt; max) &#123; max = array[i]; &#125; if (array[i] &lt; min) &#123; min = array[i]; &#125; &#125; int d = max - min; // 2.创建统计数组并统计对应元素的个数 int[] countArray = new int[d + 1]; for (int i = 0; i &lt; array.length; i++) &#123; countArray[array[i] - min]++; &#125; // 3.统计数组做变形，后面的元素等于前面的元素之和 for (int i = 1; i &lt; countArray.length; i++) &#123; countArray[i] += countArray[i - 1]; &#125; // 4.倒序遍历原始数列，从统计数组找到正确位置，输出到结果数组 int[] sortedArray = new int[array.length]; for (int i = array.length - 1; i &gt;= 0; i--) &#123; sortedArray[countArray[array[i] - min] - 1] = array[i]; countArray[array[i] - min]--; &#125; return sortedArray; &#125; public static void main(String[] args) &#123; int[] array = new int[]&#123;5, 8, 6, 3, 10, 11, 12, 58&#125;; int[] sortedArray = countSort(array); System.out.println(Arrays.toString(sortedArray)); &#125;&#125; 局限性 当数列最大和最小值差距过大时，并不适用计数排序 当数列元素不是整数时，也不适合用计数排序 桶排序主要为了弥补计数排序的局限性，两者类似，所不同的是，对应的不再是下标，而是一个区间 区间跨度 = (最大值 - 最小值) / (桶数量 - 1)，分好之后，对每个桶进行排序 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import java.util.*;public class uxdl &#123; public static double[] bucketSort(double[] array) &#123; // 1.得到数列的最大值和最小值，并算出差值 double max = array[0]; double min = array[0]; for (int i = 1; i &lt; array.length; i++) &#123; if (array[i] &gt; max) &#123; max = array[i]; &#125; if (array[i] &lt; min) &#123; min = array[i]; &#125; &#125; double d = max - min; // 2.初始化桶 int bucketNum = array.length; ArrayList&lt;LinkedList&lt;Double&gt;&gt; bucketList = new ArrayList&lt;LinkedList&lt;Double&gt;&gt;(bucketNum); for (int i = 0; i &lt; bucketNum; i++) &#123; bucketList.add(new LinkedList&lt;Double&gt;()); &#125; // 3.遍历原始数组，将每个元素放入桶中 for (int i = 0; i &lt; array.length; i++) &#123; int num = (int) ((array[i] - min) * (bucketNum - 1) / d); bucketList.get(num).add(array[i]); &#125; // 4.对每个桶内部进行排序 for (int i = 0; i &lt; bucketList.size(); i++) &#123; // JDK底层采用了归并排序或归并的 Collections.sort(bucketList.get(i)); &#125; // 5.输出全部元素 double[] sortedArray = new double[array.length]; int index = 0; for (LinkedList&lt;Double&gt; list : bucketList) &#123; for (double element : list) &#123; sortedArray[index] = element; index++; &#125; &#125; return sortedArray; &#125; public static void main(String[] args) &#123; double[] array = new double[]&#123;5.222, 8.4, 6.45, 3.45, 10.45, 11.45, 12.78, 58.78&#125;; double[] sortedArray = bucketSort(array); System.out.println(Arrays.toString(sortedArray)); &#125;&#125;// [3.45, 5.222, 6.45, 8.4, 10.45, 11.45, 12.78, 58.78] 上面代码使用了JDK的集合工具类Collections.sort来为桶内部的元素进行排序。Collections.sort底层采用的是归并排序或TimSort，时间复杂度为O(nlogn); 复杂度 第一步，求数列最大、最小值，运算量为n 第二步，创建空桶，运算量为n 第三步，把原始数列的元素分配到各个桶中，运算量为n 第四步，把每个桶内部做排序，在元素分布相对均匀的情况下，所有桶的运算量之和为n。 第五步，输出排序数列，运算量为n 所以空间复杂度是O(n); 桶排序的性能并不是绝对稳定。如果元素的分布极不均匀，在极端情况下，第一个桶中有n-1个元素，最后一个桶有1个元素，此时的时间复杂度将退化为O(nlogn)，而且还白白创建了很多空桶。 几种排序效率比较 综述 时间复杂度为O(n^2)的排序算法 冒泡排序 选择排序 插入排序 希尔排序(性能优于O(n^2)，但低于O(nlogn)) 时间复杂度为O(nlogn)的排序算法 快速排序 归并排序 堆排序 时间复杂度为线性的排序算法 计数排序 桶排序 基数排序]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django的QuerySet接口和查询]]></title>
    <url>%2F2019%2F10%2F13%2Fpython%2Fweb%2Fdjango%2Fdjango%E5%B8%B8%E7%94%A8%E7%9A%84QuerySet%E6%8E%A5%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[在Django的model层中，通过给model增加一个objects属性来提供数据操作的接口，比如，想要查询所有文章的数据，Post.objects.all()，这样就能拿到QuerySet对象，这个对象包含了我们需要的数据，当我们需要它时，它会去DB中获取数据。QuerySet本质上是一个懒加载的对象，只有在真正用到的时候才会执行查询。 1. 支持链式调用的接口 all：相当于SELECT * FROM table_name，用于查询所有数据。 filter：根据条件过滤。 exclude：同filter，只是相反的逻辑 reverse：把QuerySet中的结果倒序排列 distinct：用来进行去重查询 none：返回空的QuerySet 2. 不支持链式调用的接口 get：比如Post.objects.get(id=1)用于查询id为1的文章：如果存在，则直接返回对应的Post实例；如果不存在则抛出DoesNotExist异常 create：直接创建一个Model对象 get_or_create：根据条件查找，如果没有找到，就调用create创建 update_or_create：同get_or_create，只是用来做更新操作 count：返回QuerySet有多少记录，相当于SELECT COUNT(*) FROM table_name latest：返回最新的一条记录，但是需要在Model的meta中定义：get_latest_by = &lt;用来排序的字段0&gt; earliest：同上，返回最早的一条记录 first：从当前QuerySet记录中获取第一条 last：同上，获取最后一条 exists：返回 True 或者 False，在数据层面执行SELECT (1) AS &quot;A&quot; FROM table_name LIMIT 1的查询，如果只是需要判断QuerySet是否有数据，用这个接口是最合适的方式。不要用 count 或者 len(queryset)这样的操作来判断是否存在。相反，如果可以预期接下来会用到QuerySet中的数据，可以考虑使用len(queryset)的方式来做判断，这样可以减少一次DB查询请求。 bulk_create：同 create，用来批量创建记录 in_bulk：批量查询，接收两个参数id_list和filed_name。可以通过Post.objects.in_bulk([1, 2, 3])查询出id为1、2、3的数据，返回结果是字典类型，字典类型的key为查询条件。 update：根据条件批量更新记录。Post.object.filter(owner__name=&#39;test&#39;).update(title=&#39;测试更新&#39;) delete：同update，根据条件批量删除记录 values：当我们明确知道只需要返回某个字段的值，不需要Model实例时。title_list = Post.objects.filter(category_id=1).values(&#39;title&#39;) values_list：同values，但是直接返回的是包含tuple的QuerySet。title_list = Post.objects.filter(category=1).values_list(&#39;title&#39;)，如果只是一个字段的话，可以通过增加flat=True参数，便于我们后续处理。 123title_list = Post.objects.filter(category=1).values_list('title', flat=True) for title in title_list: print(title) 3. 进阶接口 defer：把不需要展示的字段做延迟加载。比如说，需要获取到文章中正文外的其他字段，就可以通过posts = Post.objects.all().defer(&#39;content&#39;)，这样拿到的记录中就不会包含content部分。但当我们需要用道这个字段时，在使用时会去加载 123posts = Post.objects.all().defer('content')for post in posts: # 此时会执行数据库查询 print(post.content) # 此时会执行数据查询，获取到content 注意：当不想加载某个过大的字段时（如text类型字段，会使用defer，但是上面的演示代码会产生N+1的查询问题，在实际使用时要注意） 所谓的N+1的问题其实是1+N的问题，就是查询到的n条记录，我们可能只用到了其中的1条。 only：同defer刚好相反，如果只想获取到所有的title记录，就可以使用only，只获取title的内容， 其他值在获取是会产生额外的查询。 select_related：这就是用来解决外键产生的N+1问题的方案 下面案例会产生这个问题： 123posts = Post.objects.all()for post in posts: print(post.owner) # owner是外键的关联表 解决方法： 123posts = Post.objects.all().select_related('category')for post in posts: print(post.category) 当然，这个借口只能用来解决一对多的关联关系。 prefetch_related：针对多对多关系的数据，可以通过这个接口来避免N+1查询。 比如：post和tag的关系可以通过这种方式来避免 123posts = Post.objects.all().prefetch_related('tag')for post in posts: print(post.tag.all()) 4. 常用的字段查询 contains：包含，用来进行相似查询 icontains：同contains，忽略大小写 exact：精确匹配 iexact：同exact，忽略大小写 in：指定某个集合，比如Post.objects.filter(id__in=[1, 2, 3])相当于SELECT * FROM blog_post WHERE IN (1, 2, 3); gt：大于 gte：大于等于 lt：小于 lte：小于等于 startswith：以某个字符串开头，与contains类似，只是会产生LIKE &#39;&lt;关键词&gt;%&#39;这样的SQL istartswith：同startswith，忽略大小写 endswith：以某个字符串结尾 iendswith：同endswith，忽略大小写 range：范围查询，多用于时间范围，Post.objects.filter(created_time__range=(&#39;2018-05-01&#39;, &#39;2018-06-01&#39;))会产生这样的查询：SELECT ... WHERE created_time BETWEEN &#39;2018-05-01&#39; AND &#39;2018-06-01&#39;; 5. 进阶查询 F：F 表达式常用来执行数据库层面的计算，从而避免出现竞争状态。比如需要处理每篇文章的访问量，假设存在post.pv这样的字段，当有用户访问时，我们对其加1： 123post = Post.objects.get(id=1)post.pv = post.pv + 1pos.save() 这在多线程的情况下会出现问题，其执行逻辑是先获取到当前的pv值，然后将其加1后赋值给post.pv，最后保存。如果多个线程同时执行了post = Post.objects.get(id=1)，那么每个线程里的post.pv值都是一样的，执行完加1和保存之后，相当于只执行了一次加1. 这时通过F表达式就可以方便的解决这个问题： 1234from django.db.models import F post = Post.objects.get(id=1) post.pv = F('pv') + 1 post.save() 这种方式最终会产生类似这样的SQL语句UPDATE blog_post SET pv = pv + 1 WHERE ID = 1。它在数据库层面执行原子性操作。 Q：Q表达式就是用来解决前面提到的那个OR查询的，可以这么用： 12from django.db.models import QPost.objects.filter(Q(id=1) | Q(id=2)) 或者进行AND查询 1Post.objects.filter(Q(id=1) &amp; Q(id=2)) Count：用来做聚合查询，比如想要得到某个分类下有多少篇文章： 12category = Category.objects.get(id=1)posts_count = category.post_set.count() 如果想要把这个结果放在category上，通过category.post_count可以访问： 123from django.db.models import Countcategories = Category.objects.annotate(posts_count=Count('post'))print(categories[0].posts_count) 这相当于给category动态增加了属性posts_count，而这个属性的值来源于Count(&#39;post&#39;) Sum：和Count类似，只是它是用来做合计的。比如想要统计目前所有文章加起来的访问量有多少 123from django.db.models import SumPost.objects.aggregate(all_pv=Sum('pv'))# 输出类似结果： &#123;'all_pv': 487&#125; 上面演示了QuerySet的annotate和aggregate的用法，其中前者用来给QuerySet结果增加属性，后者只是用来直接计算结果。 除了Count和Sum外，还有Avg、Min 和 Max等表达式。]]></content>
      <categories>
        <category>python</category>
        <category>web</category>
        <category>django</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>python</tag>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django添加生产环境配置]]></title>
    <url>%2F2019%2F10%2F13%2Fpython%2Fweb%2Fdjango%2Fdjango%E6%B7%BB%E5%8A%A0%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[在初始化阶段，Django会帮我们创建一个settings.py目录，所有Django的配置都在这个模块中，这样就会产生一些问题，比如同一份配置怎么来更好的区分开发环境和线上环境。当然可以在settings中编写if....else之类的条件，但是这样会导致settings会越来越复杂。理论上说开发环境的配置和生产环境的配置完全没关系，我们在维护生产环境的配置时，不需要考虑其他环境的配置，因此就有了拆分的逻辑。 具体的做法是把之前的settings.py中的内容放在settings/base.py中，删除原settings.py文件，同时新增__init__.py、develop.py和product.py。拆分独立模块之后，把需要的独立配置的内容分别放在不同的模块中，比如DATABASE配置，在develop.py中可以配置SQLite数据库，在produect.py中配置正式的MySQL数据库等。 下面是相关的具体配置(仅以创建develop，区分不同数据库为例，product类似)： 在settings.py同级目录下创建setttings的python package文件夹。 tips: 或者新建文件夹，创建__init__.py文件 将setting.py copy 到settings文件夹中，并重命名为base.py， 新建develop.py 在develop.py 中进行相关数据库配置 12345678910from .base import * DEBUG = True DATABASES = &#123; 'default': &#123; 'ENGINE': 'django.db.backends.sqlite3', 'NAME': os.path.join(BASE_DIR, 'db.sqlite3'), &#125; &#125; 修改django的启动文件 将manage.py和wsgi.py中的代码 os.environ.setdefault(&#39;DJANGO_SETTINGS_MODULE&#39;, &#39;ideatype.settings&#39;) 替换为 12profile = os.environ.get(&apos;TYPEIDEA_PROFILE&apos;, &apos;develop&apos;)os.environ.setdefault(&apos;DJANGO_SETTINGS_MODULE&apos;, &apos;ideatype.settings.%s&apos; %profile) 配置环境变量 开发环境：TYPEIDEA_PROFILE = develop 执行命令： echo export TYPEIDEA_PROFILE=develop &gt;&gt; /etc/profile source /etc/profile]]></content>
      <categories>
        <category>python</category>
        <category>web</category>
        <category>django</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>python</tag>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django单元测试]]></title>
    <url>%2F2019%2F10%2F12%2Fpython%2Fweb%2Fdjango%2Fdjango%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[单元测试​ 单元测试是实际开发中很重要但也经常被忽视的部分，其主要原因是编写Web功能测试所耗费的时间可能会大于你开发此功能的时间，因此，对于需要快速开发、上线的业务来说，项目中关于单元测试的部分很少。但是对于需要长期维护的项目，还需要考虑增加单元测试。只是第一次编写时会比较耗费时间，一旦基础结构完成，后续跟着功能的增加来增加单元测试并不会耗费多少时间，但是收益却是十分明显的。 ​ 单元测试的主要目的是让代码更健壮，尤其是在进行重构或者业务增加的时候，跑通单元测试，就意味着新加入的代码或者修改的代码没有问题。在实际开发中，单元测试的覆盖率没有那么高，其主要原因也是写单元测试的成本过高，尤其是对于很复杂的业务。下面就Django内置的测试工具进行说明。 TestCase中几个方法的说明​ 在Django中运行测试用例时，如果我们使用的SQLite数据库，Django会帮助我们创建一个基于内存测试的数据库，用于测试。 ​ 但是对于MySQL数据库，Django会直接用配置的数据库用户和密码创建一个名为test_student_db的数据库，用于测试。因此需要保证有建表和建库的权限 ​ 当然，也可以自定义测试用的数据库名称，通过setting配置： 12345678910DATABASE = &#123; &apos;default&apos;: &#123; &apos;ENGINE&apos;: &apos;django.db.backends.mysql&apos;, &apos;USER&apos;: &apos;mydatabaseuser&apos;, &apos;NAME&apos;: &apos;mydatabase&apos;, &apos;TEST&apos;: &#123; &apos;NAME&apos;: &apos;mytestdatabase&apos;, # 这里配置 &#125;， &#125;，&#125; Django提供了一个名为TestCase的基类，我们可以通过继承这个类来实现自己的测试逻辑。TestCase为我们系统了以下方法（需要用到的）： def setUp(self)：用来初始化环境，包括创建初始化的数据，或者做一些其他准备工作。 def test_xxxx(self)：方法后面的xxxx可以是任意东西。以test_开头的方法，会被认为是需要测试的方法，跑测试时会被执行，每个需要被测试的方法是相互独立的。 def tearDown(self)：跟setUp相对，用来清理测试环境和测试数据。 样例代码：Model层测试主要保证数据的写入和查询是可用的，同时也需要保证我们在model所提供的方法是复合预期的。 比如在Model中增加了sex_show这样的属性，用来展示sex这个字段的中文显示，而不是1，2。当然，这个功能是Django已经提供给我们的。 123456789101112131415161718192021222324252627282930313233343536373839from django.db import models# Create your models here.class Student(models.Model): SEX_ITEMS = [ (1, '男'), (2, '女'), (0, '未知'), ] STATUS_ITEMS = [ (0, '申请'), (1, '通过'), (2, '拒绝'), ] name = models.CharField(max_length=128, verbose_name='姓名') sex = models.IntegerField(choices=SEX_ITEMS, verbose_name='性别') profession = models.CharField(max_length=128, verbose_name='职业') email = models.EmailField(verbose_name='Email') qq = models.CharField(max_length=128, verbose_name='QQ') phone = models.CharField(max_length=128, verbose_name='电话') status = models.IntegerField(choices=STATUS_ITEMS, default=0, verbose_name='审核状态') created_time = models.DateTimeField(auto_now_add=True, editable=False, verbose_name='创建时间') def __str__(self): return '&lt;Student: ()&gt;'.format(self.name) class Meta: verbose_name = verbose_name_plural = '学员信息' @property def sex_show(self): return dict(self.SEX_ITEMS)[self.sex] @classmethod def get_all(cls): return cls.objects.all() views.py同级目录下的test.py，它是App初始化时Django默认帮我们创建的 1234567891011121314151617181920212223242526272829303132333435363738from django.test import TestCase, Clientfrom .models import Student# Create your tests here.class StudentTestCase(TestCase): def setUp(self) -&gt; None: Student.objects.create( name='test', sex=1, email='test@test.com', profession='test职业', qq='123', phone='123456', ) def test_create_and_sex_show(self): student = Student.objects.create( name='zhangsan', sex=1, email='zhangsan@test.com', profession='程序员', qq='123123', phone='12121212', ) self.assertEqual(student.sex_show, '男', '性别字段内容更展示不一致') def test_filter(self): Student.objects.create( name='lisi', sex=1, email='lisi@test.com', profession='程序员', qq='2124325', phone='11111111' ) name='test' students= Student.objects.filter(name=name) self.assertEqual(students.count(), 1, '应该只存在一个名称为&#123;&#125;的记录'.format(name)) 在setUp中，创建了一条数据用于测试。test_create_and_sex_show用来测试数据创建以及sex字段的正确展示，test_filter测试查询是否可用。 需要说明的是，每一个以test_开头的函数都是独立运行的。因此，setUp和tearDown也会在每个函数运行时被执行。简单理解就是每个函数都处于独立的运行环境。 View层测试test.py 添加如下代码 123456789101112131415161718192021def test_get_index(self): # 测试首页的可用性 client = Client() response = client.get('/') self.assertEqual(response.status_code, 200, 'status code must be 200!')def test_post_student(self): client = Client() data = dict( name='test_for_post', sex=1, email='333@test.com', profession='程序员', qq='333', phone='222', ) response=client.post('/', data) self.assertEqual(response.status_code, 302, 'status code must be 302!') response = client.get('/') self.assertEqual(b'test_for_post' in response.content, 'response content must contain `test_for_post`')]]></content>
      <categories>
        <category>python</category>
        <category>web</category>
        <category>django</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>python</tag>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WSGI——python-Web框架基础]]></title>
    <url>%2F2019%2F10%2F12%2Fpython%2Fweb%2Fdjango%2Fwsgi%2F</url>
    <content type="text"><![CDATA[1. 简介WSGI​ WSGI：web服务器网关接口，这是python中定义的一个网关协议，规定了Web Server如何跟应用程序交互。可以理解为一个web应用的容器，通过它可以启动应用，进而提供HTTP服务。​ 它最主要的目的是保证在Python中所有的Web Server程序或者说Gateway程序，能够通过统一的协议跟Web框架或者说Web应用进行交互。 uWSGI​ uWGSI：是一个web服务器，或者wsgi server服务器，他的任务就是接受用户请求，由于用户请求是通过网络发过来的，其中用户到服务器端之间用的是http协议，所以我们uWSGI要想接受并且正确解出相关信息，我们就需要uWSGI实现http协议，没错，uWSGI里面就实现了http协议。所以现在我们uWSGI能准确接受到用户请求，并且读出信息。现在我们的uWSGI服务器需要把信息发给Django，我们就需要用到WSGI协议，刚好uWSGI实现了WSGI协议，所以。uWSGI把接收到的信息作一次简单封装传递给Django，Django接收到信息后，再经过一层层的中间件，于是，对信息作进一步处理，最后匹配url，传递给相应的视图函数，视图函数做逻辑处理……后面的就不叙述了，然后将处理后的数据通过中间件一层层返回，到达Djagno最外层，然后，通过WSGI协议将返回数据返回给uWSGI服务器，uWSGI服务器通过http协议将数据传递给用户。这就是整个流程。 ​ 这个过程中我们似乎没有用到uwsgi协议，但是他也是uWSGI实现的一种协议，鲁迅说过，存在即合理，所以说，他肯定在某个地方用到了。我们过一会再来讨论 ​ 我们可以用这条命令：python manage.py runserver，启动Django自带的服务器。DJango自带的服务器（runserver 起来的 HTTPServer 就是 Python 自带的 simple_server）。是默认是单进程单多线程的，对于同一个http请求，总是先执行一个，其他等待，一个一个串行执行。无法并行。而且django自带的web服务器性能也不好，只能在开发过程中使用。于是我们就用uWSGI代替了。 为什么有了WSGI为什么还需要nginx？​ 因为nginx具备优秀的静态内容处理能力，然后将动态内容转发给uWSGI服务器，这样可以达到很好的客户端响应。支持的并发量更高，方便管理多进程，发挥多核的优势，提升性能。这时候nginx和uWSGI之间的沟通就要用到uwsgi协议。 2. 简单的Web Server在了解WSGI协议之前，首先看一个通过socker编程实现的Web服务的代码。 123456789101112131415161718192021222324252627282930313233343536373839404142import socketEOL1 = b'\n\n'EOL2 = b'\n\r\n'body = """hello world &lt;h1&gt; from test&lt;/h1&gt;"""response_params = [ 'HTTP/1.0 200 OK', 'DATE: Sun, 27 may 2019 01:01:01 GMT', 'Content-Type:text/plain; charset=utf-8', 'Content-Length: &#123;&#125;\r\n'.format(len(body.encode())), body,]response = '\r\n'.join(response_params)def handle_connection(conn, addr): request = b"" print('new conn', conn, addr) import time time.sleep(100) while EOL1 not in request and EOL2 not in request: request += conn.recv(1024) print(request) conn.send(response.encode()) conn.close()def main(): serversocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) serversocket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) serversocket.bind(('127.0.0.1', 8000)) serversocket.listen(5) print('http://127.0.0.1:8000') try: while True: conn, address = serversocket.accept() handle_connection(conn, address) finally: serversocket.close()if __name__ == '__main__': main() 多线程 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import errnoimport socketimport threadingimport timeEOL1 = b'\n\n'EOL2 = b'\n\r\n'body = """hello world &lt;h1&gt; from test&lt;/h1&gt;"""response_params = [ 'HTTP/1.0 200 OK', 'DATE: Sun, 27 may 2019 01:01:01 GMT', 'Content-Type:text/plain; charset=utf-8', 'Content-Length: &#123;&#125;\r\n'.format(len(body.encode())), body,]response = '\r\n'.join(response_params)def handle_connection(conn, addr): print(conn, addr) time.sleep(60) request = b"" while EOL1 not in request and EOL2 not in request: request += conn.recv(1024) print(request) current_thread = threading.currentThread() content_length = len(body.format(thread_name=current_thread.name).encode()) print(current_thread.name) conn.send(response.format(thread_name=current_thread.name, length=content_length).encode()) conn.close()def main(): serversocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) serversocket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) serversocket.bind(('127.0.0.1', 8000)) serversocket.listen(10) print('http://127.0.0.1:8000') serversocket.setblocking(True) # 设置socket为阻塞模式 try: i = 0 while True: try: conn, address = serversocket.accept() except socket.error as e: if e.args[0] != errno.EAGAIN: raise continue i += 1 print(i) t = threading.Thread(target=handle_connection, args=(conn, address), name='thread-%s'%i) t.start() finally: serversocket.close()if __name__ == '__main__': main() 3. 简单的WSGI Application该协议分为两个部分： Web Server 或者Gateway 监听在某个端口上接收外部的请求 Web Application ​ Web Server接收请求之后，会通过WSGI协议规定的方式把数据传递给Web Application，在Web Application中处理完之后，设置对应的状态和header，之后返回body部分。Web Server拿到返回的数据之后，再进行HTTP协议的封装，最终返回完整的HTTPResponse数据。 下面我们来实现一个简单的应用： 1234567app.pydef simple_app(environ, start_response): status = '200 OK' response_headers = [('Content-type', 'text/plain')] start_response(status, response_headers) return [b'Hello world! -by test \n'] 我们需要一个脚本运行上面这个应用： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import osimport sysfrom app import simple_appdef wsgi_to_bytes(s): return s.encode()def run_with_cgi(application): environ = dict(os.environ.items()) environ['wsgi.input'] = sys.stdin.buffer environ['wsgi.errors'] = sys.stderr environ['wsgi.version'] = (1, 0) environ['wsgi.multithread'] = False environ['wsgi.multiprocess'] = True environ['wsgi.run_once'] = True if environ.get('HTTPS', 'off') in ('on', '1'): environ['wsgi.url_scheme'] = 'https' else: environ['wsgi.url_scheme'] = 'http' headers_set = [] headers_sent = [] def write(data): out = sys.stdout.buffer if not headers_set: raise AssertionError('Write() before start_response()') elif not headers_sent: # 在输出第一行数据之前，先发送响应头 status, response_headers = headers_sent[:] = headers_set out.write(wsgi_to_bytes('Status: %s\r\n' % status)) for header in response_headers: out.write(wsgi_to_bytes('%s: %s\r\n' % header)) out.write(wsgi_to_bytes('\r\n')) out.write(data) out.flush() def start_response(status, response_headers, exc_info=None): if exc_info: try: if headers_sent: # 如果已经发送了header，则重新抛出原始异常信息 raise (exc_info[0], exc_info[1], exc_info[2]) finally: exc_info = None elif headers_set: raise AssertionError('*Headers already set!') headers_set[:] = [status, response_headers] return write result = application(environ, start_response) try: for data in result: if data: write(data) if not headers_sent: write('') finally: if hasattr(result, 'close'): result.close()if __name__ == '__main__': run_with_cgi(simple_app) 运行结果： 1234Status: 200 OKContent-type: text/plainHello world! -by test 如果不是windows系统，还可以采用另一种方式运行： pip install gunicorn gunicorn app:simle_app 4. 理解​ 对于上述代码我们只需要关注一点，result = application(environ, start_response)，我们要实现的Application，只需要能够接收一个环境变量以及一个回调函数即可。但处理完请求之后，通过回调函数（start_response）来设置response的状态和header，最终返回结果，也就是body。 WSGI协议规定，application必须是一个可调用对象，这意味这个对象既可以是Python中的一个函数，也可以是一个实现了__call__方法的类的实例，比如： 样例一1234567891011class AppClass(object): status = '200 OK' response_headers = [('Content-type', 'text/plain')] def __call__(self, environ, start_response): print(environ, start_response) start_response(self.status, self.response_headers) return [b'Hello AppClass.__call__\n']application = AppClass() gunicorn app: application运行上述文件 样例二​ 除此之外，我们还可以通过另一种方式实现WSGI协议，从上面的simpleapp和这里的AppClass._call的返回值来看，WSGI Server只需要返回一个可迭代的对象就行 1234567891011class AppClassIter(object): status = '200 OK' response_headers = [('Content-type', 'text/plain')] def __init__(self, environ, start_response): self.environ = environ self.start_response = start_response def __iter__(self): self.start_response(self.status, self.response_headers) yield b'Hello AppClassIter\n' gunicorn app: AppClassIter运行上述文件 ​ 这里的启动命令并不是一个类的实例，而是类本身。通过上面两个代码，我们可以看到能够被调用的方法会传environ和start_response过来，而现在这个实现没有可调用的方式，所以就需要在实例化的时候通过参数传递进来，这样在返回body之前，可以先调用start_response方法。 ​ 因此，可以推测出WSGI Server是如何调用WSGI Application的，大概代码如下： 12345678910111213def start_response(status, headers): # 伪代码 set_status(status) for k, v in headers: set_header(k, v)def handle_conn(conn): # 调用我们定义的application(也就是上面的simple_app, 或者是AppClass的实例，或者是AppClassIter本身) app = application(environ, start_response) # 遍历返回的结果，生成response for data in app: response += data conn.sendall(response) 5. WSGI中间件和Werkzeug​ WSGI中间件可以理解为Python中的一个装饰器，可以在不改变原方法的情况下对方法的输入和输出部分进行处理。 类似这样： 1234def simple_app(enbiron, start_response): response = Response('Hello World', start_response=start_response) response.set_header('Content-Type', 'text/plain') # 这个函数里面调用start_response return response 这样就看起来更加自然一点。 ​ 因此，就存在Werkzeug这样的WSGI工具集，让你能够跟WSGI协议更加友好的交互。从理论上来看，我们可以直接通过WSGI协议的简单实现写一个Web服务。但是有了Werkzeug之后，我们可以写的更加容易。 6. 杂谈django 的并发能力真的是令人担忧，这里就使用 nginx + uwsgi 提供高并发 nginx 的并发能力超高，单台并发能力过万（这个也不是绝对），在纯静态的 web 服务中更是突出其优越的地方，由于其底层使用 epoll 异步IO模型进行处理，使其深受欢迎 做过运维的应该都知道， Python需要使用nginx + uWSGI 提供静态页面访问，和高并发 php 需要使用 nginx + fastcgi 提供高并发， java 需要使用 nginx + tomcat 提供 web 服务 1234567891011121314151617django 原生为单线程序，当第一个请求没有完成时，第二个请求辉阻塞，知道第一个请求完成，第二个请求才会执行。Django就没有用异步，通过线程来实现并发，这也是WSGI普遍的做法，跟tornado不是一个概念官方文档解释django自带的server默认是多线程django开两个接口,第一个接口sleep(20),另一个接口不做延时处理(大概耗时几毫秒)先请求第一个接口,紧接着请求第二个接口,第二个接口返回数据,第一个接口20秒之后返回数据证明django的server是默认多线程启动uWSGI服务器# 在django项目目录下 Demo工程名uwsgi --http 0.0.0.0:8000 --file Demo/wsgi.py经过上述的步骤测试,发现在这种情况下启动django项目,uWSGI也是单线程,访问接口需要&quot;排队&quot;不给uWSGI加进程,uWSGI默认是单进程单线程uwsgi --http 0.0.0.0:8000 --file Demo/wsgi.py --processes 4 --threads 2# processes: 进程数 # processes 和 workers 一样的效果 # threads : 每个进程开的线程数经过测试,接口可以&quot;同时&quot;访问,uWSGI提供多线程 Python因为GIL的存在,在一个进程中,只允许一个线程工作,导致单进程多线程无法利用多核 多进程的线程之间不存在抢GIL的情况,每个进程有一个自己的线程锁,多进程多GIL]]></content>
      <categories>
        <category>python</category>
        <category>web</category>
        <category>django</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>python</tag>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git统计项目中成员代码量]]></title>
    <url>%2F2019%2F10%2F11%2Fgit%2Fgit%E7%BB%9F%E8%AE%A1%E9%A1%B9%E7%9B%AE%E6%88%90%E5%91%98%E4%BB%A3%E7%A0%81%E9%87%8F%2F</url>
    <content type="text"><![CDATA[查看git上个人代码量1git log --author=&quot;username&quot; --pretty=tformat: --numstat | awk &apos;&#123; add += $1; subs += $2; loc += $1 - $2 &#125; END &#123; printf &quot;added lines: %s, removed lines: %s, total lines: %s\n&quot;, add, subs, loc &#125;&apos; - 统计每个人的增删行数1git log --format=&apos;%aN&apos; | sort -u | while read name; do echo -en &quot;$name\t&quot;; git log --author=&quot;$name&quot; --pretty=tformat: --numstat | awk &apos;&#123; add += $1; subs += $2; loc += $1 - $2 &#125; END &#123; printf &quot;added lines: %s, removed lines: %s, total lines: %s\n&quot;, add, subs, loc &#125;&apos; -; done 查看仓库提交者排名前 51git log --pretty=&apos;%aN&apos; | sort | uniq -c | sort -k1 -n -r | head -n 5 贡献者统计：1git log --pretty=&apos;%aN&apos; | sort -u | wc -l 提交数统计：1git log --oneline | wc -l]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nodejs pdfkit]]></title>
    <url>%2F2019%2F10%2F09%2Fjavascript%2Fnodejs%2F04%E3%80%81pdfkit%2F</url>
    <content type="text"><![CDATA[pdfkit官方文档0、准备需要引入两个包，首先要npm install pdfkit安装pdfkit包12const PDF = require('pdfkit');const fs = require('fs');通过下面方法创建pdf对象，如果没有传入任何的参数，默认自动创建第一页，页面大小为A4 doc = new PDF(); 通过管道流创建名为test.pdf的文件 doc.pipe(fs.createWriteStream(&#39;test.pdf&#39;)); 写入内容 doc.text(&#39;test&#39;) 结束写入，生成文件 doc.end() 1、页面和文本设置首先要解决的问题就是页面的设置了，在这里我们要求的是16:9的页面比例，而并非默认的A4纸的页面。 创建对象，取消生成第一页 doc = new PDF({ autoFirstPage: false }); 添加页面，设置页面大小 doc.addPage({ margin: 10, size: [960,540] }); 设置中文字体，支持中文 此时，需要有一个本地中文文件，在C:\Windows\Fonts中可以找到，这里使用的是微软雅黑 doc.font(&#39;../WeiRuanYaHei-1.ttf&#39;); 在输入文本前，创建文件流 doc.pipe(fs.createWriteStream(&#39;test.pdf&#39;)); 插入文本 123doc.text('Hello world!')doc.text('Hello world!', 100, 100) //设定文本位置，如果要上下移动，只需要使用您要移动的行数（默认为1）调用moveDownor moveUp方法。doc.lineGap(4); 文本属性(同样的可以直接配置doc的属性作为全局属性) lineBreak-设置为false禁用所有换行 width -文本应换行的宽度（默认情况下，页面宽度减去左右边距） height -文本应剪切到的最大高度 ellipsis-太长时显示在文本末尾的字符。设置为true使用默认字符。 columns -文本流入的列数 columnGap -每列之间的间距（默认为1/4英寸） indent -以PDF磅为单位（每英寸72英寸）的缩进量 paragraphGap -文本各段之间的间距 lineGap -每行文字之间的间距 wordSpacing -文本中每个单词之间的间距 characterSpacing -文本中每个字符之间的间距 fill-是否填写文字（true默认情况下） stroke -是否描边文本 link -链接此文本的URL（创建注释的快捷方式） underline -是否在文字下划线 strike -是否删除文字 oblique-是否倾斜文字（角度或度数true） baseline-文本相对于其插入点的垂直对齐方式（值为canvas textBaseline） 可以连续对文本进行操作 123456doc.fillColor('green') .text(lorem.slice(0, 500), &#123; width: 465, continued: true &#125;).fillColor('red') .text(lorem.slice(500)); 2、插入图片和图形12doc.image('image/img.png', x, y, &#123;width: 100, height: 100&#125;) // 插入图片，并设置图片大小doc.rect(100, 100, 90, 100).dash(3, &#123;space: 3&#125;).fillAndStroke("#11487B"); // 在（100， 100）处，画一个90*100的方形，并用#11487B颜色填充，设置边框为曲线，space为线条长度。 image方法可以插入图片 rect可以画一个方形，可以使用dash使边框变为虚线，undash再变为实线。 fillAndStroke填充颜色，第二个参数为描边颜色 1234567891011const PDF = require('pdfkit');const fs = require('fs');const doc = new PDF(&#123; autoFirstPage: false &#125;);//creating a new PDF objectdoc.lineGap(4);doc.addPage(&#123; margin: 10, size: [960,540] &#125;);doc.font('./WeiRuanYaHei-1.ttf');doc.pipe(fs.createWriteStream('test.pdf')); //creating a write streamconst x = 30doc.rect(x, 85, 315, 145).dash(3, &#123;space: 3&#125;).strokeColor("#808080").stroke();doc.image('./kit/banner.png', x+110, 110, &#123; width: 90 &#125;);doc.end() 上述代码生成结果：]]></content>
      <categories>
        <category>javascript</category>
        <category>nodejs</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>javascript</tag>
        <tag>nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx配置]]></title>
    <url>%2F2019%2F09%2F30%2Fnginx%2Fnginx%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[整体结构Nginx配置文件的整体结构包含以下几个部分1、全局块该部分配置主要影响Nginx全局，通常包括下面几个部分 配置运行Nginx服务器用户【组】 worker process数 Nginx进程PID存放路径 错误日志的存放路径 配置文件的引入 2、events块该部分配置主要影响Nginx服务器与用户的网络连接，主要包括： 设置网络连接的序列化 是否允许同时接收多个网络连接 时间驱动模型的选择 最大连接数的配置 3、http块 定义MIMI-Type 自定义服务日志 允许sendfile方式传输文件 连接超时时间 单连接请求数上限 4、server块 配置网络监听 基于名称的虚拟主机配置 基于IP的虚拟主机配置 5、location块 location配置 请求根目录配置 更改location的URI 网站默认首页配置 配置信息1、配置运行Nginx服务器用户【组】指令格式 user user [group]; user: 指定可以运行Nginx服务器的用户 group: 可选项，可以运行Nginx服务器的用户组 如果user指令不配置，或者配置为user nobody nobody 则默认所有的用户都可以启动Nginx进程 2、worker process数配置Nginx服务器实现并发处理服务的关键，指令格式： worker_processes number | auto; number: Nginx进程最多可以产生的worker process数 auto: Nginx进程将自动检测 可以通过ps -aux|grep nginx查看进程数量 3、Nginx进程PID存放路径Nginx进程是作为系统守护进程在运行，需要在某文件中保存当前运行程序的主进程号，Nginx支持该保存文件路径的自定义，指令格式 pid file; file: 指定存放路径和文件名称 如果不指定默认置于路径logs/nignx.pid 4、错误日志的存放路径指定格式 error_log file | stderr; file: 日志输出到某个文件file stderr: 日志输出到标准错误输出 5、配置文件的引入指令格式 include file; 该指令主要用于将其他的Nginx配置或者第三方模块的配置引用到当前的主配置文件中 6、设置网络连接的序列化指令格式 accept_mutex on | off; 该指令默认为on状态，表示会对多个Nginx进程接收连接进行序列化，防止多个进程对连接的争抢。 注意：当一个新网络裂解来到时，多个worker进程会被同时唤醒，但仅仅只有一个进程可以真正获得连接并做处理。如果每次唤醒的进程数目过多的话，会影响一部分性能。如果accept_mutex on，name多个worker将是以串行的方式来处理，其中只有一个worker会被唤醒；反之若accept_mutex off，那么所有的worker都会被唤醒，不过只有一个worker能获取新连接，其他的worker会重新进入休眠状态。 7、是否允许同时接收多个网络连接指令格式 multi_accept on | off; 该指令默认为off状态，指每个worker process一次只能接收一个新到达的网络连接。若想让每个Nginx的workerprocess都有能力同时接收多个网络连接，则需要开启该配置 8、事件驱动模型的选择指令格式 use model model模型可选择项包括：selece, poll, kqueue, epoll, rtsig等 9、最大连接数的配置指令格式 worker_connections number; number默认值为512，表示允许每一个worker process 可以同时开启最大连接数 10、定义MIME-Type指令格式： include mime.types; default_type mime-type; MIME-Type指的是网络资源的媒体类型，也即前端请求的资源类型 include指令将mime.types文件包含进来 mime.types文件内容如下，其实就是一个types结构，里面包含了各种浏览器能够识别的MIME类型以及对应类型的文件名后缀名字 11、自定义服务日志指令格式 access_log path [format]; path: 自定义服务日志的路径 + 名称 format: 可选项，自定义服务日至的字符串格式 12、允许sendfile方式传输文件指令格式 sendfile on | off; sendfile_max_chunk size; 前者用于开启或关闭使用sendfile()传输文件，默认off 后者指令若size&gt;0，则Nginx进程的每个workerprocess每次调用sendfile()传输的数据最大不能超过此值；若size=0，则表示不限制。默认为0 13、连接超时时间配置指令格式： keepalive_timeout timeout [header_timeout] timeout 表示server端对连接的保持时间，默认为75秒 header_timeout为可选项，表示在应答报文头部的Keep-Alive域设置超时时间Keep-Alive:time=header_timeout 14、单连接请求数上限指令格式 keepalive_requests number; 该指令用于限制用户通过某一个连接向Nginx服务器发起请求的次数 15、配置网络监听指令格式 配置监听IP地址listen ip[:PORT]; 配置监听端口listen PORT; 例如 listen 192.168.31.177:8080; 监听具体IP和具体端口上的连接 listen 192.168.31.177; 监听IP上所有端口上的连接 listen 8080; 监听具体端口上的所有IP的连接 16、基于名称和IP的虚拟主机配置指令格式 server_name name1 name2 ... server_name IP地址 name可以有多个并列名称，而且此处的name支持正则表达式书写 17、location配置指令格式 location [ = | ~ | ~* | ^~ ] uri {...} 这里的uri分为标准uri和正则uri，两者的唯一区别是uri中是否包含正则表达式 uri前面的方括号中的内容是可选项，解释如下： =：用于标准uri前，要求请求字符串与uri严格匹配，一旦匹配成功则停止 ~：用于正则uri前，并且区分大小写 ~*：用于正则uri前，但不区分大小写 ^~：用于标准uri前，要求Nginx找到标识uri和请求字符串匹配度最高的location后，立即使用此location处理请求，而不再使用location块中的正则uri和请求字符串做匹配 18、请求根目录配置指令格式 root path; path：Nginx接收到请求以后查找资源的根目录路径 还可以通过alias指令来更改location接收到的URI请求路径，指令为： alias path; path为修改后的根路径 19、设置网站的默认首页指令格式 index file ......; file可以包含多个用空格隔开的文件名，首先找到哪个页面，就使用哪个页面响应请求 附录nginx整体结构]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>服务器</tag>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ES查询]]></title>
    <url>%2F2019%2F09%2F17%2Fes%2FES%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[漫画版原理介绍搜索引擎的核心：倒排索引elasticsearch 基于Lucene的，封装成一个restful的api，通过api就可进行操作(Lucene是一个apache开放源代码的全文检索引擎工具包，必须使用Java作为开发语言集成到项目中) 分布式 elasticsearch和DB的对应关系12DB -&gt; Databases -&gt; Tables -&gt; Rows -&gt; ColumnsES -&gt; Indices -&gt; Types -&gt; Documents -&gt; Fields 如果我们要在ES中创建一条数据，步骤为 1、首先创建document，每个文档包含响应的信息，然后赋予每个文档一种类型，为一个类型创建一条索引，最后将索引存储到es中。 1、集群的健康在Elasticsearch集群中可以监控统计很多信息，但是只有一个是最重要的：集群健康(cluster health)。集群健康有三种状态：green、yellow或red。 1GET /_cluster/health 在一个没有索引的空集群中运行如上查询，将返回这些信息： 1234567891011121314151617&#123; "cluster_name": "elasticsearch", "status": "yellow", &lt;1&gt; "timed_out": false, "number_of_nodes": 1, "number_of_data_nodes": 1, "active_primary_shards": 11, "active_shards": 11, "relocating_shards": 0, "initializing_shards": 0, "unassigned_shards": 11, &lt;2&gt; "delayed_unassigned_shards": 0, "number_of_pending_tasks": 0, "number_of_in_flight_fetch": 0, "task_max_waiting_in_queue_millis": 0, "active_shards_percent_as_number": 50.0&#125; status 是我们最感兴趣的字段 unassigned_shards 没有启用的复制分片，原因是还没有分配给节点。同一个节点上保存相同的数据副本是没有必要的。 status字段提供一个综合的指标来表示集群的的服务状况。三种颜色各自的含义： 颜色 意义 green 所有主要分片和复制分片都可用 yellow 所有主要分片可用，但不是所有复制分片都可用 red 不是所有的主要分片都可用 2、搜索（以下相关搜索均以5.6版本为例，5.2.2版本之前的参考https://es.xiaoleilu.com（书籍涉及到的版本为2.x）） 搜索的地址为 ​ /索引/类型/_search 我们创建一个基于alpha数据库的ES，索引为data库，类型为entity表，所以搜索的地址为：/data/entity/_search,请求可以为GET，也可为POST，注意，两者的请求数据均在body中 tips: python flask中提供了request方法，可以使用request.data获取get请求的body 可以直接发送搜索请求，不附加任何条件，默认会返回10条数据，数据内容会在响应体的hits中 返回字段解释： 1、took Elasticsearch执行搜索的时间（以毫秒为单位） 2、timed_out 检索是否超时，可以定义超时时间GET /search?timeout=10ms 3、_shards 检索了多少分片，成功失败了多少 4、hits 检索的结果 5、hits.total 检索文档的总数 6、hits.hits 实际的检索结果，默认前10个 7、hits.sort 排序的key（如果按分值排序的话则不显示） 8、每个节点都有一个_score字段，是文档的相关性得分，衡量了文档与查询的匹配程度。返回结果默认按照_score降序排列，max_score指的是所有文档匹配查询中_score的最大值 1、普通简单查询字符串查询这种查询，我们只需要像一般传递URL参数的方法去传递查询语句 /data/entity/_search?q=name:张三 如果没有指定字段，查询字符串搜索使用的_all字段 短语搜索搜索名字带有李四的数据 1234567&#123; &quot;query&quot; : &#123; &quot;match_phrase&quot; : &#123; &quot;name&quot; : &quot;李四&quot; &#125; &#125;&#125; 高亮显示123456789101112&#123; &quot;query&quot; : &#123; &quot;match_phrase&quot; : &#123; &quot;name&quot; : &quot;李四&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;name&quot;:&#123;&#125; &#125; &#125;&#125; result 1234567891011121314151617181920212223242526272829303132333435&#123; "took": 19, "timed_out": false, "_shards": &#123; "total": 5, "successful": 5, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 1, "max_score": 29.54232, "hits": [ &#123; "_index": "data", "_type": "entity", "_id": "5c7397d2ac623e001071c7b3", "_score": 29.54232, "_source": &#123; "name": "李四", "nick": "李四", "abs": "", "type": 1.0, "intro": "", "_deleted": false &#125;, "highlight": &#123; "name": [ "&lt;em&gt;李&lt;/em&gt;&lt;em&gt;四&lt;/em&gt;" ] &#125; &#125; ] &#125;&#125; 2、多索引查询 /_search在所有索引的所有类型中搜索 /test/_search在索引test的所有类型中查找 /test,us/_search 在索引test和us的所有类型中搜索 /t*,u*/_search在t或u开头的索引的所有类型中搜索 /test/user/_search在索引test的user类型中搜索 /test,u/user,class/_search 在索引test和u的类型为user和class中搜索 /_all/user,class/_search在所有索引的user和class类型中搜索 3、分页传参时，接收from（默认为0）和size（默认为10）参数 如果想每页显示5个结果，页码从1到3，请求如下 123GET /_search?size=5GET /_search?size=5&amp;from=5GET /_search?size=5&amp;from=10 注：假设一个有5个主分片的索引中搜索，当我们请求结果的第一页时（1-10），每个分片都会产生自己最顶端的10个结果，然后返回它们给请求节点，然后在排序这50个结果，选出顶端的10个结果 4、结构化查询以json请求体的形式出现 1、查询子句123456789101112131415&#123; QUERY_NAME: &#123; ARGUMENT: VALUE, ARGUMENT: VALUE,... &#125;&#125;或者指向一个指定的字段&#123; QUERY_NAME: &#123; FIELD_NAME: &#123; ARGUMENT: VALUE, ARGUMENT: VALUE,... &#125; &#125;&#125; 完整的请求 12345678GET /_search&#123; "query": &#123; "match": &#123; "name": "张三" &#125; &#125;&#125; 2、合并多子句 叶子子句(leaf clauses)(比如match子句) 用以在将查询字符串与一个字段(或多字段)进行比较 复合子句(compound) 用以合并其他的子句。例如，bool子句允许你合并其他的合法子句，must，must_not或者should，如果可能的话： 1234567&#123; "bool": &#123; "must": &#123; "match": &#123; "tweet": "elasticsearch" &#125;&#125;, "must_not": &#123; "match": &#123; "name": "mary" &#125;&#125;, "should": &#123; "match": &#123; "tweet": "full text" &#125;&#125; &#125;&#125; 复合子句能合并任意其他查询子句，包括其他的复合子句。 这就意味着复合子句可以相互嵌套，从而实现非常复杂的逻辑。 12345678910111213&#123; "bool": &#123; "must": &#123; "match": &#123; "email": "business opportunity" &#125;&#125;, "should": [ &#123; "match": &#123; "starred": true &#125;&#125;, &#123; "bool": &#123; "must": &#123; "folder": "inbox" &#125;&#125;, "must_not": &#123; "spam": true &#125;&#125; &#125;&#125; ], "minimum_should_match": 1 &#125;&#125; 3、match_all 查询使用match_all 可以查询到所有文档，是没有查询条件下的默认语句。 123&#123; "match_all": &#123;&#125;&#125; 此查询常用于合并过滤条件。 比如说你需要检索所有的邮箱,所有的文档相关性都是相同的，所以得到的_score为1 4、match 查询使用match时，可以选择短语查找type:phrase或者operator:&#39;and&#39; 同时，支持slop选项，表示相隔多远时仍然将文档视为匹配（int） match查询是一个标准查询，不管你需要全文本查询还是精确查询基本上都要用到它。 如果你使用 match 查询一个全文本字段，它会在真正查询之前用分析器先分析match一下查询字符： 12345&#123; "match": &#123; "tweet": "About Search" &#125;&#125; 如果用match下指定了一个确切值，在遇到数字，日期，布尔值或者not_analyzed 的字符串时，它将为你搜索你给定的值： 1234&#123; "match": &#123; "age":26&#125;&#125;&#123; "match": &#123; "date":"2014-09-01"&#125;&#125;&#123; "match": &#123; "public": true&#125;&#125;&#123; "match": &#123; "tag":"full_text"&#125;&#125; 提示： 做精确匹配搜索时，你最好用过滤语句，因为过滤语句可以缓存数据。 match查询只能就指定某个确切字段某个确切的值进行搜索，而你要做的就是为它指定正确的字段名以避免语法错误。 5、multi_match 查询multi_match查询允许你做match查询的基础上同时搜索多个字段： 1234"multi_match": &#123; "query": "full text search", "fields": [ "title", "body" ]&#125; type: 1.最佳字段(Best fields): 当搜索代表某些概念的单词时，例如”brown fox”，几个单词合在一起表达出来的意思比单独的单词更多。类似title和body的字段，尽管它们是相关联的，但是也是互相竞争着的。文档在相同的字段中应该有尽可能多的单词(译注：搜索的目标单词)，文档的分数应该来自拥有最佳匹配的字段。 2.多数字段(Most fields): 一个用来调优相关度的常用技术是将相同的数据索引到多个字段中，每个字段拥有自己的分析链(Analysis Chain)。 主要字段会含有单词的词干部分，同义词和消除了变音符号的单词。它用来尽可能多地匹配文档。 相同的文本可以被索引到其它的字段中来提供更加精确的匹配。一个字段或许会包含未被提取成词干的单词，另一个字段是包含了变音符号的单词，第三个字段则使用shingle来提供关于单词邻近度(Word Proximity)(match_phrase)的信息。 以上这些额外的字段扮演者signal的角色，用来增加每个匹配的文档的相关度分值。越多的字段被匹配则意味着文档的相关度越高。 使用most_fields存在的问题 使用most_fields方法执行实体查询有一些不那么明显的问题： 它被设计用来找到匹配任意单词的多数字段，而不是找到跨越所有字段的最匹配的单词。 它不能使用operator或者minimum_should_match参数来减少低相关度结果带来的长尾效应。 每个字段的词条频度是不同的，会互相干扰最终得到较差的排序结果。 3.跨字段(Cross fields): 对于一些实体，标识信息会在多个字段中出现，每个字段中只含有一部分信息： Person: first_name 和 last_name Book: title, author, 和 description Address: street, city, country, 和 postcode 此时，我们希望在任意字段中找到尽可能多的单词。我们需要在多个字段中进行查询，就好像这些字段是一个字段那样。 以上这些都是多词，多字段查询，但是每种都需要使用不同的策略。我们会在本章剩下的部分解释每种策略。 12345678910111213&#123; "query": &#123; "multi_match": &#123; "query": "张三", "type": "best_fields", "fields": [ "name", "abs" ], "minimum_should_match": "30%" &#125; &#125;&#125; 加权 field: [“name^2”, “abs”] 字段也可以使用通配符：”fields”: “*_title” 6、bool 查询bool 查询与 bool 过滤相似，用于合并多个查询子句。不同的是，bool 过滤可以直接给出是否匹配成功， 而bool 查询要计算每一个查询子句的 _score （相关性分值）。 must: 查询指定文档一定要被包含。 must_not: 查询指定文档一定不要被包含。 should: 查询指定文档，有则可以为文档相关性加分。 以下查询将会找到 title 字段中包含 “how to make millions”，并且 “tag” 字段没有被标为 spam。 如果有标识为 “starred” 或者发布日期为2014年之前，那么这些匹配的文档将比同类网站等级高： 123456789101112&#123; "query": &#123; "bool": &#123; "must": &#123; "match": &#123; "title": "quick" &#125;&#125;, "must_not": &#123; "match": &#123; "title": "lazy" &#125;&#125;, "should": [ &#123; "match": &#123; "title": "brown" &#125;&#125;, &#123; "match": &#123; "title": "dog" &#125;&#125; ] &#125; &#125;&#125; 提示： 如果bool 查询下没有must子句，那至少应该有一个should子句。但是 如果有must子句，那么没有should子句也可以进行查询。 可以使用bool代替match 注意：term query会去倒排索引中寻找确切的term，它并不知道分词器的存在，这种查询适合keyword、numeric、date等明确值的 以下来自官网：Avoid using the term query for text fields. By default, Elasticsearch changes the values of text fields as part of analysis. This can make finding exact matches for text field values difficult. To search text field values, use the match query instead. 123456789101112131415161718192021222324252627282930&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &#123; &quot;query&quot;: &quot;张三&quot;, &quot;operator&quot;: &quot;and&quot; &#125; &#125; &#125;&#125;=&gt;&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;term&quot;: &#123; &quot;name&quot;: &quot;张&quot; &#125; &#125;, &#123; &quot;term&quot;: &#123; &quot;name&quot;: &quot;三&quot; &#125; &#125; ] &#125; &#125;&#125; 7、多词查询12345678910&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &#123; &quot;query&quot;: &quot;张三&quot;, &quot;operator&quot;: &quot;and&quot; &#125; &#125; &#125;&#125; 控制精度 minimum_should_match指定必须存在的最小数量或百分比 Type Example Description Integer 3 Indicates a fixed value regardless of the number of optional clauses. Negative integer -2 Indicates that the total number of optional clauses, minus this number should be mandatory. Percentage 75% Indicates that this percent of the total number of optional clauses are necessary. The number computed from the percentage is rounded down and used as the minimum. Negative percentage -25% Indicates that this percent of the total number of optional clauses can be missing. The number computed from the percentage is rounded down, before being subtracted from the total to determine the minimum. Combination 3&lt;90% A positive integer, followed by the less-than symbol, followed by any of the previously mentioned specifiers is a conditional specification. It indicates that if the number of optional clauses is equal to (or less than) the integer, they are all required, but if it’s greater than the integer, the specification applies. In this example: if there are 1 to 3 clauses they are all required, but for 4 or more clauses only 90% are required. Multiple combinations 2&lt;-25% 9&lt;-3 Multiple conditional specifications can be separated by spaces, each one only being valid for numbers greater than the one before it. In this example: if there are 1 or 2 clauses both are required, if there are 3-9 clauses all but 25% are required, and if there are more than 9 clauses, all but three are required. 123456789101112&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;Indian&quot; &#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;Corporaton&quot; &#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;of&quot; &#125;&#125; ], &quot;minimum_should_match&quot;: 2 &#125; &#125;&#125; 8、提交权重boost 123456789101112131415161718192021&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;name&quot;: &#123; &quot;query&quot;: &quot;张三&quot;, &quot;boost&quot;: 1 &#125; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;abs&quot;:&quot;test&quot;, &#125; &#125; ] &#125; &#125;&#125; 9、dis_max查询不使用 bool 查询，可以使用 dis_max 即分离最大化查询(Disjuction Max Query)。Disjuction的意思”OR”(而Conjunction的意思是”AND”)，因此Disjuction Max Query的意思就是返回匹配了任何查询的文档，并且分值是产生了最佳匹配的查询所对应的分值： 12345678910&#123; &quot;query&quot;: &#123; &quot;dis_max&quot;: &#123; &quot;queries&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;Brown fox&quot; &#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;body&quot;: &quot;Brown fox&quot; &#125;&#125; ] &#125; &#125;&#125; 它会产生我们期望的结果： 1234567891011121314151617181920&#123; &quot;hits&quot;: [ &#123; &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.21509302, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;Keeping pets healthy&quot;, &quot;body&quot;: &quot;My quick brown fox eats rabbits on a regular basis.&quot; &#125; &#125;, &#123; &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.12713557, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;Quick brown rabbits&quot;, &quot;body&quot;: &quot;Brown rabbits are commonly seen.&quot; &#125; &#125; ]&#125; 最佳字段查询的调优如果用户搜索的是”quick pets”，那么会发生什么呢？两份文档都包含了单词quick，但是只有文档2包含了单词pets。两份文档都没能在一个字段中同时包含搜索的两个单词。 一个像下面那样的简单dis_max查询会选择出拥有最佳匹配字段的查询子句，而忽略其他的查询子句： 123456789101112131415161718192021222324252627282930&#123; &quot;query&quot;: &#123; &quot;dis_max&quot;: &#123; &quot;queries&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;Quick pets&quot; &#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;body&quot;: &quot;Quick pets&quot; &#125;&#125; ] &#125; &#125;&#125;&#123; &quot;hits&quot;: [ &#123; &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.12713557, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;Quick brown rabbits&quot;, &quot;body&quot;: &quot;Brown rabbits are commonly seen.&quot; &#125; &#125;, &#123; &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.12713557, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;Keeping pets healthy&quot;, &quot;body&quot;: &quot;My quick brown fox eats rabbits on a regular basis.&quot; &#125; &#125; ]&#125; 可以发现，两份文档的分值是一模一样的。 我们期望的是同时匹配了title字段和body字段的文档能够拥有更高的排名，但是结果并非如此。需要记住：dis_max查询只是简单的使用最佳匹配查询子句得到的_score。 tie_breaker 但是，将其它匹配的查询子句考虑进来也是可能的。通过指定tie_breaker参数： 1234567891011&#123; &quot;query&quot;: &#123; &quot;dis_max&quot;: &#123; &quot;queries&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;Quick pets&quot; &#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;body&quot;: &quot;Quick pets&quot; &#125;&#125; ], &quot;tie_breaker&quot;: 0.3 &#125; &#125;&#125; 它会返回以下结果： 1234567891011121314151617181920&#123; &quot;hits&quot;: [ &#123; &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.14757764, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;Keeping pets healthy&quot;, &quot;body&quot;: &quot;My quick brown fox eats rabbits on a regular basis.&quot; &#125; &#125;, &#123; &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.124275915, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;Quick brown rabbits&quot;, &quot;body&quot;: &quot;Brown rabbits are commonly seen.&quot; &#125; &#125; ]&#125; 现在文档2的分值比文档1稍高一些。 tie_breaker参数会让dis_max查询的行为更像是dis_max和bool的一种折中。它会通过下面的方式改变分值计算过程： 取得最佳匹配查询子句的_score。 将其它每个匹配的子句的分值乘以tie_breaker。 将以上得到的分值进行累加并规范化。 通过tie_breaker参数，所有匹配的子句都会起作用，只不过最佳匹配子句的作用更大。 NOTE tie_breaker的取值范围是0到1之间的浮点数，取0时即为仅使用最佳匹配子句(译注：和不使用tie_breaker参数的dis_max查询效果相同)，取1则会将所有匹配的子句一视同仁。它的确切值需要根据你的数据和查询进行调整，但是一个合理的值会靠近0，(比如，0.1 -0.4)，来确保不会压倒dis_max查询具有的最佳匹配性质。 tips可以通过/data/entity/_validate/query来查看查询语句是否正确 1234567891011121314151617181920212223242526272829303132&#123; &quot;query&quot;: &#123; &quot;bool&quot;:&#123; &quot;must&quot; : [ &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;business opportunity&quot; &#125;&#125; ], &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;abs&quot;: &quot;inbox&quot; &#125;&#125;&#125; &#125;&#125;----------------------------------&#123; &quot;valid&quot;: true, &quot;_shards&quot;: &#123; &quot;total&quot;: 1, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;&#125;===================================&#123; &quot;query&quot;: &#123; &quot;name&quot; : &#123; &quot;match&quot; : &quot;张三&quot; &#125; &#125;&#125;-----------------------------------&#123; &quot;valid&quot;: false&#125; 如果想知道错误信息 /data/entity/_validate/query?explain 123456789101112&#123; &quot;query&quot;: &#123; &quot;name&quot; : &#123; &quot;match&quot; : &quot;张三&quot; &#125; &#125;&#125;-----------------------------------&#123; &quot;valid&quot;: false, &quot;error&quot;: &quot;org.elasticsearch.common.ParsingException: no [query] registered for [name]&quot;&#125; 5、排序 可以使用字符串参数直接排序 1GET /_search?sort=date:desc&amp;sort=_score&amp;q=search 也可以使用body进行排序 1234567891011121314151617181920212223242526272829303132333435363738&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123;&quot;name&quot;:&quot;张三&quot;&#125; &#125;, &quot;sort&quot;: &#123; &quot;type&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125;&#125;, &quot;from&quot;: 0, &quot;size&quot;: 24&#125;-------------------------------------&#123; &quot;took&quot;: 5, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 15, &quot;max_score&quot;: null, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;data&quot;, &quot;_type&quot;: &quot;entity&quot;, &quot;_id&quot;: &quot;5c89fa7c11b069001757a00f&quot;, &quot;_score&quot;: null, &lt;1&gt; &quot;_source&quot;: &#123; &quot;name&quot;: &quot;张三&quot; &#125;, &quot;sort&quot;: [ 16.0 ] &#125;, ...... &lt;1&gt; _score 字段没有经过计算，因为没有用作排序 &lt;2&gt; 如果是时间字段会被转为毫秒当作排序依据。_score 和 max_score 字段都为 null。计算 _score 是比较消耗性能的, 而且通常主要用作排序 -- 我们不是用相关性进行排序的时候，就不需要统计其相关性。 如果你想强制计算其相关性，可以设置track_scores为 true。 多级排序 结果集会先用第一排序字段来排序，当用用作第一字段排序的值相同的时候， 然后再用第二字段对第一排序值相同的文档进行排序，以此类推 12345678910111213141516171819202122232425262728&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123;&quot;name&quot;:&quot;张三&quot;&#125; &#125;, &quot;sort&quot;: [&#123; &quot;type&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125;&#125;, &#123;&quot;asset.type&quot;: &#123;&quot;order&quot;: &quot;desc&quot;&#125;&#125; ], &quot;_source&quot;: [ &quot;name&quot; ], &quot;from&quot;: 0, &quot;size&quot;: 24&#125;=============================================&quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;data&quot;, &quot;_type&quot;: &quot;entity&quot;, &quot;_id&quot;: &quot;5c89fa7c11b069001757a00f&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;张三&quot; &#125;, &quot;sort&quot;: [ 16.0, -9223372036854775808 ]&#125;, 多值字段排序 在为一个字段的多个值进行排序的时候， 其实这些值本来是没有固定的排序的— 一个拥有多值的字段就是一个集合， 你准备以哪一个作为排序依据呢？ 对于数字和日期，你可以从多个值中取出一个来进行排序，你可以使用min, max, avg 或 sum这些模式。 比说你可以在 dates 字段中用最早的日期来进行排序： 123456"sort": &#123; "dates": &#123; "order": "asc", "mode": "min" &#125;&#125; 重要 为了提高排序效率，ES会将所有字段的值加载到内存中（所有字段数据加载到内存中并不是匹配到的那部分数据，而是索引下所有文档中的值，包括所有类型） 6、过滤1、term 过滤term主要用于精确匹配哪些值，比如数字，日期，布尔值或 not_analyzed的字符串(未经分析的文本数据类型)： 1234&#123; "term": &#123; "age": 26 &#125;&#125;&#123; "term": &#123; "date": "2014-09-01" &#125;&#125;&#123; "term": &#123; "public": true &#125;&#125;&#123; "term": &#123; "tag": "full_text" &#125;&#125; 2、terms过滤（或者）和term有点类似，但是terms允许指定多个匹配条件，如果某个字段指定了多个值，那么文档需要一起去做匹配 12345&#123; &quot;terms&quot;: &#123; &quot;tag&quot;: [ &quot;search&quot;, &quot;full_text&quot;, &quot;nosql&quot; ] &#125;&#125; 3、range过滤range过滤允许我们按照指定范围查找一批数据： 12345678&#123; "range": &#123; "age": &#123; "gte": 20, "lt": 30 &#125; &#125;&#125; 范围操作符包含： gt : 大于 gte: 大于等于 lt : 小于 lte: 小于等于 4、exists过滤exists 过滤可以用于查找文档中是否包含指定字段或没有某个字段，类似于SQL语句中的IS_NULL条件 不存在可以使用 bool -&gt; must_not -&gt; exists 123456789101112131415&#123; "query": &#123; "exists": &#123;"field":"asset.type"&#125; &#125;&#125;&#123; "query": &#123; "bool":&#123; "must_not":&#123; "exists":&#123;"field":"asset.type" &#125; &#125; &#125; &#125;&#125; 这两个过滤只是针对已经查出一批数据来，但是想区分出某个字段是否存在的时候使用。 5、bool过滤bool 过滤可以用来合并多个过滤条件查询结果的布尔逻辑，它包含一下操作符： must : 多个查询条件的完全匹配,相当于 and。 must_not : 多个查询条件的相反匹配，相当于 not。 should : 至少有一个查询条件匹配, 相当于 or。 这些参数可以分别继承一个过滤条件或者一个过滤条件的数组： 12345678910&#123; "bool": &#123; "must": &#123; "term": &#123; "folder": "inbox" &#125;&#125;, "must_not": &#123; "term": &#123; "tag": "spam" &#125;&#125;, "should": [ &#123; "term": &#123; "starred": true &#125;&#125;, &#123; "term": &#123; "unread": true &#125;&#125; ] &#125;&#125; 6、组合过滤1234567891011&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: [ &#123;&quot;bool&quot;: &#123; &quot;must&quot;: [&#123;&quot;match&quot;: &#123;&quot;name&quot;:&quot;张森&quot;&#125;&#125;, &#123;&quot;match&quot;: &#123;&quot;type&quot;:16&#125;&#125;] &#125;&#125; ] &#125; &#125;&#125; 7、包含，而不是相等理解 term 和 terms 是包含操作，而不是相等操作，这点非常重要。这意味着什么？ 假如你有一个 term 过滤器 { &quot;term&quot; : { &quot;tags&quot; : &quot;search&quot; } }，它将匹配下面两个文档： 12&#123; "tags" : ["search"] &#125;&#123; "tags" : ["search", "open_source"] &#125; &lt;1&gt; 虽然这个文档除了 search 还有其他短语，它还是被返回了 回顾一下 term 过滤器是怎么工作的：它检查倒排索引中所有具有短语的文档，然后组成一个字节集。在我们简单的示例中，我们有下面的倒排索引： Token DocIDs open_source 2 search 1,2 当执行 term 过滤器来查询 search 时，它直接在倒排索引中匹配值并找出相关的 ID。如你所见，文档 1 和文档 2 都包含 search，所以他们都作为结果集返回。 提示： 倒排索引的特性让完全匹配一个字段变得非常困难。你将如何确定一个文档只能包含你请求的短语？你将在索引中找出这个短语，解出所有相关文档 ID，然后扫描 索引中每一行来确定文档是否包含其他值。 由此可见，这将变得非常低效和开销巨大。因此，term 和 terms 是 必须包含 操作，而不是 必须相等。 完全匹配 假如你真的需要完全匹配这种行为，最好是通过添加另一个字段来实现。在这个字段中，你索引原字段包含值的个数。引用上面的两个文档，我们现在包含一个字段来记录标签的个数： 12&#123; "tags" : ["search"], "tag_count" : 1 &#125;&#123; "tags" : ["search", "open_source"], "tag_count" : 2 &#125; 一旦你索引了标签个数，你可以构造一个 bool 过滤器来限制短语个数： 123456789101112GET /my_index/my_type/_search&#123; "query": &#123; "bool" : &#123; "must" : [ &#123; "term" : &#123; "tags" : "search" &#125; &#125;, &#123; "term" : &#123; "tag_count" : 1 &#125; &#125; ] &#125; &#125;&#125; 找出所有包含 search 短语的文档 但是确保文档只有一个标签 这将匹配只有一个 search 标签的文档，而不是匹配所有包含了 search 标签的文档。 7、查询过滤合并123456"bool":&#123; "must" : [ &#123; "match": &#123; "name": "business opportunity" &#125;&#125; ], "filter": &#123; "term": &#123; "abs": "inbox" &#125;&#125;&#125; 8、aggtegations（聚合分析）聚合分析是数据库中重要的功能特性，完成对一个查询的数据集中数据的聚合计算，如：找出某字段（或计算表达式的结果）的最大值、最小值，计算和、平均值等。ES作为搜索引擎兼数据库，同样提供了强大的聚合分析能力。 对一个数据集求最大、最小、和、平均值等指标的聚合，在ES中称为指标聚合 metric 而关系型数据库中除了有聚合函数外，还可以对查询出的数据进行分组group by，再在组上进行指标聚合。在 ES 中group by 称为分桶，桶聚合 bucketing ES中还提供了矩阵聚合（matrix）、管道聚合（pipleline），但还在完善中。 12345678910"aggregations" : &#123; "&lt;aggregation_name&gt;" : &#123; "&lt;aggregation_type&gt;" : &#123; &lt;aggregation_body&gt; &#125; [,"meta" : &#123; [&lt;meta_data_body&gt;] &#125; ]? [,"aggregations" : &#123; [&lt;sub_aggregation&gt;]+ &#125; ]? &#125; [,"&lt;aggregation_name_2&gt;" : &#123; ... &#125; ]*&#125; aggregations 也可简写为 aggs 1、max、min、sum、avg 查询type的最大值 12345678910111213141516171819202122232425262728293031&#123; "size": 0, "aggs": &#123; "masssbalance": &#123; // 这个字段名为自定义字段名，会把最中的结果以对象的形式放在里面 "max": &#123; "field": "type" &#125; &#125; &#125;&#125;// ===========================&#123; "took": 452, "timed_out": false, "_shards": &#123; "total": 5, "successful": 5, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 1352448, "max_score": 0.0, "hits": [] &#125;, "aggregations": &#123; "masssbalance": &#123; "value": 256.0 &#125; &#125;&#125; 2、termsterms根据字段值项分组聚合.field按什么字段分组,size指定返回多少个分组,shard_size指定每个分片上返回多少个分组,order排序方式.可以指定include和exclude正则筛选表达式的值,指定missing设置缺省值 1234567891011&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;terms&quot;:&#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;type&quot;, &quot;size&quot;: 10 &#125; &#125; &#125;&#125; 3、去重cardinality12345678910&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;count_type&quot;: &#123; &quot;cardinality&quot;: &#123; &quot;field&quot;: &quot;type&quot; &#125; &#125; &#125;&#125; 4、percentiles百分比12345678910111213141516171819202122232425262728293031323334353637383940414243percentiles对指定字段（脚本）的值按从小到大累计每个值对应的文档数的占比（占所有命中文档数的百分比），返回指定占比比例对应的值。默认返回[ 1, 5, 25, 50, 75, 95, 99 ]分位上的值&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;age_percents&quot;:&#123; &quot;percentiles&quot;: &#123; &quot;field&quot;: &quot;age&quot;, &quot;percents&quot;: [ 1, 5, 25, 50, 75, 95, 99 ] &#125; &#125; &#125;&#125;&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;states&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;gender&quot; &#125;, &quot;aggs&quot;: &#123; &quot;banlances&quot;: &#123; &quot;percentile_ranks&quot;: &#123; &quot;field&quot;: &quot;balance&quot;, &quot;values&quot;: [ 20000, 40000 ] &#125; &#125; &#125; &#125; &#125; 5、percentiles rank统计小于等于指定值得文档比 1234567891011121314&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;tests&quot;: &#123; &quot;percentile_ranks&quot;: &#123; &quot;field&quot;: &quot;age&quot;, &quot;values&quot;: [ 10, 15 ] &#125; &#125; &#125;&#125; 6、filter聚合filter对满足过滤查询的文档进行聚合计算,在查询命中的文档中选取过滤条件的文档进行聚合,先过滤在聚合 1234567891011121314151617&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;agg_filter&quot;:&#123; &quot;filter&quot;: &#123; &quot;match&quot;:&#123;&quot;gender&quot;:&quot;F&quot;&#125; &#125;, &quot;aggs&quot;: &#123; &quot;avgs&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;age&quot; &#125; &#125; &#125; &#125; &#125;&#125; 7、filtters聚合多个过滤组聚合计算 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&#123; "size": 0, "aggs": &#123; "message": &#123; "filters": &#123; "filters": &#123; "asset": &#123; "exists": &#123; "field": "asset.type" &#125; &#125;, "Patent":&#123; "term": &#123; "asset.type": 1 &#125; &#125;, "product":&#123; "term": &#123; "asset.type": 0 &#125; &#125; &#125; &#125; &#125; &#125;&#125;// =================================&#123; "size": 0, "aggs": &#123; "message": &#123; "filters": &#123; "filters": &#123; "asset": &#123; "exists": &#123; "field": "asset.type" &#125; &#125;, "Patent":&#123; "term": &#123; "asset.type": 1 &#125; &#125;, "product":&#123; "term": &#123; "asset.type": 0 &#125; &#125; &#125; &#125; &#125; &#125;&#125; 8、range聚合12345678910111213141516171819202122232425&#123; "aggs": &#123; "agg_range": &#123; "range": &#123; "field": "cost", "ranges": [ &#123; "from": 50, "to": 70 &#125;, &#123; "from": 100 &#125; ] &#125;, "aggs": &#123; "bmax": &#123; "max": &#123; "field": "cost" &#125; &#125; &#125; &#125; &#125;&#125; 9、date_range聚合12345678910111213141516&#123; "aggs": &#123; "date_aggrs": &#123; "date_range": &#123; "field": "accepted_time", "format": "MM-yyy", "ranges": [ &#123; "from": "now-10d/d", "to": "now" &#125; ] &#125; &#125; &#125;&#125; 10、date_histogram时间直方图聚合,就是按天、月、年等进行聚合统计。可按 year (1y), quarter (1q), month (1M), week (1w), day (1d), hour (1h), minute (1m), second (1s) 间隔聚合或指定的时间间隔聚合 123456789101112131415&#123; "aggs": &#123; "sales_over_time": &#123; "date_histogram": &#123; "field": "accepted_time", "interval": "quarter", "min_doc_count" : 0, //可以返回没有数据的月份 "extended_bounds" : &#123; //强制返回数据的范围 "min" : "2014-01-01", "max" : "2014-12-31" &#125; &#125; &#125; &#125;&#125; 11、missing聚合123456789&#123; "aggs": &#123; "account_missing": &#123; "missing": &#123; "field": "__type" &#125; &#125; &#125;&#125; 3、映射与分析映射(mapping)机制用于进行字段类型确认，将每个字段匹配为一种确定的数据类型(string, number, booleans, date等)。 分析(analysis)机制用于进行全文文本(Full Text)的分词，以建立供搜索用的反向索引。 在索引中处理数据时，我们注意到一些奇怪的事。有些东西似乎被破坏了： 在索引中有12个tweets，只有一个包含日期2014-09-15，但是我们看看下面查询中的total hits。 1234GET /_search?q=2014 # 12 个结果GET /_search?q=2014-09-15 # 还是 12 个结果 !GET /_search?q=date:2014-09-15 # 1 一个结果GET /_search?q=date:2014 # 0 个结果 ! 可以通过GET /data/_mapping/entity查看es是如何解读文档 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566&#123; "data": &#123; "mappings": &#123; "entity": &#123; "properties": &#123; "_deleted": &#123; "type": "boolean" &#125;, "abs": &#123; "type": "text", "fields": &#123; "keyword": &#123; "type": "keyword", "ignore_above": 256 &#125; &#125; &#125;, "asset": &#123; "properties": &#123; "type": &#123; "type": "long" &#125; &#125; &#125;, "intro": &#123; "type": "text", "fields": &#123; "keyword": &#123; "type": "keyword", "ignore_above": 256 &#125; &#125; &#125;, "name": &#123; "type": "text", "fields": &#123; "keyword": &#123; "type": "keyword", "ignore_above": 256 &#125; &#125; &#125;, "nick": &#123; "type": "text", "fields": &#123; "keyword": &#123; "type": "keyword", "ignore_above": 256 &#125; &#125; &#125;, "portfolio": &#123; "properties": &#123; "type": &#123; "type": "long" &#125; &#125; &#125;, "type": &#123; "type": "float" &#125; &#125; &#125; &#125; &#125;&#125; 其中，每一种的类型的索引方式是不同的，因此导致查询结果的不同 搜索中，可能需要确切值(exact values) 及 全文文本 确切值：准确的值，要么匹配，要么不匹配 全文文本 分析和分析器 字符过滤器 在标记化前处理字符串，字符过滤器能过去除HTML标记，或者转化&amp;为and 分词器 将字符根据空格或逗号等，将单词分开 标记过滤 可以将单词统一为小写，取掉a, and, the等词（或者增加词） es 支持的字段类型 类型 表示的数据类型 String string Whole number byte, short, integer, long Floating point float, double Boolean boolean Date date 4、相关度查询中，可以在query外加上 &quot;min_score&quot; : -1 来控制返回的最小score ElasticSearch的相似度算法被定义为 TF/IDF，即检索词频率/反向文档频率，包括一下内容： 检索词频率: 检索词在该字段出现的频率？出现频率越高，相关性也越高。 字段中出现过5次要比只出现过1次的相关性高。 反向文档频率: 每个检索词在索引中出现的频率？频率越高，相关性越低。 检索词出现在多数文档中会比出现在少数文档中的权重更低， 即检验一个检索词在文档中的普遍重要性。 字段长度准则: 字段的长度是多少？长度越长，相关性越低。 检索词出现在一个短的 title 要比同样的词出现在一个长的 content 字段高。 单个查询可以使用TF/IDF评分标准或其他方式，比如短语查询中检索词的距离或模糊查询里的检索词相似度。 相关性并不只是全文本检索的专利。也适用于yes|no的子句，匹配的子句越多，相关性评分越高。 如果多条查询子句被合并为一条复合查询语句，比如 bool 查询，则每个查询子句计算得出的评分会被合并到总的相关性评分中。 理解评分标准当调试一条复杂的查询语句时，想要理解相关性评分 _score 是比较困难的。ElasticSearch 在 每个查询语句中都有一个explain参数，将 explain 设为 true 就可以得到更详细的信息。 1234GET /_search?explain &lt;1&gt;&#123; "query" : &#123; "match" : &#123; "tweet" : "honeymoon" &#125;&#125;&#125; explain 参数可以让返回结果添加一个 _score 评分的得来依据。 增加一个 explain 参数会为每个匹配到的文档产生一大堆额外内容，但是花时间去理解它是很有意义的。 如果现在看不明白也没关系 — 等你需要的时候再来回顾这一节就行。下面我们来一点点的了解这块知识点。 首先，我们看一下普通查询返回的元数据： 1234567&#123; "_index" : "us", "_type" : "tweet", "_id" : "12", "_score" : 0.076713204, "_source" : &#123; ... trimmed ... &#125;,&#125; 这里加入了该文档来自于哪个节点哪个分片上的信息，这对我们是比较有帮助的，因为词频率和 文档频率是在每个分片中计算出来的，而不是每个索引中： 12"_shard" : 1,"_node" : "mzIVYCsqSWCG_M_ZffSs9Q", 然后返回值中的 _explanation 会包含在每一个入口，告诉你采用了哪种计算方式，并让你知道计算的结果以及其他详情： 12345678910111213141516171819202122232425262728293031"_explanation": &#123; &lt;1&gt; "description": "weight(tweet:honeymoon in 0) [PerFieldSimilarity], result of:", "value": 0.076713204, "details": [ &#123; "description": "fieldWeight in 0, product of:", "value": 0.076713204, "details": [ &#123; &lt;2&gt; "description": "tf(freq=1.0), with freq of:", "value": 1, "details": [ &#123; "description": "termFreq=1.0", "value": 1 &#125; ] &#125;, &#123; &lt;3&gt; "description": "idf(docFreq=1, maxDocs=1)", "value": 0.30685282 &#125;, &#123; &lt;4&gt; "description": "fieldNorm(doc=0)", "value": 0.25, &#125; ] &#125; ]&#125; honeymoon 相关性评分计算的总结 检索词频率 反向文档频率 字段长度准则 重要： 输出 explain 结果代价是十分昂贵的，它只能用作调试工具 —千万不要用于生产环境。 第一部分是关于计算的总结。告诉了我们 &quot;honeymoon&quot; 在 tweet字段中的检索词频率/反向文档频率或 TF/IDF， （这里的文档 0 是一个内部的ID，跟我们没有关系，可以忽略。） 然后解释了计算的权重是如何计算出来的： 检索词频率: 1检索词 `honeymoon` 在 `tweet` 字段中的出现次数。 反向文档频率: 1检索词 `honeymoon` 在 `tweet` 字段在当前文档出现次数与索引中其他文档的出现总数的比率。 字段长度准则: 1文档中 `tweet` 字段内容的长度 -- 内容越长，值越小。 复杂的查询语句解释也非常复杂，但是包含的内容与上面例子大致相同。 通过这段描述我们可以了解搜索结果是如何产生的。 提示： JSON形式的explain描述是难以阅读的 但是转成 YAML 会好很多，只需要在参数中加上 format=yaml 文档是如何被匹配到的当explain选项加到某一文档上时，它会告诉你为何这个文档会被匹配，以及一个文档为何没有被匹配。 请求路径为 /index/type/id/_explain, 如下所示： 123456789GET /us/tweet/12/_explain&#123; "query" : &#123; "filtered" : &#123; "filter" : &#123; "term" : &#123; "user_id" : 2 &#125;&#125;, "query" : &#123; "match" : &#123; "tweet" : "honeymoon" &#125;&#125; &#125; &#125;&#125; 除了上面我们看到的完整描述外，我们还可以看到这样的描述： 1"failure to match filter: cache(user_id:[2 TO 2])" 也就是说我们的 user_id 过滤子句使该文档不能匹配到。 附：关键词 query 12345matchmatch_allmulti_matchmatch_phrase [field] bool 1234567891011must matchmust_notshould match bool term ...filter term [field]相关文档详情见官方文档 12345678query -- value 支持正则type 类型fields 字段tie_breaker 最佳匹配的调优minimum_should_match 最小匹配度boost 权重operator or 或者 and 默认为or aggregate(agg) 1234567891011&quot;aggregations&quot; : &#123; &quot;&lt;aggregation_name&gt;&quot; : &#123; &quot;&lt;aggregation_type&gt;&quot; : &#123; &lt;aggregation_body&gt; &#125; [,&quot;meta&quot; : &#123; [&lt;meta_data_body&gt;] &#125; ]? [,&quot;aggregations&quot; : &#123; [&lt;sub_aggregation&gt;]+ &#125; ]? &#125; [,&quot;&lt;aggregation_name_2&gt;&quot; : &#123; ... &#125; ]*&#125; size from _source]]></content>
      <categories>
        <category>ES</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>ES</tag>
        <tag>查询</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ES的搭建]]></title>
    <url>%2F2019%2F09%2F15%2Fes%2FES%E7%9A%84%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[1、搭建ES采用docker-compose的方式搭建123456789101112131415161718192021# elasticsearch will server as the index search db# kibana will server as the web ui of elasticsearch# the version of the elasticsearch and kibana should be equalelasticsearch: image: elasticsearch:5.6 mem_limit: 1024m environment: - TZ=Asia/Shanghai ports: - &quot;9200:9200&quot; expose: - &quot;9200&quot; - &quot;9300&quot;kibana: image: kibana:5.6 ports: - &quot;5601:5601&quot; links: - elasticsearch 2、设置连接这里是连接的mongodb数据库，要求mongodb数据库必须是集群，采用mongo-connector的方式连接 有关oplog全量导入的官方解释，大概就是说，当oplog.timestamp文件不存在的时候，将会采用全量导入。 这里有一个注意点，mongo-connector是通过oplog的方式进行数据同步，所以，所要链接的数据库必须是集群。 创建mongo-connector的Dockerfile 123456789FROM ubuntu:18.04ENV LANG C.UTF-8RUN apt updateRUN apt install vim -yRUN apt install python3 -y \ &amp;&amp; apt install python3-pip -y \ &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple &apos;mongo-connector[elastic5]&apos; &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple &apos;elastic2-doc-manager&apos;CMD mongo-connector -c /root/data/config 创建镜像 docker build -t mongo-connector . 配置文件 1234567891011121314151617181920212223242526272829303132333435363738&#123; &quot;mainAddress&quot;: &quot;&quot;, &quot;oplogFile&quot;: &quot;/root/data/log/oplog.timestamp&quot;, &quot;continueOnError&quot;: true, &quot;fields&quot;: [ &quot;name&quot;, &quot;abs&quot;, &quot;intro&quot;, &quot;nick&quot;, &quot;type&quot;, &quot;asset.type&quot;, &quot;_deleted&quot;, &quot;portfolio.type&quot; ], &quot;namespaces&quot;: &#123; &quot;data.entity&quot;: true &#125;, &quot;logging&quot;: &#123; &quot;type&quot;: &quot;file&quot;, &quot;filename&quot;: &quot;/root/data/log/mongo-connector.log&quot; &#125;, &quot;docManagers&quot;: [ &#123; &quot;docManager&quot;: &quot;elastic2_doc_manager&quot;, &quot;targetURL&quot;: &quot;esurl&quot;, &quot;uniqueKey&quot;: &quot;_id&quot;, &quot;autoCommitInterval&quot;: 0 &#125; ]&#125;- mainAddress 为mongo集群的链接- oplogFile 为mongo-connector每次执行的数据操作的时间戳- continueOnError 出错后是否继续- fields 同步的字段- namespaces 同步的数据库和表- logging 以mongo-connector 的操作日志- docManagers 数据插入的格式，包括唯一键的设置，数据同步的时间（单位是s），默认是只同步一次，0为实时同步（实时指的是mongo-connector检测到所连接的数据库oplog有变化，由于连接的是数据库的副本集，所以这里的变化指的是副本集的oplog的变化） 配置信息 样例 运行容器 docker run --restart=always -d -v /workspace/ES:/root/data mongo-connector:latest 附 可以把mongo-connector创建镜像之后，加入docker-compose中，使用docker-compose直接创建elasticsearch、kibana、mongo-connector容器 12345678910111213141516171819202122232425262728293031# elasticsearch will server as the index search db# kibana will server as the web ui of elasticsearchelasticsearch: image: elasticsearch:5.6 restart: always environment: - TZ=Asia/Shanghai ports: - &quot;9200:9200&quot; volumes: - (本地文件存储路径，es数据库存放路径):/usr/share/elasticsearch/data expose: - &quot;9200&quot;kibana: image: kibana:5.6 restart: always ports: - &quot;5601:5601&quot; links: - elasticsearchmongo-connector: image: mongo-connector-alpha restart: always links: - elasticsearch volumes: - (本地config.json的路径):/root/data]]></content>
      <categories>
        <category>ES</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>ES</tag>
        <tag>查询</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nodejs执行shell脚本，实时传输]]></title>
    <url>%2F2019%2F07%2F02%2Fjavascript%2Fnodejs%2F06%E3%80%81node%E6%89%A7%E8%A1%8C%E8%84%9A%E6%9C%AC%E5%B9%B6%E5%AE%9E%E6%97%B6%E8%BE%93%E5%87%BA%2F</url>
    <content type="text"><![CDATA[接到需求需要一个服务来执行shell脚本，要求可以实时打印shell脚本执行的过程，并看到脚本执行的结果。明确任务目标： 这是一个web服务，需要执行shell脚本 当一个脚本执行的时候，再次发送请求需要等待当前脚本执行完毕，再自动执行这次请求 使用长连接而不是socket 添加脚本不需要重启服务器 这里采用的是express框架 开始首先搭好express基本框架新建app.js文件, npm install express 1234567891011121314const express = require('express');const app = express();app.get('/:id', (req, res) =&gt; &#123; const &#123; id &#125; = req.params; if (id === 'favicon.ico') &#123; res.sendStatus(200); return; &#125; // 执行脚本&#125;);app.set('port', 3018);app.listen(app.get('port'), () =&gt; console.log(`server listening at $&#123;app.get('port')&#125;`)); 新建文件新建config.json用于配置id和脚本名的对应关系，新建scripts目录用于存放脚本。 这里定义一个函数execute参数为id和response对象，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243const pidDict = &#123;&#125;;async function execute(id, res) &#123; delete require.cache[require.resolve('./config.json')]; const config = require('./config.json'); const filePath = config[id]; if (!filePath) &#123; res.sendStatus(404); return; &#125; console.log(`The script:$&#123;filePath&#125; with $&#123;id&#125; begin execute`); const readable = new Readable(); readable._read = () =&gt; &#123;&#125;; readable.pipe(res); while (pidDict[id]) &#123; readable.push('\nWaiting for another script request.'); await wait(5000); &#125; const handle = spawn('sh', [`./scripts/$&#123;filePath&#125;`]); pidDict[id] = handle.pid; handle.stdout.on('data', (data) =&gt; &#123; readable.push(`\n$&#123;data&#125;`); getLogger(filePath).log(`\n$&#123;data&#125;`); &#125;); handle.stderr.on('data', (data) =&gt; &#123; getLogger(filePath).warn(`\n$&#123;data&#125;`); readable.push(`\n$&#123;data&#125;`); &#125;); handle.on('error', (code) =&gt; &#123; getLogger(filePath).error(`child process error with information: \n$&#123;code&#125;`); readable.push(`child process error with information: \n$&#123;code&#125;`); delete pidDict[id]; readable.push(null); &#125;); handle.on('close', (code) =&gt; &#123; getLogger(filePath).log(`child process close with code $&#123;code&#125;`); delete pidDict[id]; readable.push(null); &#125;);&#125; 解释： 首先要加载config.json，需要注意的是，因为是需要动态引入，所以这里不能直接使用require(&#39;config.json&#39;)，在这之前，需要先删除引入的缓存：delete require.cache[require.resolve(&#39;./config.json&#39;)]; 获取文件路径 const filePath = config[id]; 新建读写流，可以直接发送到前端。 再执行脚本前，需要判断当前有无脚本执行，这里在外部定义了一个pidDict，文件对应的id直接指向文件执行的handle的pid 紧接着就是输入输出流了 handle.stdout是标准输出 handle.stderr是错误输出，这里指的是输出的警告 handle的error事件指的是脚本执行中遇到的错误，也就是脚本执行不成功报的错误信息 这里定义了两个外部函数，一个是自定义的日志打印，另一个是遇到有脚本执行时的等待 新建utility.js 12345678910111213141516171819202122232425262728293031323334353637383940const fs = require('fs');/** * time wait * * @param time &#123;number&#125; time(ms) to wait *//* eslint-disable compat/compat */const wait = async (time = 1000) =&gt; &#123; return new Promise((resolve) =&gt; &#123; setTimeout(resolve, time); &#125;);&#125;;/** * set log * * getLogger(path).level * level: * log * trace * debug * info * warn * error * @param path */function getLogger(path) &#123; return require('tracer').console(&#123; transport: (data) =&gt; &#123; console.log(data.output); fs.appendFile(`./logs/$&#123;path&#125;.log`, `$&#123;data.rawoutput&#125; \n`, () =&gt; &#123;&#125;); &#125;, &#125;);&#125;module.exports = &#123; wait, getLogger,&#125;; 新建脚本现在，新建scripts/hello-world.sh 12345#!/bin/sh echo 'hello...'sleep 5echo 'world!' config.json中注册该脚本 123&#123; "hello-world": "hello-world.sh"&#125; 执行node app.js，通过curl http://localhost:3018/hello-world即可观察到运行结果。 附这里放上app.js的完整代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071const express = require('express');const &#123; spawn &#125; = require('child_process');const &#123; Readable &#125; = require('stream');const &#123; wait, getLogger &#125; = require('./utility');const app = express();app.get('/:id', (req, res) =&gt; &#123; // 执行脚本 const &#123; id &#125; = req.params; if (id === 'favicon.ico') &#123; res.sendStatus(200); return; &#125; execute(id, res).then();&#125;);const pidDict = &#123;&#125;;/** * 执行sh脚本 * * @param id 脚本id * @param res response object *//* eslint-disable no-underscore-dangle, no-await-in-loop */async function execute(id, res) &#123; delete require.cache[require.resolve('./config.json')]; const config = require('./config.json'); const filePath = config[id]; if (!filePath) &#123; res.sendStatus(404); return; &#125; console.log(`The script:$&#123;filePath&#125; with $&#123;id&#125; begin execute`); const readable = new Readable(); readable._read = () =&gt; &#123;&#125;; readable.pipe(res); while (pidDict[id]) &#123; readable.push('\nWaiting for another script request.'); await wait(5000); &#125; const handle = spawn('sh', [`./scripts/$&#123;filePath&#125;`]); pidDict[id] = handle.pid; handle.stdout.on('data', (data) =&gt; &#123; readable.push(`\n$&#123;data&#125;`); getLogger(filePath).log(`\n$&#123;data&#125;`); &#125;); handle.stderr.on('data', (data) =&gt; &#123; getLogger(filePath).warn(`\n$&#123;data&#125;`); readable.push(`\n$&#123;data&#125;`); &#125;); handle.on('error', (code) =&gt; &#123; getLogger(filePath).error(`child process error with information: \n$&#123;code&#125;`); readable.push(`child process error with information: \n$&#123;code&#125;`); delete pidDict[id]; readable.push(null); &#125;); handle.on('close', (code) =&gt; &#123; getLogger(filePath).log(`child process close with code $&#123;code&#125;`); delete pidDict[id]; readable.push(null); &#125;);&#125;app.set('port', 3018);app.listen(app.get('port'), () =&gt; console.log(`server listening at $&#123;app.get('port')&#125;`));]]></content>
      <categories>
        <category>javascript</category>
        <category>nodejs</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>javascript</tag>
        <tag>nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dockerfile]]></title>
    <url>%2F2019%2F02%2F27%2Fdocker%2F06%E3%80%81dockerFile%2F</url>
    <content type="text"><![CDATA[1、基本结构​ Dockerfile一般由一行行命令语句组成，并且支持以#开头的注释行 ​ Dockerfile主体内容分为四部分：基础镜像信息，维护者信息，镜像操作指令，容器启动时执行指令1234567891011121314151617181920# escape=\ (backslash)# This dockerfile uses the ubuntu:xeniel image# VERSION 2 - EDITION 1# Author: docker_user# Command format: Ins七ruction [arguments / command]# Base image to use, this must be set as the first lineFROM ubuntu:xeniel＃ Maintainer: docker_user &lt;docker_user at email.com&gt; （ @docker_user ）LABEL maintainer docker user&lt;docker user@email.com&gt;# Commands to upda七e the imageRUN echo &quot;deb http://archive.ubuntu.com/ubuntu/ xeniel main universe&quot;apt/sources.list&gt;&gt; /etc/apt/sources.listRUN apt-getupdate &amp;&amp; apt-get install -y nginxRUN echo &quot;\ndaemon off;&quot; &gt;&gt; /etc/nginx/nginx.conf# Commands when creating a new containerCMD /usr/sbin/nginx ​ 首行可以通过注释来指定解析器命令，后续通过注释说明镜像的相关信息。主体部分首先使用FROM指令说明所基于的镜像名称，接下来一般是使用LABEL指令说明维护者信息。后面则是镜像操作指令，例如RUN指令将对镜像执行跟随的命令。每运行一条RUN指令，镜像添加新的一层，并提交。最后是CMD指令，来指定运行容器时的操作命令。 例如： 1234567891011121314151617181920212223242526FROM debain:jessieLABEL maintainer docker_user&lt;docker_user@email.com&gt;ENV NGINX_VERSION 1.10.1-1-jessieRUN apt-key adv --keyserver hkp://pgp.mit.edu:80 --recv-keys 573BFD6B3D8FBC641079A6ABABF5BD827BD9BF62 \ &amp;&amp; echo &quot;deb http://ngix.org/packages/debian/ jessie ngix&quot; &gt;&gt; /etc/apt/sources.list \ &amp;&amp; apt-get update \ &amp;&amp; apt-get install --no-install-recommends --no-install-suggest -y \ ca-certificates \ nginx=$&#123;NGINX_VERSION&#125; \ nginx-module-xslt \ nginx-module-geoip \ nginx-module-image-filter \ nginx-module-perl \ nginx-module-njs \ gettext-base \ &amp;&amp; rm -rf /var/lib/apt/lists/*# forward request and error logs to docker log collectorRUN ln -sf /dev/stdout /var/log/nginx/access.log \ &amp;&amp; ln -sf /dev/stderr /var/log/nginx/error.logEXPOSE 80 443CMD [&quot;nginx&quot;,&quot;-g&quot;,&quot;daemon off;&quot;] 2、指令说明1、分类① 配置指令 ARG 定义创建镜像过程中使用的变量 FROM 指定所创建镜像的基础镜像 LABEL 为生成的镜像添加元数据标签信息 EXPOSE 声明镜像内服务监听的端口 ENV 指定环境变量 ENTRYPOINT 指定镜像的默认入口命令 VOLUME 创建一个数据卷挂载点 USER 指定运行容器时的用户名或UID WORKDIR 配置工作目录 ONBUILD 创建子镜像时指定自动执行的操作指令 STOPSIGNAL 指定退出的信号值 HEALTHCHECK 配置所启动容器如何进行健康检查 SHELL 指定默认shell类型 ② 操作指令 RUN 运行指定命令 CMD 启动容器时指定默认执行的命令 ADD 添加内容到镜像 COPY 复制内容到镜像 2、配置指令 ARG 定义创建镜像过程中使用的变量。 格式：ARG &lt;name&gt;[=&lt;default value&gt;] ​ 在执行docker build时，可以通过-build-arg[=]来为变量赋值。当镜像编译成功后，ARG指令的变量将不再存在（ENV指定的变量将在镜像中保留）。 ​ Docker内置了一些镜像创建变量，用户可以直接使用而无须声明，包括（不区分大小写）HTTP_PROXY、HTTPS_PROXY、FTP_PROXY、NO_PROXY。 FROM 指定所创建镜像的基础镜像。 格式：FROM &lt;image&gt; [AS &lt;name&gt;] 或 FROM &lt;image&gt;:&lt;tag&gt; [AS &lt;name&gt;] 或 FROM &lt;image&gt;@&lt;digest&gt; [AS &lt;name&gt;]。 ​ 任何Dockerfile中的第一条指令必须为FROM指令。并且。如果在同一个Dockerfile中创建多个镜像时，可以使用多个FROM指令（每个镜像一次） ​ 为了保证镜像精简，可以选用体积较小的镜像如Alpine或Debian作为基础镜像，例如： 12ARG VERSION=9.3FROM debian:$&#123;VERSION&#125; LABEL LABEL指令可以为生成的镜像添加元数据标签信息。这些信息可以用来辅助过滤出特定镜像。 格式：LABEL &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt;...。 例如： 1234LABEL version=&quot;1.0.0-rc3&quot;LABEL author=&quot;yeasy@github&quot; date=&quot;2020-01-01&quot;LABEL description=&quot;This text illustrates\ that label-value can span multiple lines.&quot; EXPOSE 声明镜像内服务监听的端口。 格式为：EXPOSE &lt;port&gt; [&lt;port&gt;/&lt;protocol&gt;...] 例如： 1EXPOSE 22 80 8443 ​ 注意该指令只是起到声明作用，并不会自动完成端口映射。 ​ 如果要映射端口出来，在启动容器时可以使用-P参数（Docker主机会自动分配一个宿主主机的临时端口）或-p HOST_PORT:CONTAINER_PORT参数（具体指定所映射的本地端口）。 ENV 指定环境变量，在镜像生成过程中会被后续RUN指令使用，在镜像启动的容器中也会存在。、 ENTRYPOINT VOLUME USER WORKDIR ONBUILD STOPSIGNAL HEALTHCHECK SHELL 样例： ubunt设置中国时区 123FROM ubuntuRUN apt update &amp;&amp; apt install -y tzdataENV TZ Asia/Shanghai]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>服务器</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker端口映射与容器互联]]></title>
    <url>%2F2019%2F02%2F25%2Fdocker%2F05%E3%80%81%E7%AB%AF%E5%8F%A3%E6%98%A0%E5%B0%84%E4%B8%8E%E5%AE%B9%E5%99%A8%E4%BA%92%E8%81%94%2F</url>
    <content type="text"><![CDATA[端口映射与容器互联1、端口映射实现容器访问① 从外部访问容器应用​ 在启动容器的时候，如果不指定对应参数，在容器外部是无法通过网络来访问容器内的网络应用和服务的。​ 当容器中运行一些网络应用，要让外部访问这些应用时，可以通过-P或-p参数来指定端口映射。当使用-P(大写)标记时，Docker会随机映射一个端口（端口范围在Linux系统使用的端口之外，一般都过万）到内部容器的开放网络端口： 12345[root@VM_0_9_centos ~]# docker run -d -P training/webapp python app.py5ebb13200ae725746742c7843031fd458360034858992b2d26945baca9221971[root@VM_0_9_centos ~]# docker ps -lCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5ebb13200ae7 training/webapp &quot;python app.py&quot; 5 seconds ago Up 5 seconds 0.0.0.0:32770-&gt;5000/tcp jolly_lumiere ​ 此时，可以通过docker ps看到，本地主机的32770被映射到了容器的5000端口。访问宿主主机的32770端口即可访问容器内web应用提供的界面。 ​ 同样，也可以通过docker logs命令来查看应用的信息： 12[root@VM_0_9_centos ~]# docker logs -f jolly_lumiere * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) ​ -p(小写)则可以指定要映射的端口，并且，在一个指定端口上只可以绑定一个容器。支持的格式有IP：HostPort:ContainerPort | IP::ContainerPort | HostPort:ContainerPort ② 映射所有接口地址​ 使用HostPort:ContainerPort格式本地的5000端口映射到容器的5000端口，可以执行如下命令： 12[root@VM_0_9_centos ~]# docker run -d -p 5000:5000 training/webapp python app.pybf299b6eaf608c01a35584b08d0274507bc3cc650db96b54fb15b0a518d02327 ​ 此时默认会绑定本地所有接口上的所有地址。多次使用-p标记可以绑定多个端口。 12[root@VM_0_9_centos ~]# docker run -d -p 4000:5000 -p 3000:80 training/webapp python app.pyf304026bbf57f603b57a358e1f84c6195881a4dad5b6674d67c96e6af4e89a37 ③ 映射到指定地址的指定端口​ 可以使用IP:HostPort:ContainerPort格式指定映射使用一个特定地址。 ​ 比如localhost地址127.0.0.1 12[root@VM_0_9_centos ~]# docker run -d -p 127.0.0.1:4500:5000 training/webapp python app.py4c080981280ee9711e61786090cd9572ceffcd0e462da24c3140a3fd5198cb4f ④ 映射到指定地址的任意端口​ 使用IP::ContainerPort绑定localhost的任意端口到容器的5000端口，本地主机会自耦东分配一个端口 12[root@VM_0_9_centos ~]# docker run -d -p 127.0.0.1::5000 training/webapp python app.pydaf785e68360a34c7580ee22c7d57c33482cd057b1a601360ae93ab3c8c45265 ​ 还可以使用udp标记来指定udp端口 12[root@VM_0_9_centos ~]# docker run -d -p 127.0.0.1:3000:5000/udp training/webapp python app.pyccbfee50d722ff459eaffe7d0bcd886f32e25505c4132f75a9d80a8492a74c77 ⑤ 查看映射端口配置使用docker port 来查看当前映射的端口配置，也可以查看到绑定的地址 12345[root@VM_0_9_centos ~]# docker port nervous_lamport5000/tcp -&gt; 0.0.0.0:400080/tcp -&gt; 0.0.0.0:3000[root@VM_0_9_centos ~]# docker port nervous_lamport 50000.0.0.0:4000 2、互联机制实现便捷互访容器的互联是一种让多个容器中的应用进行快速交互的方式。它会在源和接收容器之间创建连接关系，接收容器可以通过容器名快速访问到源容器，而不用指定具体的IP地址。 ① 自定义容器命名​ 连接系统依据容器的名称来执行。因此，首先需要自定义一个好记的容器命名。虽然当创建容器的时候，系统默认会分配一个名字，但自定义命名容器有两个好处： 自定义的命名，比较好记，一目了然 当要连接其他容器时候（即便重启），也可以使用容器名而不用改变 使用--name标记可以为容器自定义命名： 12345[root@VM_0_9_centos ~]# docker run -d -P --name weba training/webapp python app.py646d2b7190d1c10c74e898b7ee6cdc758b4cb9384dd7af48912ac8bc8296467a[root@VM_0_9_centos ~]# docker ps -lCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES646d2b7190d1 training/webapp &quot;python app.py&quot; 6 seconds ago Up 5 seconds 0.0.0.0:32771-&gt;5000/tcp weba ​ 也可以使用docker [container] inspect来查看容器的名字 12[root@VM_0_9_centos ~]# docker inspect -f &quot;&#123;&#123; .Name &#125;&#125;&quot; 66a1e7696aca /db3 注： 容器的名称是唯一的。如果已经命名了一个叫web的容器，当你要再次使用web这个名字的时候，需要先docker rm命令删除之前创建的同名容器。 ​ 在执行docker [container] run 的时候如果添加--rm标记，则容器在终止后会立刻删除。注意，-rm和-d参数不能同时使用 1-d: 后台运行容器，并返回容器ID； ② *容器互联​ 使用 --link参数可以让容器之间安全的进行交互。 123456789先新建一个新的数据库容器[root@VM_0_9_centos ~]# docker run -d --name db training/postgresb43fc415add0fa299fa94a54cf9c3e7ce786a6e69d03902da23fe4956935e822删除之前创建的web容器[root@VM_0_9_centos ~]# docker rm -f webweb然后创建一个新的web容器，并将它连接到db容器[root@VM_0_9_centos ~]# docker run -d -P --name web --link db:db training/webapp python app.pyfc65435956f4ac5c24344d6734d79b656f58454ac0506d64042478499b3f8437 ​ 此时，db容器和web容器建立互联关系。 ​ --link参数的格式为--link name:alias，其中name是要链接的容器的名称，alias是别名。 ​ 使用docker ps来查看容器的连接 123456$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES349169744e49 training/postgres:latestsu pos七gres -c &apos;/usr About a minute agoUp Abouta minute 5432/tcp db, web/dbaed84ee2lbde training/webapp:lates七python app.py 16 hours ago Up 2 minutes0.0.0.0:49154-&gt;5000/tcp web ​ 可以看到自定义命名的容器：db和web，db容器的name列有db也有web/db。这表示web容器链接到db容器，web容器将被允许访问db容器的信息。 ​ Docker相当于在两个互联的容器之间创建了一个虚拟通道，而且不用映射他们的端口到宿主主机上。在启动db容器的时候并没有使用-p和-P标记，从而避免了暴露数据库服务端口到外部网络上。 ​ Docker通过两种方式为容器公开连接信息： 更新环境变量； 更新/etc/hosts文件。 使用env命令来查看web容器的环境变量： 1234567891011[root@VM_0_9_centos ~]# docker run --name web2 --link db:db training/webapp envPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binHOSTNAME=b3dadca04854DB_PORT=tcp://172.17.0.2:5432DB_PORT_5432_TCP=tcp://172.17.0.2:5432DB_PORT_5432_TCP_ADDR=172.17.0.2DB_PORT_5432_TCP_PORT=5432DB_PORT_5432_TCP_PROTO=tcpDB_NAME=/web2/dbDB_ENV_PG_VERSION=9.3HOME=/root ​ 其中DB_开头的环境变量是提供web容器连接db容器使用。前缀采用大写的连接别名。 ​ 除了环境变量，Docker还添加host信息到父容器的/etc/hosts的文件。 1234[root@VM_0_9_centos ~]# docker run -t -i --rm --link db:db training/webapp /bin/bashroot@42504ec2700b:/opt/webapp# cat /etc/hosts172.17.0.2 db 34a71574de69172.17.0.4 42504ec2700b ​ 这里有两个host信息，一个是web容器，web容器用自己的id作为默认主机名，另一个是db容器的IP和主机名 ​ 可以在web容器中安装ping来测试跟db容器的连通 ​ 用户可以链接多个子容器到父容器]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>服务器</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker数据管理]]></title>
    <url>%2F2019%2F02%2F18%2Fdocker%2F04%E3%80%81docker%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[Docker数据管理Docker容器中的管理数据主要有两种方式： 数据卷：容器内数据直接映射到本地主机环境。 数据卷容器：使用特定容器维护数据卷。 1、数据卷数据卷是一个可供容器使用的特殊目录，它将主机操作系统目录直接映射进容器，类似于Linux的mount行为。数据卷可以提供很多有用的特性： 数据卷可以在容器之间共享和重用，容器间传递数据将变得高效与方便； 对数据卷内数据的修改会立马生效，无论是容器内操作 还是本地操作； 对数据卷的更新不会影响镜像，解耦开应用和数据； 卷会一直存在，直到没有容器使用，可以安全地卸载它。 ① 创建数据卷12[root@VM_0_9_centos ~]# docker volume create -d local testtest 此时，查看/var/lib/docker/volumes路径下，会发现所创建的数据卷位置： 12[root@VM_0_9_centos ~]# ls -l /var/lib/docker/volumes/drwxr-xr-x 3 root root 4096 Mar 5 10:34 test ② 绑定数据卷除了使用volume子命令来管理数据卷外，还可以在创建容器时将主机本地的任意路径挂载到容器内作为数据卷，这种形式创建的数据卷称为绑定数据卷。 使用docker [container] run时，可以使用-mount选项来使用数据卷。 -mount选项支持三种数据类型的数据卷： volume：普通数据卷，映射到主机/var/lib/docker/volumes路径下； bind：绑定数据卷，映射到主机指定路径下； tmpfs：临时数据卷，只存在于内存中。 下面使用training/webapp镜像创建一个Web容器，并创建一个数据卷挂载到容器的/opt/webapp目录中： 1$ docker run -d -P --name web --mount type=bind,source=/webapp,destination=/opt/webapp traning/webapp python app.py 上述命令等同于使用旧的-v标记可以在容器内创建一个数据卷： 12[root@VM_0_9_centos ~]# docker run -d -P --name web -v /webapp:/opt/webapp training/webapp python app.py691ea9aa7132bc0c7ed1f0e8f88cd075bbd96406c8ab5661d73db2a1a1f50553 用户可以放置一些程序或数据到本地目录中实时进行更新，然后在容器内运行和使用。 本地目录的路径必须是绝对路径，容器内路径可以为相对路径。如果目录不存在，Docker会自动创建。 Docker挂载数据卷的默认权限是读写(rw)，用户也可以通过ro指定为只读： 12[root@VM_0_9_centos ~]# docker run -d -P --name webs -v /webapp:/opt/webapp:ro training/webapp python app.py9c3618466bbfdf3962a007420312bf41a0c5b788d74e8f79e223998212305763 加了:ro之后，容器内对所挂载的数据卷内的数据就无法修改了。 如果直接挂载一个文件到容器，使用文件编辑工具，包括vi或者sed --in-place的时候，可能会造成文件inode的改变。从Docker1.1.0起，这会导致报错误信息。所以推荐的方式是直接挂载文件所在的目录到容器内。 2、数据卷容器如果用户需要再多个容器之间共享一些持续更新的数据，最简单的方式是使用数据卷容器。数据卷容器也是一个容器，但是它的目的是专门提供数据卷给其他容器挂载。 首先，创建一个数据卷容器dbdata，并在其中创建一个数据卷挂载到/dbdata： 1234567891011121314[root@VM_0_9_centos ~]# docker run -it -v /dbdata --name dbdata ubuntuUnable to find image &apos;ubuntu:latest&apos; locallyTrying to pull repository docker.io/library/ubuntu ... latest: Pulling from docker.io/library/ubuntu6cf436f81810: Pull complete 987088a85b96: Pull complete b4624b3efe06: Pull complete d42beb8ded59: Pull complete Digest: sha256:7a47ccc3bbe8a451b500d2b53104868b46d60ee8f5b35a24b41a86077c650210Status: Downloaded newer image for docker.io/ubuntu:latestroot@b376cd800aba:/# ls bin dbdata etc lib media opt root sbin sys usrboot dev home lib64 mnt proc run srv tmp varroot@b376cd800aba:/# 然后，可以在其他容器中使用--volumes-from来挂载dbdata容器中的数据卷。 例如：创建db1和db2两个容器，并从dbdata容器挂载数据卷： 1234567[root@VM_0_9_centos ~]# docker run -it --volumes-from dbdata --name db1 ubunturoot@3c141ef76ccd:/# [root@VM_0_9_centos ~]# docker run -it --volumes-from dbdata --name db2 ubunturoot@7e023043e5c5:/# 此时，容器db1和db2都挂载同一个数据卷到相同的/dbdata目录，三个容器任何一方在该目录下的写入，其他容器都可以看到。 可以多次使用--volumes-from参数来从多个容器挂载多个数据卷，还可以从其他已经挂载了容器卷的容器来挂载数据卷： 1234567891011121314151617[root@VM_0_9_centos ~]# docker run -d --name db3 --volumes-from db1 training/postgresUnable to find image &apos;training/postgres:latest&apos; locallyTrying to pull repository docker.io/training/postgres ... latest: Pulling from docker.io/training/postgresa3ed95caeb02: Pull complete 6e71c809542e: Pull complete 2978d9af87ba: Pull complete e1bca35b062f: Pull complete 500b6decf741: Pull complete 74b14ef2151f: Pull complete 7afd5ed3826e: Pull complete 3c69bb244f5e: Pull complete d86f9ec5aedf: Pull complete 010fabf20157: Pull complete Digest: sha256:a945dc6dcfbc8d009c3d972931608344b76c2870ce796da00a827bd50791907eStatus: Downloaded newer image for docker.io/training/postgres:latest66a1e7696aca5d051186832b1b5e3f081efd4a84f8e5ae2d7228544ca0269101 注意：使用--volumes-from参数所挂载数据卷的容器自身并不需要保持在运行状态。 如果删除了挂载的容器（包括dbdata、db1和db2），数据卷并不会被自动删除。如果要删除一个数据卷，必须在删除最后一个还挂载这它的容器时显式使用docker rm -v命令来指定同时删除关联的容器。 3、利用数据卷容器来迁移数据可以利用数据卷容器对其中的数据卷进行备份、恢复，以实现数据的迁移。 ① 备份使用下面命令来备份dbdata数据卷容器内的数据卷： 123[root@VM_0_9_centos ~]# docker run --volumes-from dbdata -v $(pwd):/backup --name worker ubuntu tar cvf /backup/backup.tar /dbdatatar: Removing leading `/&apos; from member names/dbdata/ 具体分析： ​ 首先利用ubuntu镜像创建了一个容器worker。使用--volumes-from dbdata参数来让worker容器挂载dbdata容器的数据卷(即dbdata数据卷)；使用-v $(pwd):/backup参数来挂载本地的当前目录到worker容器的/backup目录 ​ worker容器启动后，使用tar cvf /backup/backup.tar /dbdata命令将/dbdata下内容备份为容器内的/backup/backup.tar，即宿主主机当前目录下的backup.tar。 ② 恢复如果要恢复数据到一个容器，可以按照下面的操作。 首先创建一个带有数据卷的容器dbdata2，然后创建另一个新的容器，挂载dbdata2的容器，并使用untar解压备份文件到所挂载的容器卷中： 1$ docker run --volumes-from dbdata2 -v $(pwd):/backup busybox tar xvf]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>服务器</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作docker容器]]></title>
    <url>%2F2019%2F02%2F10%2Fdocker%2F03%E3%80%81%E6%93%8D%E4%BD%9Cdocker%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[操作docker容器容器是docker的另一个核心概念，容器是一个镜像的运行实例。所不同的是，镜像是静态的只读文件，而容器带有运行时需要的可写的文件层，同时容器中的应用进程处于运行状态。如果认为虚拟机是模拟运行的一套操作系统（包括内核，应用运行环境和其他系统环境）和跑在上面的应用。那么docker容器就是独立运行的一个（一组）应用，以及他们必须的运行环境。 1、创建容器① 新建容器docker [container] create 1234567$ docker create -it ubuntu:latest89a89ea8db487a4d266bf556ae9c3c06ca3ba49e533b9ab031bcfa70aab44e55sen@TR-PC MINGW64 /d/Program Files/Docker Toolbox$ docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES89a89ea8db48 ubuntu:latest &quot;/bin/bash&quot; 8 seconds ago Created optimistic_varahamihira 使用docker 【container】create 命令新建的容器处于停止状态，可以使用docker【container】start 命令来启动它 create命令与容器运行模式相关的选项 选项 说明 -a , --attach=[] 是否绑定到标准输入、输出和错误 -d, `—detach=true false` 是否在后台运行容器，默认为否 --detach-keys=&quot;&quot; 从attach模式退出的快捷键 --entrypoint=&quot;&quot; 镜像存在入口命令时，覆盖为新的命令 --expose=[] 指定容器会暴露出来的端口或端口范围 --group-add=[] 运行容器的用户组 -i,`—interactive=true false` 保持标准输入打开，默认为false --ipc=&quot;&quot; 容器IPC命名空间，可以其他容器或主机 --isolation=default 容器使用的隔离机制 --log-driver=&quot;json-file&quot; 指定容器的日志驱动类型，可以为：json-file、syslog、journald、gelf、fluentd、awslogs、splunk、etwlogs、gcplogs、none --log-opt=[] 传递给日志驱动的选项 --net=&quot;bridge&quot; 指定容器的网络模式，包括bridge、none、其他容器内网络、host的网络或某个现有网络等 --net-alias=[] 容器在网络中的别名 -p,`—publish-all=true false` 通过NAT机制将容器标记暴露的端口自动映射到本地主机的临时端口 -p,--publish=[] 指定如何映射到本地主机端口，例如-p 11234-12234:1234-2234 --pid=host 容器的PID命名空间 --userns=&quot;&quot; 启用userns-remap时配置用户命名空间的模式 -uts=host 容器的UTS命名空间 -restart=&quot;no&quot; 容器的重启策略，包括no、on-failure[: max-retry]、always、unless-stopped等 `—rm=true false` 容器退出后是否自动删除，不能跟-d同时使用 -t,`—tty=true false` 是否分配一个伪终端，默认为false --tmpfs=[] 挂载临时文件系统到容器 `-v —volume[=[[HOST-DIR:]COMTAONER-DIR[:OPTIONS]]]` 挂载主机上的文件卷到容器内 --volunme-driver=&quot;&quot; 挂载文件卷的驱动类型 --volume-from=[] 从其他容器挂载卷 -w,--workdir=&quot;&quot; 容器内的默认工作目录 create命令与容器环境和配置相关的选项 选项 说明 --add-host=[] 在容器内添加一个主机名到IP地址的映射关系（通过/etc/hosts文件） --device=[] 映射物理机上的设备到容器内 --dns-search=[] DNS搜索域 --dns-opt=[] 自定义的DNS选项 --dns 自定义的DNS服务器 -e、--env=[] 指定容器内的环境变量 --env-file=[] 从文件中读取环境变量到容器内 -h, --hostname=&quot;&quot; 指定容器内主机名 --ip=&quot;&quot; 指定容器内的主机IPv4地址 --ip6=&quot;&quot; 指定容器内的主机IPv6地址 --link=[&lt;name or id&gt;:alias] 链接到其他容器 --link-local-ip=[] 容器的本地链接地址列表 --mac-address=&quot;&quot; 指定容器的Mac地址 --name=&quot;&quot; 指定容器的别名 create命令与容器资源限制和安全保护相关的选项 选项 说明 -blkio-weight=10~1000 容器读写块设备的I/O性能权重，默认为0 --blkio-weight-device=[DEVICE_NAME:WEIGHT] 指定各个块设备I/O性能权重 --cpu-shares=0 允许容器使用CPU资源的相对权重，默认一个容器能用满一个核的CPU --cap-add=[] 增加容器的Linux指定安全能力 --cap-drop=[] 移除容器的Linux指定安全能力 --cgroup-parent=&quot;&quot; 容器cgroups限制的创建路径 --cidfile=&quot;&quot; 指定容器的进程ID号写到文件 --cpu-period=0 限制容器在CFS调度器下的CPU占用时间片 --cpuset-cpus=&quot;&quot; 限制容器能使用哪些CPU核心 --cpuset-mems=&quot;&quot; NUMA架构下使用哪些核心的内存 --cpu-quota=0 限制容器在CFS调度器下的CPU配额 --device-read-bps=[] 挂载设备的读吞率（以bps为单位）限制 --device-write-bps=[] 挂载设备的写吞率（以bps为单位）限制 --device-read-iops=[] 挂载设备的读速率（以每秒i/o次数为单位）限制 --device-write-iops=[] 挂载设备的写速率（以每秒i/o次数为单位）限制 --health-cmd=&quot;&quot; 指定检查容器健康状态的命令 --health-interval=0s 执行健康检查的间隔时间，单位可以为ms、s、m或h --health-retries=int 健康检查失败重试次数，超过则认为不健康 --health-start-period=0s 容器启动后进行健康检查的等待时间，单位可以为ms、s、m或h --health-timeout=0s 健康检查的执行超时，单位可以为ms、s、m或h `—no-healthcheck=true false` 是否禁用健康检查 --init 在容器中执行一个init进程，来负责响应信号和处理僵尸状态子进程 --kernel-memory=&quot;&quot; 限制容器使用内核的内存大小，单位可以是b、k、m或g -m, --memory=&quot;&quot; 限制容器内应用使用的内存，单位可以是b、k、m或g --memory-reservation=&quot;&quot; 当系统中内存过低时，容器会被强制限制内存到给定值，默认情况下等于内存限制值 --memory-swap=&quot;LIMIT&quot; 限制容器使用内存和交换区的总大小 --oom-score-adj=&quot;&quot; 调整容器的内存耗尽参数 --pids-limit=&quot;&quot; 限制容器的pid个数 `—privileged=true false` 是否给容器高权限，这意味着容器内应用将不受权限的限制，一般不推荐 `—read-only=true false` 是否让容器内的文件系统只读 --security-opt=[] 指定一些安全参数，包括权限、安全能力、apparmor等 --stop-signal=SIGTERM 指定停止容器的系统信号 --shm-size=&quot;&quot; /dev/shm的大小 `—sig-proxy=true false` 是否代理收到的信号给应用，默认为true，不能代理SIGCHLD、SIGSTOP和SIGKILL信号 --memory-swappiness=&quot;0~100&quot; 调整容器的内存交换区参数 -u,--user=&quot;&quot; 指定在容器内执行命令的用户信息 --userns=&quot;&quot; 指定用户命名空间 --ulimit=[] 通过ulimit来限制最大文件数、最大进程数等 其他选项还包括： -l, --label=[]：以键值对方式指定容器的标签信息； --label-file=[]：从文件中读取标签信息。 ② 启动容器docker 【container】 start启动一个已经创建的容器(89为id开头两位) docker ps 查看一个运行中的容器 1234567$ docker start 8989sen@TR-PC MINGW64 /d/Program Files/Docker Toolbox$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES89a89ea8db48 ubuntu:latest &quot;/bin/bash&quot; 30 hours ago Up 7 seconds optimistic_varahamihira ③ 新建并启动容器docker [container] run 等价于先执行docker [container] create ， 再执行docker [container] start 例如：下面的命令输出一个“hello world”，之后容器自动停止 12$ docker run ubuntu /bin/echo &quot;hello world&quot;hello world 这和本地直接执行/bin/echo &#39;hello world&#39;相比几乎感觉不出任何区别 docker [container] run 来创建并启动容器时，Docker在后台运行的标准包括： 检查本地是否存在指定的镜像，不存在就从共有仓库下载； 利用镜像创建一个容器，并启动该容器； 分配一个文件系统给容器，并在只读的镜像层外面挂载一层可写层； 从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去； 从网桥的地址池配置一个IP地址给容器； 执行用户指定的应用程序； 执行完毕后容器被自动终止。 下面的命令启动一个bash终端，允许用户进行交互： 12$ docker run -it ubuntu:18.04 /bin/bashroot@....:/# 其中，-t 选项让Docker分配一个伪终端并绑定到容器的标准输出上，-i 则让容器的标准输入保持打开。更多的命令选项可以通过 man docker-run 命令来查看。 对于所创建的bash容器，当用户使用exit命令退出bash进程后，容器也会自动退出。这是因为对于容器来说，当其中的应用退出后，容器的使命完成，也就没有继续运行的必要了。 可以通过docker container wait CONTAINER [CONTAINER…] 子命令来等待容器退出，并打印退出返回结果。 某些时候，执行docker [container] run 时候因为命令无法正常执行，容器会出错直接退出，此时可以查看退出的错误代码。 默认情况下，错误代码包括： 125：Docker deamon 执行出错，例如指定了不支持的Docker命令参数； 126：所指定命令无法执行，例如权限出错； 127：容器内命令无法找到。 命令执行后出错，会默认返回命令的退出错误码。 ④ 守护态运行更多时候，需要让Docker容器在后台以守护态形式运行。此时，可以通过添加-d参数来实现。 容器启动后会返回一个唯一的id，也可以通过docker ps 或 docker container ls 命令来查看容器信息 ⑤ 查看容器输出要获取容器的输出信息，可以通过docker [container] logs 命令。 该命令支持的选项包括： -details：打印详细信息； -f，-follow：持续保持输出； -since string：输出从某个时间开始的日志； -tail string：输出最近的若干日志； -t，-timestamps：显示时间戳信息； -until string：输出某个时间之前的日志。 例如，查看某容器的输出可以使用如下命令： 1$ docker logs ...... 2、停止容器① 暂停容器docker [container] pause CONTAINER [CONTAINER...] 处于paused状态的容器，可以使用docker [container] unpause CONTAINER [CONTAINER…] 命令来恢复到运行状态 ② 终止容器 docker [container] stop来终止一个运行中的容器。 格式为：docker [container] stop [-t|--time[=10]] [CONTAINER...] 这个命令会首先向容器发送SIGTERM信号，等待一段超时时间后（默认为10秒），在发送SIGKILL信号来终止容器： 1docker stop ce5 docker container prune 会自动清除所有处于停止状态的容器 docker [container] kill直接发送SIGKILL信号来强行终止容器 当Docker容器中指定的应用终结时，容器也会自动终止。 当用户通过exit命令或Ctrl+d来退出终端时，所创建的容器立刻终止，处于stopped状态，可以通过docker ps -qa命令看到所有容器的ID。 处于终止状态的容器，可以通过docker [container] start命令来重新启动 docker [container] restart命令 先让一个运行中的容器终止，然后再重新启动 3、进入容器在使用-d参数时，容器启动后会进入后台，用户无法查看容器内的信息，也无法操作。 ① attach命令格式 ​ docker [container] attach [--detach-keys[=[]]] [--no-stdin] [--sig-proxy[=true]] CONTAINER 这个命令支持三个主要选项： --detach-keys[=[]]： 指定退出attach模式的快捷键序列，默认是CTRL-p CTRL-q; --no-stdin=true|false：是否关闭标准输入，默认是保持打开； --sig-proxy=true|false：是否代理收到的系统信号给应用进程，默认为true。 1234567891011121314Administrator@SC-201809020623 MINGW64 /c/Program Files/Docker Toolbox$ docker run -itd ubuntuUnable to find image &apos;ubuntu:latest&apos; locallylatest: Pulling from library/ubuntuDigest: sha256:7a47ccc3bbe8a451b500d2b53104868b46d60ee8f5b35a24b41a86077c650210Status: Downloaded newer image for ubuntu:latestd6d505b79f88e5e1a2d8c4c5a926e0dfcac59aa992896d7cd9f0790fc2bd34a8$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESd6d505b79f88 ubuntu &quot;/bin/bash&quot; 12 seconds ago Up 13 seconds laughing_kare$ docker attach laughing_kareroot@d6d505b79f88:/# 当多个窗口同时attach到同一个容器的时候，所有的窗口都会同步显示；当某个窗口因命令阻塞时，其他窗口也无法执行操作了。 ② exec命令命令格式：docker [container] exec [-d|--detach] [--detach-keys[=[]]] [-i|--interactive] [--privileged] [-t|--tty] [-u|--user[=USER]] CONTAINER COMMAND [ARG...] 比较重要的参数： -d，--detach：在容器中后台执行命令； --detach-keys=&quot;&quot;：指定将容器切回后台的按键； -e，--env=[]：指定环境变量列表； -i，--interactive=true|false：打开标准输入接受用户输入命令，默认为false； --privileged=true|false：是否给指定命令以最高权限，默认为false； -t，--tty=true|false：分配伪终端，默认为false； -u，--user=&quot;&quot;：执行命令的用户名或ID。 1234567Administrator@SC-201809020623 MINGW64 /c/Program Files/Docker Toolbox$ docker start 9545e561180d9545e561180dAdministrator@SC-201809020623 MINGW64 /c/Program Files/Docker Toolbox$ docker exec -it 9545e561180d /bin/bashroot@9545e561180d:/# 4、删除容器docker [container] rm命令来删除处于终止或退出状态的容器。 命令格式为docker [container] rm [-f|--force] [-l|--link] [-v|--volumes] CONTAINER [CONTAINER...] 主要支持选项： -f，--force=false：是否强行终止并删除一个运行中的容器； -l，--link=false：删除容器的连接，但保留容器； -v，--volumes=false：删除容器挂载的数据卷。 123456789101112Administrator@SC-201809020623 MINGW64 /c/Program Files/Docker Toolbox$ docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES9545e561180d ubuntu &quot;/bin/bash&quot; 33 minutes ago Exited (0) 22 minutes ago sad_mestorfAdministrator@SC-201809020623 MINGW64 /c/Program Files/Docker Toolbox$ docker rm 9545e561180d9545e561180dAdministrator@SC-201809020623 MINGW64 /c/Program Files/Docker Toolbox$ docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 默认情况下，docker rm命令只能删除已经处于终止或退出状态的容器，并不能删除还处于运行状态的容器。 如果想直接删除一个运行中的容器，可以添加-f参数，Docker会先发送SIGKILL信号给容器，终止其中的应用，之后强行删除。 1234567891011121314Administrator@SC-201809020623 MINGW64 /c/Program Files/Docker Toolbox$ docker run -d ubuntu:18.04 /bin/sh -c &quot;while true; do echo hello world;sleep 1; done&quot;fe2d369c6ee1205afeb3ae270d39b2b79e89f66127c1db1f17eb96b3ff142ccaAdministrator@SC-201809020623 MINGW64 /c/Program Files/Docker Toolbox$ docker rm fe2d369c6ee1Error response from daemon: You cannot remove a running container fe2d369c6ee1205afeb3ae270d39b2b79e89f66127c1db1f17eb96b3ff142cca. Stop the container before attempting removal or force removeAdministrator@SC-201809020623 MINGW64 /c/Program Files/Docker Toolbox$ docker rm -f fe2d369c6ee1fe2d369c6ee1Administrator@SC-201809020623 MINGW64 /c/Program Files/Docker Toolbox$ docker ps -qa 5、导入和导出容器① 导出容器docker [container] export 命令格式： ​ docker [container] export [-o| --output[=&quot;&quot;]] CONTAINER ​ -o 用来指定导出的tar的文件名，也可以直接通过重定向来实现。 1$ docker export -o test_for_run.tar ce5 ② 导入容器docker [container] import 命令格式： ​ docker import [-c|--change[=[]]] [-m|--message[=MASSAGE]] file|URL|-[REPOSITORY[:TAG]] ​ 可以通过 -c，--change=[] 在导入的同时，执行对容器进项修改的Dockerfile指令 1$ docker import test_for_run.tar -test/ubuntu:v1.0 docker load 载入镜像文件的区别在于容器快照文件将丢弃所有的历史记录和元数据信息（即仅保存容器当时的快照状态），而镜像存储文件将保存完整记录，体积更大。 6、查看容器① 查看容器详情docker containers inspect [OPTIONS] CONTAINER [CONTAINER...] 例如，查看某容器的具体信息，会以json格式返回包括容器ID、创建时间、路径、状态、镜像、配置等在内的信息； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196$ docker container inspect 0435deadfa7a[ &#123; &quot;Id&quot;: &quot;0435deadfa7af4aa0c4aab2f20bba4e29a39526357db7a08b0cbf808f5879fc1&quot;, &quot;Created&quot;: &quot;2019-03-03T13:34:43.584015659Z&quot;, &quot;Path&quot;: &quot;/bin/bash&quot;, &quot;Args&quot;: [], &quot;State&quot;: &#123; &quot;Status&quot;: &quot;created&quot;, &quot;Running&quot;: false, &quot;Paused&quot;: false, &quot;Restarting&quot;: false, &quot;OOMKilled&quot;: false, &quot;Dead&quot;: false, &quot;Pid&quot;: 0, &quot;ExitCode&quot;: 0, &quot;Error&quot;: &quot;&quot;, &quot;StartedAt&quot;: &quot;0001-01-01T00:00:00Z&quot;, &quot;FinishedAt&quot;: &quot;0001-01-01T00:00:00Z&quot; &#125;, &quot;Image&quot;: &quot;sha256:ccc7a11d65b1b5874b65adb4b2387034582d08d65ac1817ebc5fb9be1baa5f88&quot;, &quot;ResolvConfPath&quot;: &quot;&quot;, &quot;HostnamePath&quot;: &quot;&quot;, &quot;HostsPath&quot;: &quot;&quot;, &quot;LogPath&quot;: &quot;&quot;, &quot;Name&quot;: &quot;/compassionate_hertz&quot;, &quot;RestartCount&quot;: 0, &quot;Driver&quot;: &quot;overlay2&quot;, &quot;Platform&quot;: &quot;linux&quot;, &quot;MountLabel&quot;: &quot;&quot;, &quot;ProcessLabel&quot;: &quot;&quot;, &quot;AppArmorProfile&quot;: &quot;&quot;, &quot;ExecIDs&quot;: null, &quot;HostConfig&quot;: &#123; &quot;Binds&quot;: null, &quot;ContainerIDFile&quot;: &quot;&quot;, &quot;LogConfig&quot;: &#123; &quot;Type&quot;: &quot;json-file&quot;, &quot;Config&quot;: &#123;&#125; &#125;, &quot;NetworkMode&quot;: &quot;default&quot;, &quot;PortBindings&quot;: &#123;&#125;, &quot;RestartPolicy&quot;: &#123; &quot;Name&quot;: &quot;no&quot;, &quot;MaximumRetryCount&quot;: 0 &#125;, &quot;AutoRemove&quot;: false, &quot;VolumeDriver&quot;: &quot;&quot;, &quot;VolumesFrom&quot;: null, &quot;CapAdd&quot;: null, &quot;CapDrop&quot;: null, &quot;Dns&quot;: [], &quot;DnsOptions&quot;: [], &quot;DnsSearch&quot;: [], &quot;ExtraHosts&quot;: null, &quot;GroupAdd&quot;: null, &quot;IpcMode&quot;: &quot;shareable&quot;, &quot;Cgroup&quot;: &quot;&quot;, &quot;Links&quot;: null, &quot;OomScoreAdj&quot;: 0, &quot;PidMode&quot;: &quot;&quot;, &quot;Privileged&quot;: false, &quot;PublishAllPorts&quot;: false, &quot;ReadonlyRootfs&quot;: false, &quot;SecurityOpt&quot;: null, &quot;UTSMode&quot;: &quot;&quot;, &quot;UsernsMode&quot;: &quot;&quot;, &quot;ShmSize&quot;: 67108864, &quot;Runtime&quot;: &quot;runc&quot;, &quot;ConsoleSize&quot;: [ 0, 0 ], &quot;Isolation&quot;: &quot;&quot;, &quot;CpuShares&quot;: 0, &quot;Memory&quot;: 0, &quot;NanoCpus&quot;: 0, &quot;CgroupParent&quot;: &quot;&quot;, &quot;BlkioWeight&quot;: 0, &quot;BlkioWeightDevice&quot;: [], &quot;BlkioDeviceReadBps&quot;: null, &quot;BlkioDeviceWriteBps&quot;: null, &quot;BlkioDeviceReadIOps&quot;: null, &quot;BlkioDeviceWriteIOps&quot;: null, &quot;CpuPeriod&quot;: 0, &quot;CpuQuota&quot;: 0, &quot;CpuRealtimePeriod&quot;: 0, &quot;CpuRealtimeRuntime&quot;: 0, &quot;CpusetCpus&quot;: &quot;&quot;, &quot;CpusetMems&quot;: &quot;&quot;, &quot;Devices&quot;: [], &quot;DeviceCgroupRules&quot;: null, &quot;DiskQuota&quot;: 0, &quot;KernelMemory&quot;: 0, &quot;MemoryReservation&quot;: 0, &quot;MemorySwap&quot;: 0, &quot;MemorySwappiness&quot;: null, &quot;OomKillDisable&quot;: false, &quot;PidsLimit&quot;: 0, &quot;Ulimits&quot;: null, &quot;CpuCount&quot;: 0, &quot;CpuPercent&quot;: 0, &quot;IOMaximumIOps&quot;: 0, &quot;IOMaximumBandwidth&quot;: 0, &quot;MaskedPaths&quot;: [ &quot;/proc/asound&quot;, &quot;/proc/acpi&quot;, &quot;/proc/kcore&quot;, &quot;/proc/keys&quot;, &quot;/proc/latency_stats&quot;, &quot;/proc/timer_list&quot;, &quot;/proc/timer_stats&quot;, &quot;/proc/sched_debug&quot;, &quot;/proc/scsi&quot;, &quot;/sys/firmware&quot; ], &quot;ReadonlyPaths&quot;: [ &quot;/proc/bus&quot;, &quot;/proc/fs&quot;, &quot;/proc/irq&quot;, &quot;/proc/sys&quot;, &quot;/proc/sysrq-trigger&quot; ] &#125;, &quot;GraphDriver&quot;: &#123; &quot;Data&quot;: &#123; &quot;LowerDir&quot;: &quot;/mnt/sda1/var/lib/docker/overlay2/b1a8abb976adc0c1cf7b894fbb8f8e610800f665238ee296ce796cafb9fab932-init/diff:/mnt/sda1/var/lib/docker/overlay2/7c902ec8d2795509eccd361ce1a6741a7cf9412f2beabfb7ee98e4f25bb71db4/diff:/mnt/sda1/var/lib/docker/overlay2/87851652aebefd2a1c636d74f0c4fdaaf124b673188606e86e9afab827a6a4eb/diff:/mnt/sda1/var/lib/docker/overlay2/1eca5e2f4e468eb8ae1ce32645d7d29adfe25b4eec966b3fc9ad49b40e220704/diff:/mnt/sda1/var/lib/docker/overlay2/d7ce2d91893b082939bbd040a5e42adf2408e32a3467492361a6c18bc51b29f2/diff:/mnt/sda1/var/lib/docker/overlay2/0dba139a8774ffb1715bc6497a4acd97ae9ae14991f8f8d1f2aebb2c7354ec6e/diff&quot;, &quot;MergedDir&quot;: &quot;/mnt/sda1/var/lib/docker/overlay2/b1a8abb976adc0c1cf7b894fbb8f8e610800f665238ee296ce796cafb9fab932/merged&quot;, &quot;UpperDir&quot;: &quot;/mnt/sda1/var/lib/docker/overlay2/b1a8abb976adc0c1cf7b894fbb8f8e610800f665238ee296ce796cafb9fab932/diff&quot;, &quot;WorkDir&quot;: &quot;/mnt/sda1/var/lib/docker/overlay2/b1a8abb976adc0c1cf7b894fbb8f8e610800f665238ee296ce796cafb9fab932/work&quot; &#125;, &quot;Name&quot;: &quot;overlay2&quot; &#125;, &quot;Mounts&quot;: [], &quot;Config&quot;: &#123; &quot;Hostname&quot;: &quot;0435deadfa7a&quot;, &quot;Domainname&quot;: &quot;&quot;, &quot;User&quot;: &quot;&quot;, &quot;AttachStdin&quot;: false, &quot;AttachStdout&quot;: true, &quot;AttachStderr&quot;: true, &quot;Tty&quot;: false, &quot;OpenStdin&quot;: false, &quot;StdinOnce&quot;: false, &quot;Env&quot;: [ &quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot; ], &quot;Cmd&quot;: [ &quot;/bin/bash&quot; ], &quot;ArgsEscaped&quot;: true, &quot;Image&quot;: &quot;hub.c.163.com/library/ubuntu&quot;, &quot;Volumes&quot;: null, &quot;WorkingDir&quot;: &quot;&quot;, &quot;Entrypoint&quot;: null, &quot;OnBuild&quot;: null, &quot;Labels&quot;: &#123;&#125; &#125;, &quot;NetworkSettings&quot;: &#123; &quot;Bridge&quot;: &quot;&quot;, &quot;SandboxID&quot;: &quot;&quot;, &quot;HairpinMode&quot;: false, &quot;LinkLocalIPv6Address&quot;: &quot;&quot;, &quot;LinkLocalIPv6PrefixLen&quot;: 0, &quot;Ports&quot;: &#123;&#125;, &quot;SandboxKey&quot;: &quot;&quot;, &quot;SecondaryIPAddresses&quot;: null, &quot;SecondaryIPv6Addresses&quot;: null, &quot;EndpointID&quot;: &quot;&quot;, &quot;Gateway&quot;: &quot;&quot;, &quot;GlobalIPv6Address&quot;: &quot;&quot;, &quot;GlobalIPv6PrefixLen&quot;: 0, &quot;IPAddress&quot;: &quot;&quot;, &quot;IPPrefixLen&quot;: 0, &quot;IPv6Gateway&quot;: &quot;&quot;, &quot;MacAddress&quot;: &quot;&quot;, &quot;Networks&quot;: &#123; &quot;bridge&quot;: &#123; &quot;IPAMConfig&quot;: null, &quot;Links&quot;: null, &quot;Aliases&quot;: null, &quot;NetworkID&quot;: &quot;&quot;, &quot;EndpointID&quot;: &quot;&quot;, &quot;Gateway&quot;: &quot;&quot;, &quot;IPAddress&quot;: &quot;&quot;, &quot;IPPrefixLen&quot;: 0, &quot;IPv6Gateway&quot;: &quot;&quot;, &quot;GlobalIPv6Address&quot;: &quot;&quot;, &quot;GlobalIPv6PrefixLen&quot;: 0, &quot;MacAddress&quot;: &quot;&quot;, &quot;DriverOpts&quot;: null &#125; &#125; &#125; &#125;] ② 查看容器内的进程docker [container] top [OPTIONS] [CONTAINER...] ③ 查看统计信息docker [container] stats [options] [CONTAINER...] 会显示CPU、内存、储存、网络等使用情况信息 支持的选项： -a，-all：输出所有容器统计信息，默认仅在运行中； -format string：格式化输出信息； -no-stream：不持续输出，默认会自动更新持续实时结果； -no-trunc：不截断输出信息 7、其他容器命令① 复制文件container cp 命令格式：docker [container] cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|- 支持的选项： -a，-archive：打包模式，复制文件会带有原始的uid/gid信息； -L，-follow-link：跟随软连接。当原始路径为软连接时，默认只复制链接信息，使用该选项将会复制链接目标内容 ② 查看变更container diff 查看容器内文件系统的变更。 命令格式：docker [container] diff CONTAINER ③ 查看端口映射container port 命令格式：docker container port CONTAINER [PRIVATE_PORT[/PROTO]] 12$ docker container port test9000/tcp -&gt; 0.0.0.0:9000 ④ 更新配置container update 命令格式：docker [container] update [OPTIONS] CONTAINER [CONTAINER...] 支持的选项： -blkio-weigth uint16：更新IO限制，10~1000，默认为0，代表无限制； -cpu-period int：限制CPU调度器CFS（Completely Faire Scheduler）使用时间； -cpu-quota int：限制CPU调度器CFS配额，单位微秒，最小1000； -cpu-rt-period int：限制CPU调度器的实时周期，单位微秒； -cpu-rt-runtime int：限制CPU调度器的实时运行时，单位微秒； -c，-cpu-shares int：限制CPU使用份额； -cpus decimal：限制CPU个数； -cpuset-cpus string：允许使用的CPU核，如0-3，0，1； -cpuset-mems string：允许使用的内存块，如0-3，0，1； -kernel-memory bytes：限制使用的内核内存； -m，-memory bytes：限制使用的内存； -memory-reservation bytes：内存软限制； -memory-swap bytes：内存加上缓存区的限制，-1表示对缓存区无限制； -restart string：容器退出后的重启策略； 例如，限制总配额为1秒，容器test所占用时间为10%，代码如下所示： 1234$ docker update --cpu-quota 1000000 testtest$ docker update --cpu-period 100000 testtest]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>服务器</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker常用命令]]></title>
    <url>%2F2019%2F02%2F05%2Fdocker%2F02%E3%80%81docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[一、docker 获取镜像：123docker pull ubuntu:18.04 获取ubuntu18.04系统的基础镜像docker pull ubuntu 获取最新版ubuntudocker pull hub.c.163.com/public/ubuntu:18.04 从网易蜂巢获取ubuntu 18.04系统镜像 pull 支持选项 -a, —all-tags=true | false: 是否获取仓库中所有镜像， 默认为否 —disable-content-trust: 取消镜像的内容校验，默认为真 此外，可以在Docker服务启动配置中增加 --registry-mirror=proxy_URL来指定镜像代理服务地址（如：https://registry.docker-cn.com） 二、查看镜像信息1、images 命令列出镜像docker images 或者 docker image ls 123$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEscrapinghub/splash latest 3926e5aac017 11 months ago 1.22GB 列出信息中，可以看到几个字段信息 REPOSITORY 来自哪个仓库 TAG 镜像的标签信息，latest表示版本信息 标签只是标记，并不能标识镜像内容 用于标记来自同一仓库的不同镜像 IMAGE ID 镜像ID（唯一标识镜像） 如果两个镜像ID相同，说明他们实际上指向了同一个镜像，只是标签名称不同 一般可以使用该ID的前若干个字符组成的可区分串来代替完整的ID CREATED 创建时间，说明镜像的最后更新时间 SIZE 镜像大小 优秀的镜像往往体积都较小 表示的是镜像的逻辑体积大小，实际上由于相同的镜像层本地只会存储一份，物理上占用的存储空间会小于各镜像逻辑体积之和 image子命令主要支持选项： -a, -all=true | false: 列出所有的（包括临时文件）镜像文件，默认为否 --digests=true | false: 列出镜像的数字摘要值，默认为否 -f, --filter=[] : 过滤列出镜像，如dangling=true 只显示没有被使用的镜像， 也可指定带有特定标注的镜像等 --format=&quot;TEMPLATE&quot; : 控制输出格式，如.ID代表ID信息， .Repository代表仓库信息 --no-trunc=true|false : 对输出结果中太长的部分是否进行截断，如ID， 默认是false -q ,--quiet=true|false : 仅输出ID信息，默认为false 其中，还支持对输出结果进行控制的选项， 如 -f, --filter=[], --notrunc=true|false, -q, --quiet=true | false 更多子命令还可以通过 man docker -images 查看 2、使用tag命令添加镜像标签​ 实际上起到了类似链接的作用 12345678$ docker tag ubuntu:latest myubuntu:latestsen@TR-PC MINGW64 /d/Program Files/Docker Toolbox$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEmyubuntu latest 20bb25d32758 3 days ago 87.5MBubuntu latest 20bb25d32758 3 days ago 87.5MBscrapinghub/splash latest 3926e5aac017 11 months ago 1.22GB 3、使用inspect命令查看详细信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596$ docker inspect ubuntu:18.04[ &#123; &quot;Id&quot;: &quot;sha256:20bb25d32758db4f91b18a9581794cfaa6a8c5fbad80093e9a9e42211e131a48&quot;, &quot;RepoTags&quot;: [ &quot;myubuntu:latest&quot;, &quot;ubuntu:18.04&quot;, &quot;ubuntu:latest&quot; ], &quot;RepoDigests&quot;: [ &quot;ubuntu@sha256:945039273a7b927869a07b375dc3148de16865de44dec8398672977e050a072e&quot; ], &quot;Parent&quot;: &quot;&quot;, &quot;Comment&quot;: &quot;&quot;, &quot;Created&quot;: &quot;2019-01-22T22:41:28.350121367Z&quot;, &quot;Container&quot;: &quot;1777162cb05fa6d1d943be26346c8127a8ad2fa2df3ff0d53d5fa768714b2ecc&quot;, &quot;ContainerConfig&quot;: &#123; &quot;Hostname&quot;: &quot;1777162cb05f&quot;, &quot;Domainname&quot;: &quot;&quot;, &quot;User&quot;: &quot;&quot;, &quot;AttachStdin&quot;: false, &quot;AttachStdout&quot;: false, &quot;AttachStderr&quot;: false, &quot;Tty&quot;: false, &quot;OpenStdin&quot;: false, &quot;StdinOnce&quot;: false, &quot;Env&quot;: [ &quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot; ], &quot;Cmd&quot;: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;#(nop) &quot;, &quot;CMD [\&quot;/bin/bash\&quot;]&quot; ], &quot;ArgsEscaped&quot;: true, &quot;Image&quot;: &quot;sha256:1497d63c8adfb96cfccfaba0eacc2d269d07cc49047d5e0ec8fe53e37d7e9d93&quot;, &quot;Volumes&quot;: null, &quot;WorkingDir&quot;: &quot;&quot;, &quot;Entrypoint&quot;: null, &quot;OnBuild&quot;: null, &quot;Labels&quot;: &#123;&#125; &#125;, &quot;DockerVersion&quot;: &quot;18.06.1-ce&quot;, &quot;Author&quot;: &quot;&quot;, &quot;Config&quot;: &#123; &quot;Hostname&quot;: &quot;&quot;, &quot;Domainname&quot;: &quot;&quot;, &quot;User&quot;: &quot;&quot;, &quot;AttachStdin&quot;: false, &quot;AttachStdout&quot;: false, &quot;AttachStderr&quot;: false, &quot;Tty&quot;: false, &quot;OpenStdin&quot;: false, &quot;StdinOnce&quot;: false, &quot;Env&quot;: [ &quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot; ], &quot;Cmd&quot;: [ &quot;/bin/bash&quot; ], &quot;ArgsEscaped&quot;: true, &quot;Image&quot;: &quot;sha256:1497d63c8adfb96cfccfaba0eacc2d269d07cc49047d5e0ec8fe53e37d7e9d93&quot;, &quot;Volumes&quot;: null, &quot;WorkingDir&quot;: &quot;&quot;, &quot;Entrypoint&quot;: null, &quot;OnBuild&quot;: null, &quot;Labels&quot;: null &#125;, &quot;Architecture&quot;: &quot;amd64&quot;, &quot;Os&quot;: &quot;linux&quot;, &quot;Size&quot;: 87475457, &quot;VirtualSize&quot;: 87475457, &quot;GraphDriver&quot;: &#123; &quot;Data&quot;: &#123; &quot;LowerDir&quot;: &quot;/mnt/sda1/var/lib/docker/overlay2/adf7c930b8b440dcfb5c968145f57a3c7f8ec8fdfb6c3a559ad58a92fcdb3f42/diff:/mnt/sda1/var/lib/docker/overlay2/3a417e85881816cc6647326e8bb654634ae88e53f3c659499cc8d764ce113468/diff:/mnt/sda1/var/lib/docker/overlay2/9b924520253fbf5e4d9b8a4753bd414f4ce0dbcadcdeed11dcb746321cb170fa/diff&quot;, &quot;MergedDir&quot;: &quot;/mnt/sda1/var/lib/docker/overlay2/166872e0c7950e01acd0e3bc3ba663074f4701cb98ccabdef9365269424c823a/merged&quot;, &quot;UpperDir&quot;: &quot;/mnt/sda1/var/lib/docker/overlay2/166872e0c7950e01acd0e3bc3ba663074f4701cb98ccabdef9365269424c823a/diff&quot;, &quot;WorkDir&quot;: &quot;/mnt/sda1/var/lib/docker/overlay2/166872e0c7950e01acd0e3bc3ba663074f4701cb98ccabdef9365269424c823a/work&quot; &#125;, &quot;Name&quot;: &quot;overlay2&quot; &#125;, &quot;RootFS&quot;: &#123; &quot;Type&quot;: &quot;layers&quot;, &quot;Layers&quot;: [ &quot;sha256:adcb570ae9ac70d0f46badf9ee0ecd49fbec2ae0bc26254653f99afa60046a4e&quot;, &quot;sha256:7604c8714555c5c681ce97ab9114c24d5b128e248724bd3e8389f3ccbe1f09a4&quot;, &quot;sha256:9e9d3c3a74581b0bb947e9621526ccec317d3cae000bc3c5d7b7f892bc1b4baa&quot;, &quot;sha256:27a216ffe82565764e05e8e0166a2505242bbf4c359a2eaf737e0a4b9d3c3000&quot; ] &#125;, &quot;Metadata&quot;: &#123; &quot;LastTagTime&quot;: &quot;2019-01-26T02:51:38.460681503Z&quot; &#125; &#125;] 4、使用history命令查看镜像历史该命令将列出各层的创建信息 1234567$ docker history ubuntu:18.04IMAGE CREATED CREATED BY SIZE COMMENT20bb25d32758 3 days ago /bin/sh -c #(nop) CMD [&quot;/bin/bash&quot;] 0B&lt;missing&gt; 3 days ago /bin/sh -c mkdir -p /run/systemd &amp;&amp; echo &apos;do… 7B&lt;missing&gt; 3 days ago /bin/sh -c rm -rf /var/lib/apt/lists/* 0B&lt;missing&gt; 3 days ago /bin/sh -c set -xe &amp;&amp; echo &apos;#!/bin/sh&apos; &gt; /… 745B&lt;missing&gt; 3 days ago /bin/sh -c #(nop) ADD file:38a199e521f5e9007… 87.5MB 注意：过长的信息被自动截断了，可以用 --no-trunc来输出完整信息 三、搜寻镜像docker search [option] keyword 支持的选项 -f, --filter filter : 过滤输出内容 --format string : 格式化输出内容 --limit int : 限制输出结果，默认25个 --no-trunc : 不截断输出结果 搜索官方提供的带Nginx关键字的镜像 123$ docker search --filter=is-official=true nginxNAME DESCRIPTION STARS OFFICIAL AUTOMATEDnginx Official build of Nginx. 10785 [OK] 搜索所有收藏数超过4的关键词包括TensorFlow的镜像 12345678910$ docker search --filter=stars=4 tensorflowNAME DESCRIPTION STARS OFFICIAL AUTOMATEDtensorflow/tensorflow Official Docker images for the machine learn… 1273 jupyter/tensorflow-notebook Jupyter Notebook Scientific Python Stack w/ … 109 xblaster/tensorflow-jupyter Dockerized Jupyter with tensorflow 52 [OK]tensorflow/serving Official images for TensorFlow Serving (http… 35 floydhub/tensorflow tensorflow 15 [OK]bitnami/tensorflow-serving Bitnami Docker Image for TensorFlow Serving 13 [OK]opensciencegrid/tensorflow-gpu TensorFlow GPU set up for OSG 7 tensorflow/tf_grpc_server Server for TensorFlow GRPC Distributed Runti… 7 四、删除和清理镜像1、使用标签删除镜像docker rmi 或 docker image rm 当镜像有两个或以上的标签时，只仅仅删除标签本身，不影响镜像 当镜像只有一个镜像的时候， 就会删除这个镜像所有文件层 1234567891011121314151617$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEmyubuntu latest 20bb25d32758 3 days ago 87.5MBubuntu 18.04 20bb25d32758 3 days ago 87.5MBubuntu latest 20bb25d32758 3 days ago 87.5MBscrapinghub/splash latest 3926e5aac017 11 months ago 1.22GBsen@TR-PC MINGW64 /d/Program Files/Docker Toolbox$ docker rmi myubuntu:latestUntagged: myubuntu:latestsen@TR-PC MINGW64 /d/Program Files/Docker Toolbox$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEubuntu 18.04 20bb25d32758 3 days ago 87.5MBubuntu latest 20bb25d32758 3 days ago 87.5MBscrapinghub/splash latest 3926e5aac017 11 months ago 1.22GB 支持选项 -f , -force : 强制删除镜像，即便有容器依赖它 -no-prne : 不要清理未带标签的父镜像 2、使用镜像ID来删除镜像docker rmi 后面跟上镜像的ID（也可以是区分的部分ID串前缀），会尝试先删除所有指向该镜像的标签，然后删除该镜像文件本身。 但，有该镜像创建的容器存在时，镜像文件默认是无法被删除的。 docker ps -a 可以查看本机上存在的所有容器 使用-f 可以强行删除（不推荐） 应该是先删除容器，在删除镜像 123456789101112131415161718sen@TR-PC MINGW64 /d/Program Files/Docker Toolbox$ docker rm 506537568a78506537568a78sen@TR-PC MINGW64 /d/Program Files/Docker Toolbox$ docker rmi 20bb25d32758Untagged: ubuntu:latestUntagged: ubuntu@sha256:945039273a7b927869a07b375dc3148de16865de44dec8398672977e050a072eDeleted: sha256:20bb25d32758db4f91b18a9581794cfaa6a8c5fbad80093e9a9e42211e131a48Deleted: sha256:7b2bffd1a66cacd8cd989f06cee49a1fba28c1d149806a0f7b536229270ddfd2Deleted: sha256:80f6e37bc2041d00cbd950851c20f0f16b81b8f323290f354279a8a7b62bb985Deleted: sha256:2069390c92947b82f9333ac82a40e3eeaa6662ae84600a9b425dd296af105469Deleted: sha256:adcb570ae9ac70d0f46badf9ee0ecd49fbec2ae0bc26254653f99afa60046a4esen@TR-PC MINGW64 /d/Program Files/Docker Toolbox$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEscrapinghub/splash latest 3926e5aac017 11 months ago 1.22GB 3、清理镜像docker image prune清理临时镜像文件 -a , -all : 删除所有无用镜像，不光是临时镜像 -filter filter : 只清理符合给定过滤器的镜像 -f , -force ：强行删除镜像，而不进行提示确认 清理临时的遗留镜像文件层，，最后会提示释放的存储空间 12$ docker image prune -fTotal reclaimed space: 0B 五、创建镜像1、基于已有的容器创建docker [container] commit [options] CONTAINER [REPOSITORY[:TAG]] -a , --author=&quot;&quot; : 作者信息； -c , --change=[] : 提交的时候执行Dockerfile 指令，包括CMD|ENTRYPOINT|ENV|EXPOSE|LABEL|ONBULLD|USER|VOLUME|WORKDIR等； -m , --message=&quot;&quot; : 提交信息； -p , --pause=true : 提交时暂停容器运行 启动一个镜像，并在其中进行修改操作 ​ 创建一个test文件 1234$ docker run -it ubuntu /bin/bashroot@632d42bede53:/# touch testroot@632d42bede53:/# exitexit 此时该容器和原ubuntu镜像相比，已经发生了改变。可以用docker commit 命令来提交一个新的镜像。提交时，可以使用ID或名称来指定容器。 顺利的话，可以返回新创建镜像的ID信息 123456$ docker commit -m &quot;Added a new file&quot; -a &quot;Docker Newbee&quot; 632d42bede53 test:0.1sha256:eb34cf66ffd8bde3e275853fab4c9840ac3c06d5a8d0472e9d32f82b7c4a6a1f$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEtest 0.1 eb34cf66ffd8 7 seconds ago 120MB 2、基于本地模板导入用户也可以直接从一个操作系统模板文件导入一个镜像，主要使用docker [container] import命令。 命令格式为docker [image] import [OPTIONS] file|URL|-[REPOSITORY[:TAG]] 要直接导入一个镜像，可以使用OpenVZ提供的模板来创建， 或者用其他已导出的镜像模板来创建。OPENVZ模板下载地址为 http://openvz.org/Download/templates/precreated 123456789101112sen@TR-PC MINGW64 /d/Program Files/Docker Toolbox$ cat ubuntu.tar | docker import - ubuntu:18.04sha256:5c7b00783d07192ea2af11f83b6847d2826cbdbc7eafd79c7e3693bef43b8858sen@TR-PC MINGW64 /d/Program Files/Docker Toolbox$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEubuntu 18.04 5c7b00783d07 5 seconds ago 124MBubuntu latest 5b1075c8fe30 38 seconds ago 124MBtest 0.1 eb34cf66ffd8 37 minutes ago 120MBscrapinghub/splash latest 3926e5aac017 11 months ago 1.22GBhub.c.163.com/library/ubuntu latest ccc7a11d65b1 17 months ago 120MB *3、基于Dockerfile创建（后面会介绍）Dockerfile是一个文本文件，利用给定的指令描述基于某个父镜像创建新镜像的过程 实例： ​ 基于debian:stretch-slim镜像安装python3环境，构成一个python3的镜像 FROM debian:stretch-slim LABEL version=&quot;1.0&quot; maintainer=&quot;docker user &lt;docker_user@github&gt;&quot; RUN apt-get update &amp;&amp; \ apt-get install -y install python3 &amp;&amp; \ apt-get clean &amp;&amp; \ rm -rf /var/lib/apt/lists/* 创建镜像的过程可以使用 docker [image] build命令，编译成功后本地将多出一个python：3的镜像 123456$ docker [image] bulid -t python:3...Successfully built 4b10f46eacc8Successfully tagged python:3$ docker images|grep pythonpython 3 4b10f46eacc8 About a minute ago 95.01MB 六、存出和载入镜像docker [image] save 和 docker [image] load 1、存出镜像docker [image] save 支持-o , -output string参数，导出镜像到指定的文件中 1$ docker save -o ubuntu.tar ubuntu 2、载入镜像docker [image] load 将导出的tar文件再导入本地镜像库。 支持-i , -input string选项，从指定的文件中读入镜像内容 。 1234567$ docker load -i ubuntu.tarThe image ubuntu:latest already exists, renaming the old one with ID sha256:5b1075c8fe309f0318d81b3365050afe10773df73425f8c1a8ef965190d195b4 to empty stringLoaded image: ubuntu:latestsen@TR-PC MINGW64 /d/Program Files/Docker Toolbox$ docker load &lt; ubuntu.tarLoaded image: ubuntu:latest 七、上传镜像docker [image] push上传镜像到仓库， 默认上传到docker hub（需要登录），命令格式：docker [image] push NAME[:TAG] | [REGISTRY_HOST[:REGISTRY_PORT]/]NAME[:TAG] 例如： ​ 用户user上传本地test: latest镜像，可以先添加新的标签 user/test:latest，然后用docker [image] push命令上传镜像： 12345$ docker tag ubuntu:latest user/test:latestsen@TR-PC MINGW64 /d/Program Files/Docker Toolbox$ docker push user/test:latestThe push refers to repository [docker.io/user/test]]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>服务器</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker入门]]></title>
    <url>%2F2019%2F01%2F27%2Fdocker%2F01%E3%80%81docker%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[docker 命令大全1、获取镜像12docker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签]docker pull nginx Docker 镜像仓库地址：地址的格式一般是 &lt;域名/IP&gt;[:端口号]。默认地址是 Docker Hub。 仓库名：如之前所说，这里的仓库名是两段式名称，即 &lt;用户名&gt;/&lt;软件名&gt;。对于 Docker Hub，如果不给出用户名，则默认为 library，也就是官方镜像。 2、启动==每次运行都是一个新的容器实例== 1docker run --name test -it --rm ubuntu:16.04 bash -it 这是两个参数，一个是 -i ：交互式操作，一个是 -t 终端。我们这里打算进入 bash执行一些命令并查看返回结果，因此我们需要交互式终端。 —rm ：这个参数是说容器退出后随之将其删除。==默认情况下，退出的容器，里面的内容并不会立即删除==，除非手动 docker rm 。我们这里只是随便执行个命令，看看结果， 不需要排障和保留结果，因此使用 —rm 可以避免浪费空间。 —name 是可以指定这个容器的名字，后面可以通过名字来操作 ubuntu:16.04：这是指用ubuntu:16.04镜像为基础来启动容器。 bash：放在镜像名后的是命令，这里我们希望有个交互式Shell，因此用的是bash 1docker run --name webserver -d -p 80:80 nginx -p 是进行端口映射 前面的外部端口 后面的是 docker 内部端口 -d 是作为守护进程后台运行 3、开始和关闭12345678# 启动新的实例docker run -it ubuntuL16.04 bash# 恢复之前的实例docker start ubuntu_ID# 关闭实例docker stop ubuntu_ID 4、查看所有镜像1docker images 5、删除容器 （删除了容器，才能删除镜像）==镜像启动后就是容器== 查看所有的容器 docker ps -a 删除指定的容器 docker rm ID 删除镜像 docker rmi ID 删除所有的镜像 -q 表示只返回容器的ID docker rm &#39;docker ps -a -q&#39; 6、构建镜像6.1、使用 commit 构建123456789# 首先我们启动了一个镜像docker run -it -p 80:80 nginx bash# 假设我们在这个系统里面修改了一些东西# 然后我们 docker commit 命令来构建新的镜像docker commit [选项] &lt;容器ID或容器名&gt; [&lt;仓库名&gt;[:&lt;标签&gt;]] nginx:stefan 是新名字docker commit --message &quot;修改了东西&quot; ID nginx:stefan# 这样我们就从原有镜像的基础上 构建了一个镜像 6.2、使用 Dockerfile 来构建 创建一个 Dockerfile 文件 内容为 12FROM nginx RUN echo '&lt;h1&gt;Hello,Docker!&lt;/h1&gt;' &gt; /usr/share/nginx/html/index.html 这个Dockerfile很简单，一共就两行。涉及到了两条指令，FROM和RUN。 命令解释 FROM 表示以那个镜像为基础 RUN 表示运行一条命令，每次 RUN 都会生成一个零时镜像 shell 格式 就像直接在命令行中输入的命令一样 RUN echo &#39;Hello&#39; exec 格式 RUN [&#39;可执行文件&#39;, &#39;参数1&#39;, &#39;参数2&#39;] 7、构建镜像12# 进到 Dockerfile 文件目录 docker build [选项] &lt;上下文路径/URL/-&gt;docker build -t test:stefan ./]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>服务器</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Node.js Express 框架]]></title>
    <url>%2F2018%2F09%2F05%2Fjavascript%2Fnodejs%2F03%E3%80%81Express%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[Node.js Express 框架1、Express 简介Express 是一个简洁而灵活的 node.js Web应用框架, 提供了一系列强大特性帮助你创建各种 Web 应用，和丰富的 HTTP 工具。使用 Express 可以快速地搭建一个完整功能的网站。Express 框架核心特性： 可以设置中间件来响应 HTTP 请求。 定义了路由表用于执行不同的 HTTP 请求动作。 可以通过向模板传递参数来动态渲染 HTML 页面。 2、安装 Express安装 Express 并将其保存到依赖列表中： 1$ cnpm install express --save 以上命令会将 Express 框架安装在当前目录的 node_modules 目录中， node_modules 目录下会自动创建 express 目录。以下几个重要的模块是需要与 express 框架一起安装的： body-parser - node.js 中间件，用于处理 JSON, Raw, Text 和 URL 编码的数据。 cookie-parser - 这就是一个解析Cookie的工具。通过req.cookies可以取到传过来的cookie，并把它们转成对象。 multer - node.js 中间件，用于处理 enctype=”multipart/form-data”（设置表单的MIME编码）的表单数据。 123$ cnpm install body-parser --save$ cnpm install cookie-parser --save$ cnpm install multer --save 安装完后，我们可以查看下 express 使用的版本号： 123$ cnpm list express/data/www/node└── express@4.15.2 -&gt; /Users/tianqixin/www/node/node_modules/.4.15.2@express 3、第一个 Express 框架实例接下来我们使用 Express 框架来输出 “Hello World”。 以下实例中我们引入了 express 模块，并在客户端发起请求后，响应 “Hello World” 字符串。 创建 express_demo.js 文件，代码如下所示： 123456789101112var express = require('express');var app = express();app.get('/',function (req,res) &#123; res.send('Hello World');&#125;)var server = app.listen(8081,function ()&#123; var host = server.address().address var port = server.address().port console.log('应用实例，访问地址为http://%s:%s',host,port);&#125;) 执行代码，浏览器访问127.0.0.1:8081： 1应用实例，访问地址为 http://:::8081 4、请求和响应Express 应用使用回调函数的参数： request 和 response 对象来处理请求和响应的数据。 123app.get(&apos;/&apos;, function (req, res) &#123; // --&#125;) request 和 response 对象的具体介绍： Request 对象 - request 对象表示 HTTP 请求，包含了请求查询字符串，参数，内容，HTTP 头部等属性。常见属性有： req.app：当callback为外部文件时，用req.app访问express的实例 req.baseUrl：获取路由当前安装的URL路径 req.body / req.cookies：获得「请求主体」/ Cookies req.fresh / req.stale：判断请求是否还「新鲜」 req.hostname / req.ip：获取主机名和IP地址 req.originalUrl：获取原始请求URL req.params：获取路由的parameters req.path：获取请求路径 req.protocol：获取协议类型 req.query：获取URL的查询参数串 req.route：获取当前匹配的路由 req.subdomains：获取子域名 req.accepts()：检查可接受的请求的文档类型 req.acceptsCharsets / req.acceptsEncodings / req.acceptsLanguages：返回指定字符集的第一个可接受字符编码 req.get()：获取指定的HTTP请求头 req.is()：判断请求头Content-Type的MIME类型 Response 对象 - response 对象表示 HTTP 响应，即在接收到请求时向客户端发送的 HTTP 响应数据。常见属性有： res.app：同req.app一样 res.append()：追加指定HTTP头 res.set()在res.append()后将重置之前设置的头 res.cookie(name，value [，option])：设置Cookie opition: domain / expires / httpOnly / maxAge / path / secure / signed res.clearCookie()：清除Cookie res.download()：传送指定路径的文件 res.get()：返回指定的HTTP头 res.json()：传送JSON响应 res.jsonp()：传送JSONP响应 res.location()：只设置响应的Location HTTP头，不设置状态码或者close response res.redirect()：设置响应的Location HTTP头，并且设置状态码302 res.render(view,[locals],callback)：渲染一个view，同时向callback传递渲染后的字符串，如果在渲染过程中有错误发生next(err)将会被自动调用。callback将会被传入一个可能发生的错误以及渲染后的页面，这样就不会自动输出了。 res.send()：传送HTTP响应 res.sendFile(path [，options][，fn])：传送指定路径的文件 -会自动根据文件extension设定Content-Type res.set()：设置HTTP头，传入object可以一次设置多个头 res.status()：设置HTTP状态码 res.type()：设置Content-Type的MIME类型 5、路由我们已经了解了 HTTP 请求的基本应用，而路由决定了由谁(指定脚本)去响应客户端请求。 在HTTP请求中，我们可以通过路由提取出请求的URL以及GET/POST参数。 接下来我们扩展 Hello World，添加一些功能来处理更多类型的 HTTP 请求。 创建 express_demo2.js 文件，代码如下所示： 123456789101112131415161718192021222324252627282930313233343536373839var express = require('express');var app = express();// 主页输出 'Hello World'app.get('/',function(erq,res)&#123; console.log('主页GET请求'); res.send('Hello GET');&#125;)// POST 请求app.post('/',function (req, res)&#123; console.log('主页 POST 请求'); res.send('Hello POST');&#125;)// /del_user 页面响应app.get('/del_user',function (req,res) &#123; console.log('/del_user 响应 DELETE 请求'); res.send('删除页面');&#125;)// /list_user页面 GET 请求app.get('/list_user',function (req,res)&#123; console.log('/list_user GET 请求'); res.send('用户列表页面');&#125;)// 对页面 abcd,abxcd,ab123cd,等响应GET请求app.get('/ab*cd',function(req,res)&#123; console.log('/ab*cd GET 请求'); res.send('正则匹配');&#125;)var server = app.listen(8081,function()&#123; var host = server.address().address var port = server.address().port console.log('应用实例，访问地址为http://%s:%s',host,port)&#125;) 浏览器访问http://127.0.0.1:8081/，http://127.0.0.1:8081/del_user，http://127.0.0.1:8081/list_user，http://127.0.0.1:8081/ab123cd， http://127.0.0.1:8081/abcdefg观察页面和控制台的变化 12345应用实例，访问地址为http://:::8081主页GET请求/del_user 响应 DELETE 请求/list_user GET 请求/ab*cd GET 请求 在浏览器中访问 http://127.0.0.1:8081/abcdefg 6、静态文件Express 提供了内置的中间件 express.static 来设置静态文件如：图片， CSS, JavaScript 等。 你可以使用 express.static 中间件来设置静态文件路径。例如，如果你将图片， CSS, JavaScript 文件放在 public 目录下，你可以这么写： 1app.use(express.static('public')); 我们可以到 public/images 目录下放些图片,如下所示： 12345node_modulesserver.jspublic/public/imagespublic/images/logo.png 让我们再修改下 “Hello World” 应用添加处理静态文件的功能。 创建 express_demo3.js 文件，代码如下所示： 1234567891011121314var express = require('express');var app = express();app.use(express.static('public'));// console.log(app.use(express.static('public')));app.get('/',function (req,res) &#123; res.send('Hello Word');&#125;)var server = app.listen(8081,function()&#123; var host = server.address().address var port = server.address().port console.log('应用实例，访问地址为http://%s:%s',host,port)&#125;) 执行以上代码： 12$ node express_demo3.js 应用实例，访问地址为http://:::8081 执行以上代码： 在浏览器中访问 http://127.0.0.1:8081/images/a.png（本实例采用了菜鸟教程的logo） 7、GET 方法以下实例演示了在表单中通过 GET 方法提交两个参数，我们可以使用 server.js 文件内的 process_get 路由器来处理输入： index_get.html 12345678910&lt;html&gt;&lt;body&gt;&lt;form action="http://127.0.0.1:8081/process_get" method="GET"&gt;First Name: &lt;input type="text" name="first_name"&gt; &lt;br&gt; Last Name: &lt;input type="text" name="last_name"&gt;&lt;input type="submit" value="Submit"&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; server_get.js 1234567891011121314151617181920212223var express = require('express');var app = express();app.use(express.static('public'));app.get('/index_get.html',function (req,res) &#123; res.sendFile(__dirname + '/' + 'index_get.html');&#125;)app.get('/process_get',function (req,res) &#123; // 输出JSON格式 var response = &#123; 'first_name':req.query.first_name, 'last_name':req.query.last_name &#125;; console.log(response); res.end(JSON.stringify(response));&#125;)var server = app.listen(8081, function()&#123; var host = server.address().address var port = server.address().port console.log('应用实例，访问地址为http://%s:%s',host,port)&#125;) 执行以上代码： 12应用实例，访问地址为http://:::8081&#123; first_name: &apos;a&apos;, last_name: &apos;a&apos; &#125; 浏览器访问 http://127.0.0.1:8081/index_get.html 现在你可以向表单输入数据，并提交,页面展示为： 1&#123;&quot;first_name&quot;:&quot;a&quot;,&quot;last_name&quot;:&quot;a&quot;&#125; 8、POST 方法以下实例演示了在表单中通过 POST 方法提交两个参数，我们可以使用 server.js 文件内的 process_post 路由器来处理输入： index_post.html 12345678910&lt;html&gt;&lt;body&gt;&lt;form action="http://127.0.0.1:8081/process_post" method="POST"&gt;First Name: &lt;input type="text" name="first_name"&gt; &lt;br&gt; Last Name: &lt;input type="text" name="last_name"&gt;&lt;input type="submit" value="Submit"&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; server_post.js 123456789101112131415161718192021222324252627282930var express = require('express');var app = express();var bodyParser = require('body-parser');// 创建application/x-www-form-urlencoded 编码解析var urlencodedParser = bodyParser.urlencoded(&#123;extended: false&#125;)app.use(express.static('public'));app.get('/index_post.html',function(req,res) &#123; res.sendFile(__dirname + '/' + 'index_post.html');&#125;)app.post('/process_post',urlencodedParser,function (req,res) &#123; // 输出 JSON 格式 var response = &#123; 'first_name':req.body.first_name, 'last_name':req.body.last_name &#125;; console.log(response); res.end(JSON.stringify(response));&#125;)var server = app.listen(8081,function()&#123; var host = server.address().address var port = server.address().port console.log('应用实例，访问地址为http://%s:%s',host,port)&#125;) 执行以上代码： 浏览器访问 http://127.0.0.1:8081/index_post.html 现在你可以向表单输入数据，并提交 1234应用实例，访问地址为http://:::8081&#123; first_name: &apos;a&apos;, last_name: &apos;a&apos; &#125;页面上：&#123;&quot;first_name&quot;:&quot;a&quot;,&quot;last_name&quot;:&quot;a&quot;&#125; 9、文件上传以下我们创建一个用于上传文件的表单，使用 POST 方法，表单 enctype 属性设置为 multipart/form-data。 index_file_upload.html 1234567891011121314&lt;html&gt;&lt;head&gt;&lt;title&gt;文件上传表单&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h3&gt;文件上传：&lt;/h3&gt;选择一个文件上传: &lt;br /&gt;&lt;form action="/file_upload" method="post" enctype="multipart/form-data"&gt;&lt;input type="file" name="image" size="50" /&gt;&lt;br /&gt;&lt;input type="submit" value="上传文件" /&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; server_file_upload.js 1234567891011121314151617181920212223242526272829303132333435363738var express = require('express');var app = express();var fs = require('fs')var bodyParser = require('body-parser');var multer = require('multer');app.use(express.static('public'));app.use(bodyParser.urlencoded(&#123;extended:false&#125;));app.use(multer(&#123;dest:'/tmp/'&#125;).array('image'));app.get('/index_file_upload.html',function(req,res) &#123; res.sendFile(__dirname + '/' + 'index_file_upload.html');&#125;)app.post('/file_upload',function (req,res)&#123; console.log(req.files[0]); // 上传的文件信息 var des_file = __dirname + '/' + req.files[0].originalname; fs.readFile(req.files[0].path,function(err) &#123; if(err)&#123; console.log(err); &#125;else&#123; response = &#123; message:'File uploaded successfully', filename:req.files[0].originalname &#125;; &#125; console.log(response); res.end(JSON.stringify(response)); &#125;);&#125;)var server = app.listen(8081,function() &#123; var host = server.address().address var port = server.address().port console.log('应用实例，访问地址为 http://%s:%s',host,port)&#125;) 执行以上代码： 1234567891011应用实例，访问地址为 http://:::8081&#123; fieldname: &apos;image&apos;, originalname: &apos;QQ浏览器截图20180906135306.png&apos;, encoding: &apos;7bit&apos;, mimetype: &apos;image/png&apos;, destination: &apos;/tmp/&apos;, filename: &apos;2a9c3f46ac9aaa9b9820723683bc1869&apos;, path: &apos;\\tmp\\2a9c3f46ac9aaa9b9820723683bc1869&apos;, size: 17532 &#125;&#123; message: &apos;File uploaded successfully&apos;, filename: &apos;QQ浏览器截图20180906135306.png&apos; &#125; 浏览器访问 http://127.0.0.1:8081/index_file_upload.html 10、Cookie 管理我们可以使用中间件向 Node.js 服务器发送 cookie 信息，以下代码输出了客户端发送的 cookie 信息： 123456789101112var express = require('express');var cookieParser = require('cookie-parser');var util = require('util');var app = express();app.use(cookieParser())app.get('/',function(req,res) &#123; console.log('Cookies: ' + util.inspect(req.cookies));&#125;)app.listen(8081) 执行以上代码： 12$ node express_cookie.js Cookies: &#123;&#125; 现在你可以访问 http://127.0.0.1:8081 并查看终端信息的输出 相关资料 Express官网： http://expressjs.com/ Express4.x API 中文版： Express4.x API Chinese Express4.x API：http://expressjs.com/zh-cn/4x/api.html]]></content>
      <categories>
        <category>javascript</category>
        <category>nodejs</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>javascript</tag>
        <tag>nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NodeJS]]></title>
    <url>%2F2018%2F09%2F03%2Fjavascript%2Fnodejs%2F02%E3%80%81nodeJS%2F</url>
    <content type="text"><![CDATA[1、安装模块npm install 模块名1234567891011121314151617181920212223242526272829303132333435npm install express # 本地安装npm install express -g # 全局安装# 如果出现错误npm err! Error: connect ECONNREFUSED 127.0.0.1:8087 # 解决方法为npm config set proxy null# 查看安装信息npm list -g# 查看某个模块的版本号npm list 模块名# 卸载模块npm uninstall 模块名NPM提供了很多命令，例如install和publish，使用npm help可查看所有命令。使用npm help &lt;command&gt;可查看某条命令的详细帮助，例如npm help install。在package.json所在目录下使用npm install . -g可先在本地安装当前命令行程序，可用于发布前的本地测试。使用npm update &lt;package&gt;可以把当前目录下node_modules子目录里边的对应模块更新至最新版本。使用npm update &lt;package&gt; -g可以把全局安装的对应命令行程序更新至最新版。使用npm cache clear可以清空NPM本地缓存，用于对付使用相同版本号发布新版本代码的人。使用npm unpublish &lt;package&gt;@&lt;version&gt;可以撤销发布自己发布过的某个版本代码。使用淘宝 NPM 镜像npm install -g cnpm --registry=https://registry.npm.taobao.org这样就可以使用 cnpm 命令来安装模块了：cnpm install [name] 2、NodeJS REPL交互解释器（1）多行表达式1234567891011121314$ node&gt; var x = 0undefined&gt; do &#123;... x++;... console.log(&quot;x: &quot; + x);... &#125; while ( x &lt; 5 );x: 1x: 2x: 3x: 4x: 5undefined&gt; （2）下划线(_)变量你可以使用下划线(_)获取上一个表达式的运算结果： 123456789101112$ node&gt; var x = 10undefined&gt; var y = 20undefined&gt; x + y30&gt; var sum = _undefined&gt; console.log(sum)30undefined REPL命令 12345678910111213141516171819ctrl + c - 退出当前终端。ctrl + c 按下两次 - 退出 Node REPL。ctrl + d - 退出 Node REPL.向上/向下 键 - 查看输入的历史命令tab 键 - 列出当前命令.help - 列出使用命令.break - 退出多行表达式.clear - 退出多行表达式.save filename - 保存当前的 Node REPL 会话到指定文件.load filename - 载入当前 Node REPL 会话的文件内容。 3、回调函数Node.js 异步编程的直接体现就是回调。 阻塞代码实现创建一个文件 input.txt ，内容如下： 1菜鸟教程官网地址：www.runoob.com 创建 main.js 文件, 代码如下： 1234var fs = require('fs');var data = fs.readFileSync('input.txt');console.log(data.toString());console.log('程序执行结束') 执行结果： 1234$ node main.js菜鸟教程官网地址：www.runoob.com程序执行结束! 非阻塞代码实例创建 main.js 文件, 代码如下： 1234567var fs = require('fs');fs.readFile('input.txt',function (err,data)&#123; if (err) return console.error(err); console.log(data.toString());&#125;)console.log('程序执行结束') 执行结果： 123$ node main.js程序执行结束!菜鸟教程官网地址：www.runoob.com 以上两个实例我们了解了阻塞与非阻塞调用的不同。第一个实例在文件读取完后才执行完程序。 第二个实例我们不需要等待文件读取完，这样就可以在读取文件时同时执行接下来的代码，大大提高了程序的性能。 因此，阻塞是按顺序执行的，而非阻塞是不需要按顺序的，所以如果需要处理回调函数的参数，我们就需要写在回调函数内。 4、事件循环Node.js 是单进程单线程应用程序，但是因为 V8 引擎提供的异步执行回调接口，通过这些接口可以处理大量的并发，所以性能非常高。 Node.js 几乎每一个 API 都是支持回调函数的。 Node.js 基本上所有的事件机制都是用设计模式中观察者模式实现。 Node.js 单线程类似进入一个while(true)的事件循环，直到没有事件观察者退出，每个异步事件都生成一个事件观察者，如果有事件发生就调用该回调函数 （1）事件驱动程序Node.js 使用事件驱动模型，当web server接收到请求，就把它关闭然后进行处理，然后去服务下一个web请求。 当这个请求完成，它被放回处理队列，当到达队列开头，这个结果被返回给用户。 这个模型非常高效可扩展性非常强，因为webserver一直接受请求而不等待任何读写操作。（这也被称之为非阻塞式IO或者事件驱动IO） 在事件驱动模型中，会生成一个主循环来监听事件，当检测到事件时触发回调函数。 执行流程 12345678// 引入events模块var events = require('events');// 创建eventsEmitter对象var eventEmitter = new events.EventEmitter();// 绑定时间处理程序eventEmitter.on('eventName',eventHandler);// 触发事件eventEmitter.emit('eventName'); 实例 123456789101112131415161718192021222324// 引入events模块var events = require('events');// 创建eventEmitter对象var eventEmitter = new events.EventEmitter();// 创建时间处理程序var coobnectHandler = function connected() &#123; console.log('连接成功'); // 触发data_received事件 eventEmitter.emit('data_received');&#125;// 绑定connection 时间处理程序eventEmitter.on('connection',coobnectHandler)// 使用匿名函数绑定data_recevied事件eventEmitter.on('data_received',function () &#123; console.log('数据连接成功');&#125;);// 触发 connection事件eventEmitter.emit('connection');console.log('程序执行完毕') 执行结果 1234E:\nodeJS\exe&gt;node exe01.js连接成功数据连接成功程序执行完毕 （2）捕获错误，并输出123456789var fs = require('fs') // 导包fs.readFile('input.txt',function (err,data) &#123; if (err)&#123; console.log('error:'+err.stack); return; &#125; console.log(data.toString());&#125;);console.log('程序执行完毕'); 如果没有input.txt，就会输出错误，no such file or directory 5、Node.js EventEmitterNode.js 所有的异步 I/O 操作在完成时都会发送一个事件到事件队列。 Node.js 里面的许多对象都会分发事件：一个 net.Server 对象会在每次有新连接时触发一个事件， 一个 fs.readStream 对象会在文件被打开的时候触发一个事件。 所有这些产生事件的对象都是 events.EventEmitter 的实例。 （1）EventEmitter 类EventEmitter 的核心就是事件触发与事件监听器功能的封装。 可以通过require(“events”);来访问该模块。 1234// 引入 events 模块var events = require('events');// 创建 eventEmitter 对象var eventEmitter = new events.EventEmitter(); EventEmitter 对象如果在实例化时发生错误，会触发 error 事件。当添加新的监听器时，newListener 事件会触发，当监听器被移除时，removeListener 事件被触发。 12345678910var EventEmitter = require('events').EventEmitter;var event = new EventEmitter();// 绑定事件处理程序event.on('some_event',function()&#123; console.log('some_event 事件触发');&#125;);setTimeout(function () &#123; event.emit('some_event'); // 调用some_event监听器&#125;,1000); // 设置时间 运行这段代码，1 秒后控制台输出了 ‘some_event 事件触发’。其原理是 event 对象注册了事件 some_event 的一个监听器，然后我们通过 setTimeout 在 1000 毫秒以后向 event 对象发送事件 some_event，此时会调用some_event 的监听器。 运行结果 12$ node event.js some_event 事件触发 EventEmitter 的每个事件由一个事件名和若干个参数组成，事件名是一个字符串，通常表达一定的语义。对于每个事件，EventEmitter 支持 若干个事件监听器。 当事件触发时，注册到这个事件的事件监听器被依次调用，事件参数作为回调函数参数传递。 12345678910var events = require('events');var emitter = new events.EventEmitter();emitter.on('someEvent',function(arg1,arg2)&#123; console.log('listener1',arg1,arg2);&#125;);emitter.on('someEvent',function(arg1,arg2)&#123; console.log('listener2',arg1,arg2);&#125;);// 绑定事件emitter.emit('someEvent','arg1参数','arg2参数');//调用，因为绑定了两次，所以会有两次输出 输出结果 123$ node event.js listener1 arg1 参数 arg2 参数listener2 arg1 参数 arg2 参数 emitter 为事件 someEvent 注册了两个事件监听器，然后触发了 someEvent 事件。 运行结果中可以看到两个事件监听器回调函数被先后调用。 这就是EventEmitter最简单的用法。 EventEmitter 提供了多个属性，如 on 和 emit。on 函数用于绑定事件函数，emit 属性用于触发一个事件。接下来我们来具体看下 EventEmitter 的属性介绍。 （2）监听事件的操作方法12345678910111213141516171819202122232425262728293031323334353637var events = require('events');var eventEmitter = new events.EventEmitter();// 监听器 #1var listener1 = function listener1()&#123; console.log('监听器 listener1 执行。')&#125;// 监听器 #2var listener2 = function listener2() &#123; console.log('监听器 listener2 执行。')&#125;// 绑定connection 事件，处理函数为listener1eventEmitter.addListener('connection',listener1);// 绑定connection 事件，处理函数为listener2eventEmitter.on('connection',listener2);var eventListeners = eventEmitter.listenerCount('connection')console.log(eventListeners + '个监听器监听连接事件');// 处理connection事件eventEmitter.emit('connection');// 移除监听绑定的listen1函数eventEmitter.removeListener('connection',listener1);console.log('listener1 不再受监听');// 触发连接事件eventEmitter.emit('connection');eventListeners = eventEmitter.listenerCount('connection');console.log(eventListeners + '个监听器监听连接事件');console.log('程序执行完毕'); 输出结果： 12345678E:\nodeJS\exe&gt;node event03.js2个监听器监听连接事件监听器 listener1 执行。监听器 listener2 执行。listener1 不再受监听监听器 listener2 执行。1个监听器监听连接事件程序执行完毕 （3）error事件EventEmitter 定义了一个特殊的事件 error，它包含了错误的语义，我们在遇到 异常的时候通常会触发 error 事件。 当 error 被触发时，EventEmitter 规定如果没有响 应的监听器，Node.js 会把它当作异常，退出程序并输出错误信息。 我们一般要为会触发 error 事件的对象设置监听器，避免遇到错误后整个程序崩溃。例如： 123var events = require('events'); var emitter = new events.EventEmitter(); emitter.emit('error'); （4）继承 EventEmitter大多数时候我们不会直接使用 EventEmitter，而是在对象中继承它。包括 fs、net、 http 在内的，只要是支持事件响应的核心模块都是 EventEmitter 的子类。 为什么要这样做呢？原因有两点： 首先，具有某个实体功能的对象实现事件符合语义， 事件的监听和发生应该是一个对象的方法。 其次 JavaScript 的对象机制是基于原型的，支持 部分多重继承，继承 EventEmitter 不会打乱对象原有的继承关系。 6、Buffer缓冲区（受限于版本问题，可能会报错）JavaScript 语言自身只有字符串数据类型，没有二进制数据类型。 但在处理像TCP流或文件流时，必须使用到二进制数据。因此在 Node.js中，定义了一个 Buffer 类，该类用来创建一个专门存放二进制数据的缓存区。 在 Node.js 中，Buffer 类是随 Node 内核一起发布的核心库。Buffer 库为 Node.js 带来了一种存储原始数据的方法，可以让 Node.js 处理二进制数据，每当需要在 Node.js 中处理I/O操作中移动的数据时，就有可能使用 Buffer 库。原始数据存储在 Buffer 类的实例中。一个 Buffer 类似于一个整数数组，但它对应于 V8 堆内存之外的一块原始内存。 1234567const buf = Buffer.from('runoob','ascii');// 输出 72756e6f6f62console.log(buf.toString('hex'));// 输出 cnVub29iconsole.log(buf.toString('base64')); Node.js 目前支持的字符编码包括： ascii - 仅支持 7 位 ASCII 数据。如果设置去掉高位的话，这种编码是非常快的。 utf8 - 多字节编码的 Unicode 字符。许多网页和其他文档格式都使用 UTF-8 。 utf16le - 2 或 4 个字节，小字节序编码的 Unicode 字符。支持代理对（U+10000 至 U+10FFFF）。 ucs2 - utf16le 的别名。 base64 - Base64 编码。 latin1 - 一种把 Buffer 编码成一字节编码的字符串的方式。 binary - latin1 的别名。 hex - 将每个字节编码为两个十六进制字符。 （1）创建 Buffer 类Buffer 提供了以下 API 来创建 Buffer 类： Buffer.alloc(size[, fill[, encoding]])： 返回一个指定大小的 Buffer 实例，如果没有设置 fill，则默认填满 0 Buffer.allocUnsafe(size)： 返回一个指定大小的 Buffer 实例，但是它不会被初始化，所以它可能包含敏感的数据 Buffer.allocUnsafeSlow(size) Buffer.from(array)： 返回一个被 array 的值初始化的新的 Buffer 实例（传入的 array 的元素只能是数字，不然就会自动被 0 覆盖） Buffer.from(arrayBuffer[, byteOffset[, length]])： 返回一个新建的与给定的 ArrayBuffer 共享同一内存的 Buffer。 Buffer.from(buffer)： 复制传入的 Buffer 实例的数据，并返回一个新的 Buffer 实例 Buffer.from(string[, encoding])： 返回一个被 string 的值初始化的新的 Buffer 实例 1234567891011121314151617181920// 创建一个长度为 10、且用 0 填充的 Buffer。const buf1 = Buffer.alloc(10);// 创建一个长度为 10、且用 0x1 填充的 Buffer。 const buf2 = Buffer.alloc(10, 1);// 创建一个长度为 10、且未初始化的 Buffer。// 这个方法比调用 Buffer.alloc() 更快，// 但返回的 Buffer 实例可能包含旧数据，// 因此需要使用 fill() 或 write() 重写。const buf3 = Buffer.allocUnsafe(10);// 创建一个包含 [0x1, 0x2, 0x3] 的 Buffer。const buf4 = Buffer.from([1, 2, 3]);// 创建一个包含 UTF-8 字节 [0x74, 0xc3, 0xa9, 0x73, 0x74] 的 Buffer。const buf5 = Buffer.from('tést');// 创建一个包含 Latin-1 字节 [0x74, 0xe9, 0x73, 0x74] 的 Buffer。const buf6 = Buffer.from('tést', 'latin1'); （2）写入缓存区1buf.write(string[, offset[, length]][, encoding]) 参数描述如下： string - 写入缓冲区的字符串。 offset - 缓冲区开始写入的索引值，默认为 0 。 length - 写入的字节数，默认为 buffer.length encoding - 使用的编码。默认为 ‘utf8’ 。 根据 encoding 的字符编码写入 string 到 buf 中的 offset 位置。 length 参数是写入的字节数。 如果 buf 没有足够的空间保存整个字符串，则只会写入 string 的一部分。 只部分解码的字符不会被写入。 返回值 返回实际写入的大小。如果 buffer 空间不足， 则只会写入部分字符串。 1234567buf = Buffer.alloc(256);len = buf.write("www.baidu.com");console.log("写入字节数 : "+ len);//写入字节数：13 （3）从缓冲区读取数据读取 Node 缓冲区数据的语法如下所示： 1buf.toString([encoding[, start[, end]]]) 参数 参数描述如下： encoding - 使用的编码。默认为 ‘utf8’ 。 start - 指定开始读取的索引位置，默认为 0。 end - 结束位置，默认为缓冲区的末尾。 返回值 解码缓冲区数据并使用指定的编码返回字符串。 123456789buf = Buffer.alloc(26);for (var i = 0 ; i &lt; 26 ; i++) &#123; buf[i] = i + 97;&#125;console.log( buf.toString('ascii')); // 输出: abcdefghijklmnopqrstuvwxyzconsole.log( buf.toString('ascii',0,5)); // 输出: abcdeconsole.log( buf.toString('utf8',0,5)); // 输出: abcdeconsole.log( buf.toString(undefined,0,5)); // 使用 'utf8' 编码, 并输出: abcde （4）将Buffer转换为JSON对象将 Node Buffer 转换为 JSON 对象的函数语法格式如下： 1buf.toJSON() 当字符串化一个 Buffer 实例时，JSON.stringify() 会隐式地调用该 toJSON()。 返回 JSON 对象。 123456789101112131415const buf = Buffer.from([0x1, 0x2, 0x3, 0x4, 0x5]);const json = JSON.stringify(buf)// 输出: &#123;"type":"Buffer","data":[1,2,3,4,5]&#125;console.log(json);// json.parse() 解析json字符串const copy = JSON.parse(json,(ksy,value)=&gt;&#123; return value &amp;&amp; value.type === 'Buffer'? Buffer.from(value.data): value;&#125;);// 输出: &lt;Buffer 01 02 03 04 05&gt;console.log(copy); （5）缓冲区合并 Node 缓冲区合并的语法如下所示： 1Buffer.concat(list[, totalLength]) 参数描述如下： list - 用于合并的 Buffer 对象数组列表。 totalLength - 指定合并后Buffer对象的总长度。 返回一个多个成员合并的新 Buffer 对象。 123456var buffer1 = Buffer.from(('菜鸟教程'));var buffer2 = Buffer.from(('www.runoob.com'));var buffer3 = Buffer.concat([buffer1,buffer2]);// buffer3 内容: 菜鸟教程 www.runoob.comconsole.log('buffer3内容:'+buffer3.toString()); （5）缓冲区比较Node Buffer 比较的函数语法如下所示, 该方法在 Node.js v0.12.2 版本引入： 1buf.compare(otherBuffer); 参数描述如下： otherBuffer - 与 buf 对象比较的另外一个 Buffer 对象。 返回一个数字，表示 buf 在 otherBuffer 之前，之后或相同。 123456789101112var buffer1 = Buffer.from('ABC');var buffer2 = Buffer.from('ABCD');var result = buffer1.compare(buffer2);// ABC在ABCD之前if(result &lt; 0)&#123; console.log(buffer1 + '在' + buffer2 + '之前');&#125;else if (result == 0)&#123; console.log(buffer1 + '与' + buffer2 + '相同');&#125;else&#123; console.log(buffer1 + '在' + buffer2 + '之后');&#125; （6）拷贝缓冲区Node 缓冲区拷贝语法如下所示： 1buf.copy(targetBuffer[, targetStart[, sourceStart[, sourceEnd]]]) 参数描述如下： targetBuffer - 要拷贝的 Buffer 对象。 targetStart - 数字, 可选, 默认: 0 sourceStart - 数字, 可选, 默认: 0 sourceEnd - 数字, 可选, 默认: buffer.length 没有返回值。 （7）缓冲区裁剪Node 缓冲区裁剪语法如下所示： 1buf.slice([start[, end]]) 参数描述如下： start - 数字, 可选, 默认: 0 end - 数字, 可选, 默认: buffer.length 返回一个新的缓冲区，它和旧缓冲区指向同一块内存，但是从索引 start 到 end 的位置剪切。 7、Node.js Stream(流)Stream 是一个抽象接口，Node 中有很多对象实现了这个接口。例如，对http 服务器发起请求的request 对象就是一个 Stream，还有stdout（标准输出）。 Node.js，Stream 有四种流类型： Readable - 可读操作。 Writable - 可写操作。 Duplex - 可读可写操作. Transform - 操作被写入数据，然后读出结果。 所有的 Stream 对象都是 EventEmitter 的实例。常用的事件有： data - 当有数据可读时触发。 end - 没有更多的数据可读时触发。 error - 在接收和写入过程中发生错误时触发。 finish - 所有数据已被写入到底层系统时触发。 （1）从流中读取数据12345678910111213141516171819202122232425var fs = require('fs');var data = '';// 创建可读流var readerStream = fs.createReadStream('a.txt');// 设置编码readerStream.setEncoding('UTF8');// 处理流事件 ---&gt; data ,end ,and errorreaderStream.on('data',function(chunk)&#123; data += chunk;&#125;);readerStream.on('end',function()&#123; console.log(data);&#125;);readerStream.on('error',function(err)&#123; console.log(err.stack);&#125;);console.log('程序执行完毕');//程序执行完毕//adasdsa:hello （2）写入流1234567891011121314151617181920212223242526var fs = require('fs');var data = '菜鸟教程官网地址：www.runoob.com';// 创建一个可以写入的流，写入到文件output.txt中var writeStream = fs.createWriteStream('output.txt');// 使用utf8 编码写入数据writeStream.write(data,'UTF8');// 标记文件末尾writeStream.end();// 处理流事件 ---&gt; data,end,and errorwriteStream.on('finish',function()&#123; console.log('写入完成。');&#125;);writeStream.on('error',function(err)&#123; console.log(err.stack);&#125;);console.log('程序执行完毕');// $ node main.js // 程序执行完毕// 写入完成。 （3）管道流管道提供了一个输出流到输入流的机制。通常我们用于从一个流中获取数据并将数据传递到另外一个流中。 12345678910111213var fs = require('fs');// 创建一个可读流var readerStream = fs.createReadStream('a.txt');// 创建一个可写流var writeStream = fs.createWriteStream('a-copy.txt');// 管道读写操作// 读取a.txt内容，并将内容写到ss.txt文件中readerStream.pipe(writeStream);console.log('程序执行完毕'); （4）链式流链式是通过连接输出流到另外一个流并创建多个流操作链的机制。链式流一般用于管道操作。 接下来我们就是用管道和链式来压缩和解压文件。 123456789101112131415161718192021var fs = require('fs');var zlib = require('zlib');// 压缩 a.txt 文件为 a.txt.gzfs.createReadStream('a.txt') .pipe(zlib.createGzip()) .pipe(fs.createWriteStream('a.txt.gz'));console.log('文件压缩完成');// ================================================var fs = require('fs');var zlib = require('zlib');// 解压 文件fs.createReadStream('a.txt.gz') .pipe(zlib.createGunzip()) .pipe(fs.createWriteStream('as.txt'));console.log('文件解压完成'); 8、函数在JavaScript中，一个函数可以作为另一个函数的参数。我们可以先定义一个函数，然后传递，也可以在传递参数的地方直接定义函数。 1234567891011// 创建函数function say(word)&#123; console.log(word);&#125;;// 执行，传入函数及其参数function execute(someFunction,value)&#123; someFunction(value);&#125;;execute(say,'Hello'); 以上代码中，我们把 say 函数作为execute函数的第一个变量进行了传递。这里传递的不是 say 的返回值，而是 say 本身！ 这样一来， say 就变成了execute 中的本地变量 someFunction ，execute可以通过调用 someFunction() （带括号的形式）来使用 say 函数。 当然，因为 say 有一个变量， execute 在调用 someFunction 时可以传递这样一个变量。 匿名函数我们可以把一个函数作为变量传递。但是我们不一定要绕这个”先定义，再传递”的圈子，我们可以直接在另一个函数的括号中定义和传递这个函数： 12345function execute(someFunction,value)&#123; someFunction(value);&#125;execute(function(word)&#123;console.log(word)&#125;,'Hello word'); 我们在 execute 接受第一个参数的地方直接定义了我们准备传递给 execute 的函数。 用这种方式，我们甚至不用给这个函数起名字，这也是为什么它被叫做匿名函数 。 9、Node.js路由我们要为路由提供请求的 URL 和其他需要的 GET 及 POST 参数，随后路由需要根据这些数据来执行相应的代码。 因此，我们需要查看 HTTP 请求，从中提取出请求的 URL 以及 GET/POST 参数。这一功能应当属于路由还是服务器（甚至作为一个模块自身的功能）确实值得探讨，但这里暂定其为我们的HTTP服务器的功能。 我们需要的所有数据都会包含在 request 对象中，该对象作为 onRequest() 回调函数的第一个参数传递。但是为了解析这些数据，我们需要额外的 Node.JS 模块，它们分别是 url 和 querystring 模块。 server.js 1234567891011121314151617181920var http = require('http');var url = require('url');function start(route)&#123; function onRequest(request,response)&#123; var pathname = url.parse(request.url).pathname; console.log('Request for' + pathname + 'received.'); route(pathname); // 调用route.js中的方法 response.writeHead(200,&#123;'Content-Type':'text/plain'&#125;); //响应码和响应头类型 response.write('Hello World');// 页面显示 response.end(); &#125; http.createServer(onRequest).listen(8888); // 绑定端口号 console.log('Server has started');&#125;exports.start = start; router.js 12345function route(pathname) &#123; console.log('About to route a request for ' + pathname)&#125;exports.route = route; index.js 1234var server = require("./server");var router = require('./router');server.start(router.route); 执行：node index.js 12345Server has startedRequest for/received.About to route a request for /Request for/favicon.icoreceived.About to route a request for /favicon.ico 10、全局对象JavaScript 中有一个特殊的对象，称为全局对象（Global Object），它及其所有属性都可以在程序的任何地方访问，即全局变量。 在浏览器 JavaScript 中，通常 window 是全局对象， 而 Node.js 中的全局对象是 global，所有全局变量（除了 global 本身以外）都是 global 对象的属性。 在 Node.js 我们可以直接访问到 global 的属性，而不需要在应用中包含它 全局对象和全局变量global 最根本的作用是作为全局变量的宿主。按照 ECMAScript 的定义，满足以下条 件的变量是全局变量： 在最外层定义的变量； 全局对象的属性； 隐式定义的变量（未定义直接赋值的变量）。 当你定义一个全局变量时，这个变量同时也会成为全局对象的属性，反之亦然。需要注 意的是，在 Node.js 中你不可能在最外层定义变量，因为所有用户代码都是属于当前模块的， 而模块本身不是最外层上下文。 注意： 永远使用 var 定义变量以避免引入全局变量，因为全局变量会污染 命名空间，提高代码的耦合风险。 （1）__filename__filename 表示当前正在执行的脚本的文件名。它将输出文件所在位置的绝对路径，且和命令行参数所指定的文件名不一定相同。 如果在模块中，返回的值是模块文件的路径。 12// 输出全局变量__filename的值console.log(__filename); // D:\node\variable01.js （2）__dirname__dirname 表示当前执行脚本所在的目录。 12// 输出全局变量__dirname的值console.log(__dirname); // D:\node （3）setTimeout(cb, ms)setTimeout(cb, ms) 全局函数在指定的毫秒(ms)数后执行指定函数(cb)。：setTimeout() 只执行一次指定函数。 返回一个代表定时器的句柄值。 12345function printHello()&#123; console.log('Hello World');&#125;//设置两秒之后执行以上函数setTimeout(printHello,2000); （4）clearTimeout(t)clearTimeout( t ) 全局函数用于停止一个之前通过 setTimeout() 创建的定时器。 参数 t 是通过 setTimeout() 函数创建的定时器。 12345678function printHello()&#123; console.log('Hello World');&#125;//设置两秒之后执行以上函数var t = setTimeout(printHello,2000);// 清除定时器clearTimeout(t); （5）setInterval(cb, ms)setInterval(cb, ms) 全局函数在指定的毫秒(ms)数后执行指定函数(cb)。 返回一个代表定时器的句柄值。可以使用 clearInterval(t) 函数来清除定时器。 setInterval() 方法会不停地调用函数，直到 clearInterval() 被调用或窗口被关闭。 12345function printHello()&#123; console.log('Hello,World!');&#125;// 两秒后执行以上函数setInterval(printHello,2000); // 每两秒执行一次上述函数，而且会永久执行下去 （6）consoleconsole 用于提供控制台标准输出。 以下为 console 对象的方法: 序号 方法 &amp; 描述 1 console.log([data][, …]) 向标准输出流打印字符并以换行符结束。该方法接收若干 个参数，如果只有一个参数，则输出这个参数的字符串形式。如果有多个参数，则 以类似于C 语言 printf() 命令的格式输出。 2 console.info([data][, …]) 该命令的作用是返回信息性消息，这个命令与console.log差别并不大，除了在chrome中只会输出文字外，其余的会显示一个蓝色的惊叹号。 3 console.error([data][, …]) 输出错误消息的。控制台在出现错误时会显示是红色的叉子。 4 console.warn([data][, …]) 输出警告消息。控制台出现有黄色的惊叹号。 5 console.dir(obj[, options]) 用来对一个对象进行检查（inspect），并以易于阅读和打印的格式显示。 6 console.time(label) 输出时间，表示计时开始。 7 console.timeEnd(label) 结束时间，表示计时结束。 8 console.trace(message[, …]) 当前执行的代码在堆栈中的调用路径，这个测试函数运行很有帮助，只要给想测试的函数里面加入 console.trace 就行了。 9 console.assert(value[, message][, …]) 用于判断某个表达式或变量是否为真，接收两个参数，第一个参数是表达式，第二个参数是字符串。只有当第一个参数为false，才会输出第二个参数，否则不会有任何结果。 console.log()：第一个参数是一个字符串，如果没有 参数，只打印一个换行。 12345678console.log('Hello Word');console.log('byvoid%diovyb');console.log('byvoid%diovyb',1991);/*Hello Wordbyvoid%diovybbyvoid1991iovyb*/ console.error()：与console.log() 用法相同，只是向标准错误流输出。 console.trace()：向标准错误流输出当前的调用栈。 123456789101112console.trace();/* Trace at Object.&lt;anonymous&gt; (D:\node\variable01.js:29:9) at Module._compile (module.js:456:26) at Object.Module._extensions..js (module.js:474:10) at Module.load (module.js:356:32) at Function.Module._load (module.js:312:12) at Function.Module.runMain (module.js:497:10) at startup (node.js:119:16) at node.js:902:3*/ 实例 1234567891011121314console.info('程序开始执行');var couter = 10;console.log('计划: %d',couter);console.time('获取数据');console.timeEnd('获取数据');console.info('程序执行完毕');/*程序开始执行计划: 10获取数据: 0ms程序执行完毕*/ （7）processprocess 是一个全局变量，即 global 对象的属性。 它用于描述当前Node.js 进程状态的对象，提供了一个与操作系统的简单接口。通常在你写本地命令行程序的时候，少不了要和它打交道。下面将会介绍 process 对象的一些最常用的成员方法。 序号 事件 &amp; 描述 1 exit 当进程准备退出时触发。 2 beforeExit 当 node 清空事件循环，并且没有其他安排时触发这个事件。通常来说，当没有进程安排时 node 退出，但是 ‘beforeExit’ 的监听器可以异步调用，这样 node 就会继续执行。 3 uncaughtException 当一个异常冒泡回到事件循环，触发这个事件。如果给异常添加了监视器，默认的操作（打印堆栈跟踪信息并退出）就不会发生。 4 Signal 事件 当进程接收到信号时就触发。信号列表详见标准的 POSIX 信号名，如 SIGINT、SIGUSR1 等。 实例 1234567891011121314// 进程退出的时候触发process.on('exit',function(code) &#123; // 以下代码永远不会执行 setTimeout(function() &#123; console.log('该代码不会执行'); &#125;,0); console.log('退出码：',code);&#125;);console.log('程序执行结束');/*程序执行结束退出码： 0 */ 退出状态码 状态码 名称 &amp; 描述 1 Uncaught Fatal Exception 有未捕获异常，并且没有被域或 uncaughtException 处理函数处理。 2 Unused 保留 3 Internal JavaScript Parse Error JavaScript的源码启动 Node 进程时引起解析错误。非常罕见，仅会在开发 Node 时才会有。 4 Internal JavaScript Evaluation Failure JavaScript 的源码启动 Node 进程，评估时返回函数失败。非常罕见，仅会在开发 Node 时才会有。 5 Fatal Error V8 里致命的不可恢复的错误。通常会打印到 stderr ，内容为： FATAL ERROR 6 Non-function Internal Exception Handler 未捕获异常，内部异常处理函数不知为何设置为on-function，并且不能被调用。 7 Internal Exception Handler Run-Time Failure 未捕获的异常， 并且异常处理函数处理时自己抛出了异常。例如，如果 process.on(‘uncaughtException’) 或 domain.on(‘error’) 抛出了异常。 8 Unused 保留 9 Invalid Argument 可能是给了未知的参数，或者给的参数没有值。 10 Internal JavaScript Run-Time Failure JavaScript的源码启动 Node 进程时抛出错误，非常罕见，仅会在开发 Node 时才会有。 12 Invalid Debug Argument 设置了参数—debug 和/或 —debug-brk，但是选择了错误端口。 128 Signal Exits 如果 Node 接收到致命信号，比如SIGKILL 或 SIGHUP，那么退出代码就是128 加信号代码。这是标准的 Unix 做法，退出信号代码放在高位。 Process 属性Process 提供了很多有用的属性，便于我们更好的控制系统的交互： 序号. 属性 &amp; 描述 1 stdout 标准输出流。 2 stderr 标准错误流。 3 stdin 标准输入流。 4 argv argv 属性返回一个数组，由命令行执行脚本时的各个参数组成。它的第一个成员总是node，第二个成员是脚本文件名，其余成员是脚本文件的参数。 5 execPath 返回执行当前脚本的 Node 二进制文件的绝对路径。 6 execArgv 返回一个数组，成员是命令行下执行脚本时，在Node可执行文件与脚本文件之间的命令行参数。 7 env 返回一个对象，成员为当前 shell 的环境变量 8 exitCode 进程退出时的代码，如果进程优通过 process.exit() 退出，不需要指定退出码。 9 version Node 的版本，比如v0.10.18。 10 versions 一个属性，包含了 node 的版本和依赖. 11 config 一个包含用来编译当前 node 执行文件的 javascript 配置选项的对象。它与运行 ./configure 脚本生成的 “config.gypi” 文件相同。 12 pid 当前进程的进程号。 13 title 进程名，默认值为”node”，可以自定义该值。 14 arch 当前 CPU 的架构：’arm’、’ia32’ 或者 ‘x64’。 15 platform 运行程序所在的平台系统 ‘darwin’, ‘freebsd’, ‘linux’, ‘sunos’ 或 ‘win32’ 16 mainModule require.main 的备选方法。不同点，如果主模块在运行时改变，require.main可能会继续返回老的模块。可以认为，这两者引用了同一个模块。 实例 1234567891011121314151617181920// 输出到终端process.stdout.write('Hello World ' + '\n');// 通过参数读取process.argv.forEach(function(val,index,array) &#123; console.log(index + ': ' + val);&#125;);// 获取执行路径console.log(process.execPath);// 平台信息console.log(process.platform);/*Hello World0: node1: D:\node\variable01.jsD:\node\node.exewin32 */ 方法参考手册Process 提供了很多有用的方法，便于我们更好的控制系统的交互： 序号 方法 &amp; 描述 1 abort() 这将导致 node 触发 abort 事件。会让 node 退出并生成一个核心文件。 2 chdir(directory) 改变当前工作进程的目录，如果操作失败抛出异常。 3 cwd() 返回当前进程的工作目录 4 exit([code]) 使用指定的 code 结束进程。如果忽略，将会使用 code 0。 5 getgid() 获取进程的群组标识（参见 getgid(2)）。获取到得时群组的数字 id，而不是名字。 注意：这个函数仅在 POSIX 平台上可用(例如，非Windows 和 Android)。 6 setgid(id) 设置进程的群组标识（参见 setgid(2)）。可以接收数字 ID 或者群组名。如果指定了群组名，会阻塞等待解析为数字 ID 。 注意：这个函数仅在 POSIX 平台上可用(例如，非Windows 和 Android)。 7 getuid() 获取进程的用户标识(参见 getuid(2))。这是数字的用户 id，不是用户名。 注意：这个函数仅在 POSIX 平台上可用(例如，非Windows 和 Android)。 8 setuid(id) 设置进程的用户标识（参见setuid(2)）。接收数字 ID或字符串名字。果指定了群组名，会阻塞等待解析为数字 ID 。 注意：这个函数仅在 POSIX 平台上可用(例如，非Windows 和 Android)。 9 getgroups() 返回进程的群组 iD 数组。POSIX 系统没有保证一定有，但是 node.js 保证有。 注意：这个函数仅在 POSIX 平台上可用(例如，非Windows 和 Android)。 10 setgroups(groups) 设置进程的群组 ID。这是授权操作，所以你需要有 root 权限，或者有 CAP_SETGID 能力。 注意：这个函数仅在 POSIX 平台上可用(例如，非Windows 和 Android)。 11 initgroups(user, extra_group) 读取 /etc/group ，并初始化群组访问列表，使用成员所在的所有群组。这是授权操作，所以你需要有 root 权限，或者有 CAP_SETGID 能力。 注意：这个函数仅在 POSIX 平台上可用(例如，非Windows 和 Android)。 12 kill(pid[, signal]) 发送信号给进程. pid 是进程id，并且 signal 是发送的信号的字符串描述。信号名是字符串，比如 ‘SIGINT’ 或 ‘SIGHUP’。如果忽略，信号会是 ‘SIGTERM’。 13 memoryUsage() 返回一个对象，描述了 Node 进程所用的内存状况，单位为字节。 14 nextTick(callback) 一旦当前事件循环结束，调用回调函数。 15 umask([mask]) 设置或读取进程文件的掩码。子进程从父进程继承掩码。如果mask 参数有效，返回旧的掩码。否则，返回当前掩码。 16 uptime() 返回 Node 已经运行的秒数。 17 hrtime() 返回当前进程的高分辨时间，形式为 [seconds, nanoseconds]数组。它是相对于过去的任意事件。该值与日期无关，因此不受时钟漂移的影响。主要用途是可以通过精确的时间间隔，来衡量程序的性能。 你可以将之前的结果传递给当前的 process.hrtime() ，会返回两者间的时间差，用来基准和测量时间间隔。 实例 12345678910111213// 输出当前目录console.log('当前目录： ' + process.cwd());// 输出当前版本console.log('当前版本：' + process.version);// 输出内存使用情况console.log(process.memoryUsage());/*当前目录： D:\node当前版本：v0.10.26&#123; rss: 13099008, heapTotal: 4083456, heapUsed: 2183176 &#125;*/ 11、常用工具util 是一个Node.js 核心模块，提供常用函数的集合，用于弥补核心JavaScript 的功能 过于精简的不足。 （1）util.inheritsutil.inherits(constructor, superConstructor)是一个实现对象间原型继承 的函数。 JavaScript 的面向对象特性是基于原型的，与常见的基于类的不同。JavaScript 没有 提供对象继承的语言级别特性，而是通过原型复制来实现的。 123456789101112131415161718192021222324252627282930var util = require('util');function Base()&#123; this.name = 'base'; this.base = 1991; this.sayHello = function() &#123; console.log('Hello ' + this.name); &#125;;&#125;Base.prototype.showName = function() &#123; console.log(this.name);&#125;function Sub() &#123; this.name = 'sub';&#125;util.inherits(Sub, Base);var objBase = new Base();objBase.showName();objBase.sayHello();console.log(objBase);var objSub = new Sub();objSub.showName();// objSub.sayHello();console.log(objSub);/*baseHello base&#123; name: 'base', base: 1991, sayHello: [Function] &#125;sub&#123; name: 'sub' &#125; */ 我们定义了一个基础对象Base 和一个继承自Base的Sub，Base有三个在构造函数内定义的属性和一个原型中定义的函数，通过util.inherits 实现继承。 注意：Sub 仅仅继承了Base 在原型中定义的函数，而构造函数内部创造的 base 属 性和 sayHello 函数都没有被 Sub 继承。 同时，在原型中定义的属性不会被console.log 作 为对象的属性输出。如果我们去掉 objSub.sayHello(); 这行的注释，将会看到： 1234567891011121314151617baseHello base&#123; name: 'base', base: 1991, sayHello: [Function] &#125;subD:\node\util.js:22objSub.sayHello(); ^TypeError: Object #&lt;Sub&gt; has no method 'sayHello' at Object.&lt;anonymous&gt; (D:\node\util.js:22:8) at Module._compile (module.js:456:26) at Object.Module._extensions..js (module.js:474:10) at Module.load (module.js:356:32) at Function.Module._load (module.js:312:12) at Function.Module.runMain (module.js:497:10) at startup (node.js:119:16) at node.js:902:3 （2）util.inspectutil.inspect(object,[showHidden],[depth],[colors])是一个将任意对象转换为字符串的方法，通常用于调试和错误输出。它至少接受一个参数 object，即要转换的对象。 showHidden 是一个可选参数，如果值为 true，将会输出更多隐藏信息。 depth 表示最大递归的层数，如果对象很复杂，你可以指定层数以控制输出信息的多 少。如果不指定depth，默认会递归2层，指定为 null 表示将不限递归层数完整遍历对象。 如果color 值为 true，输出格式将会以ANSI 颜色编码，通常用于在终端显示更漂亮 的效果。 特别要指出的是，util.inspect 并不会简单地直接把对象转换为字符串，即使该对 象定义了toString 方法也不会调用。 123456789101112131415161718192021var util = require('util');function Person() &#123; this.name = 'zs'; this.toString = function() &#123; return this.name; &#125;;&#125;var obj = new Person();console.log(util.inspect(obj));console.log(util.inspect(obj, true));/*&#123; name: 'zs', toString: [Function] &#125;&#123; name: 'zs', toString: &#123; [Function] [length]: 0, [name]: '', [arguments]: null, [caller]: null, [prototype]: &#123; [constructor]: [Circular] &#125; &#125; &#125; */ （3）util.isArray(object)如果给定的参数 “object” 是一个数组返回true，否则返回false。 12345678var util = require('util');util.isArray([])// trueutil.isArray(new Array)// trueutil.isArray(&#123;&#125;)// false （4）util.isRegExp(object)如果给定的参数 “object” 是一个正则表达式返回true，否则返回false。 1234567var util = require('util');console.log(util.isRegExp(/some regexp/));//trueconsole.log(util.isRegExp(new RegExp('another regexp')));//trueconsole.log(util.isRegExp(&#123;&#125;));//false （5）util.isDate(object)如果给定的参数 “object” 是一个日期返回true，否则返回false。 1234567var util = require('util');console.log(util.isDate(new Date()));//trueconsole.log(util.isDate(Date()));//false 前面没有new方法的话，返回的是字符串console.log(util.isDate(&#123;&#125;));//false （6）util.isError(object)如果给定的参数“object”是一个错误对象返回true，否则返回false 1234567var util = require('util');console.log(util.isError(new Error()));// trueconsole.log(util.isError(new TypeError()));// trueconsole.log(util.isError(&#123;name:'Error',message:'an error occurred'&#125;))// false 12、文件系统Node.js 提供一组类似 UNIX（POSIX）标准的文件操作API。 语法如下 1var fs = require('fs); （1）异步和同步Node.js 文件系统（fs 模块）模块中的方法均有异步和同步版本，例如读取文件内容的函数有异步的 fs.readFile() 和同步的 fs.readFileSync()。 异步的方法函数最后一个参数为回调函数，回调函数的第一个参数包含了错误信息(error)。 建议大家使用异步方法，比起同步，异步方法性能更高，速度更快，而且没有阻塞。 123456789101112131415161718192021222324252627282930313233343536373839var fs = require('fs');// 异步读取fs.readFile('input.txt',function(err,data) &#123; if(err) &#123; return console.error(err); &#125; console.log('异步读取：'+ data.toString());&#125;);// 同步读取var data = fs.readFileSync('input.txt');console.log('同步读取：' + data.toString());console.log('程序执行完毕');/*同步读取：菜鸟教程官网地址：www.runoob.com文件读取实例程序执行完毕异步读取：菜鸟教程官网地址：www.runoob.com文件读取实例 */// 异步，同样还有同步版本function getAllAlbums(callback)&#123; fs.readdir('./uploads',function(err,files)&#123; // err: 在有错误发生时等于异常对象， // files:始终用于放回API方法的执行结果. var allAlbums = []; callback(allAlbums); &#125;);&#125;;var albums = function(req,res,next)&#123; getAllAlbums(function(err,allAlbums)&#123; res.render('index',&#123; 'albums':allAlbums &#125;); &#125;);&#125;; （2）打开文件语法以下为在异步模式下打开文件的语法格式： 1fs.open(path, flags[, mode], callback) 参数参数使用说明如下： path - 文件的路径。 flags - 文件打开的行为。具体值详见下文。 mode - 设置文件模式(权限)，文件创建默认权限为 0666(可读，可写)。 callback - 回调函数，带有两个参数如：callback(err, fd)。 flags 参数可以是以下值： Flag 描述 r 以读取模式打开文件。如果文件不存在抛出异常。 r+ 以读写模式打开文件。如果文件不存在抛出异常。 rs 以同步的方式读取文件。 rs+ 以同步的方式读取和写入文件。 w 以写入模式打开文件，如果文件不存在则创建。 wx 类似 ‘w’，但是如果文件路径存在，则文件写入失败。 w+ 以读写模式打开文件，如果文件不存在则创建。 wx+ 类似 ‘w+’， 但是如果文件路径存在，则文件读写失败。 a 以追加模式打开文件，如果文件不存在则创建。 ax 类似 ‘a’， 但是如果文件路径存在，则文件追加失败。 a+ 以读取追加模式打开文件，如果文件不存在则创建。 ax+ 类似 ‘a+’， 但是如果文件路径存在，则文件读取追加失败。 实例1234567891011121314var fs = require('fs');// 异步打开文件console.log('准备打开文件');fs.open('input.txt','r+',function(err,fd) &#123; if (err) &#123; return console.error(err); &#125; console.log('文件打开成功');&#125;);/*准备打开文件文件打开成功 */ （3）获取文件信息语法以下为通过异步模式获取文件信息的语法格式： 1fs.stat(path, callback) 参数参数使用说明如下： path - 文件路径。 callback - 回调函数，带有两个参数如：(err, stats), stats 是 fs.Stats 对象。 fs.stat(path)执行后，会将stats类的实例返回给其回调函数。可以通过stats类中的提供方法判断文件的相关属性。例如判断是否为文件： 12345var fs = require('fs');fs.stat('./file.js',function (err,stats)&#123; console.log(stats.isFile()); // true&#125;) stats类中的方法有： 方法 描述 stats.isFile() 如果是文件返回 true，否则返回 false。 stats.isDirectory() 如果是目录返回 true，否则返回 false。 stats.isBlockDevice() 如果是块设备返回 true，否则返回 false。 stats.isCharacterDevice() 如果是字符设备返回 true，否则返回 false。 stats.isSymbolicLink() 如果是软链接返回 true，否则返回 false。 stats.isFIFO() 如果是FIFO，返回true，否则返回 false。FIFO是UNIX中的一种特殊类型的命令管道。 stats.isSocket() 如果是 Socket 返回 true，否则返回 false。 实例12345678910111213141516171819202122232425262728293031var fs = require('fs');console.log('准备打开文件');fs.stat('input.txt',function (err,stats)&#123; if(err)&#123; return console.error(err); &#125; console.log(stats); console.log('读取文件信息成功'); // 检测文件类型 console.log('是否是文件（isFile）?' + stats.isFile()); console.log('是否为目录（isDirectory）?' + stats.isDirectory());&#125;);/*准备打开文件&#123; dev: 0, mode: 33206, nlink: 1, uid: 0, gid: 0, rdev: 0, ino: 0, size: 64, atime: Tue Sep 11 2018 15:15:05 GMT+0800 (中国标准时间), mtime: Tue Sep 11 2018 15:19:29 GMT+0800 (中国标准时间), ctime: Tue Sep 11 2018 15:15:05 GMT+0800 (中国标准时间) &#125;读取文件信息成功是否是文件（isFile）?true是否为目录（isDirectory）?false */ （4）写入文件语法以下为异步模式下写入文件的语法格式： 1fs.writeFile(file, data[, options], callback) writeFile 直接打开文件默认是 w 模式，所以如果文件存在，该方法写入的内容会覆盖旧的文件内容。 参数参数使用说明如下： file - 文件名或文件描述符。 data - 要写入文件的数据，可以是 String(字符串) 或 Buffer(缓冲) 对象。 options - 该参数是一个对象，包含 {encoding, mode, flag}。默认编码为 utf8, 模式为 0666 ， flag 为 ‘w’ callback - 回调函数，回调函数只包含错误信息参数(err)，在写入失败时返回。 实例接下来我们创建 file.js 文件，代码如下所示： 1234567891011121314151617181920212223var fs = require('fs');console.log('准备写入文件');fs.writeFile('input.txt','我是通过fs.writeFile写入的文件内容',function(err)&#123; if (err) &#123; return console.error(err); &#125; console.log('数据写入成功！'); console.log('--------------------分割线--------------------'); console.log('读取写入的数据！'); fs.readFile('input.txt',function(err,data)&#123; if(err)&#123; return console.error(err); &#125; console.log('异步读取文件数据：' + data.toString()) &#125;);&#125;);/*准备写入文件数据写入成功！--------------------分割线--------------------读取写入的数据！异步读取文件数据：我是通过fs.writeFile写入的文件内容*/ （5）读取文件语法以下为异步模式下读取文件的语法格式： 1fs.read(fd, buffer, offset, length, position, callback) 该方法使用了文件描述符来读取文件。 参数参数使用说明如下： fd - 通过 fs.open() 方法返回的文件描述符。 buffer - 数据写入的缓冲区。 offset - 缓冲区写入的写入偏移量。 length - 要从文件中读取的字节数。 position - 文件读取的起始位置，如果 position 的值为 null，则会从当前文件指针的位置读取。 callback - 回调函数，有三个参数err, bytesRead, buffer，err 为错误信息， bytesRead 表示读取的字节数，buffer 为缓冲区对象。 实例1234567891011121314151617181920212223242526272829var fs = require("fs");var buf = new Buffer.alloc(1024);console.log("准备打开已存在的文件！");fs.open('input.txt', 'r+', function(err, fd) &#123; if (err) &#123; return console.error(err); &#125; console.log("文件打开成功！"); console.log("准备读取文件："); fs.read(fd, buf, 0, buf.length, 0, function(err, bytes)&#123; if (err)&#123; console.log(err); &#125; console.log(bytes + " 字节被读取"); // 仅输出读取的字节 if(bytes &gt; 0)&#123; console.log(buf.slice(0, bytes).toString()); &#125; &#125;);&#125;);/*准备打开已存在的文件！文件打开成功！准备读取文件：42 字节被读取菜鸟教程官网地址：www.runoob.com*/ （6）关闭文件语法以下为异步模式下关闭文件的语法格式： 1fs.close(fd, callback) 该方法使用了文件描述符来读取文件。 参数参数使用说明如下： fd - 通过 fs.open() 方法返回的文件描述符。 callback - 回调函数，没有参数。 实例input.txt 文件内容为： 1菜鸟教程官网地址：www.runoob.com 接下来我们创建 file.js 文件，代码如下所示： 123456789101112131415161718192021222324252627282930313233343536var fs = require("fs");var buf = new Buffer.alloc(1024);console.log("准备打开文件！");fs.open('input.txt', 'r+', function(err, fd) &#123; if (err) &#123; return console.error(err); &#125; console.log("文件打开成功！"); console.log("准备读取文件！"); fs.read(fd, buf, 0, buf.length, 0, function(err, bytes)&#123; if (err)&#123; console.log(err); &#125; // 仅输出读取的字节 if(bytes &gt; 0)&#123; console.log(buf.slice(0, bytes).toString()); &#125; // 关闭文件 fs.close(fd, function(err)&#123; if (err)&#123; console.log(err); &#125; console.log("文件关闭成功"); &#125;); &#125;);&#125;);/*准备打开文件！文件打开成功！准备读取文件！菜鸟教程官网地址：www.runoob.com文件关闭成功*/ （7）截取文件语法以下为异步模式下截取文件的语法格式： 1fs.ftruncate(fd, len, callback) 该方法使用了文件描述符来读取文件。 参数参数使用说明如下： fd - 通过 fs.open() 方法返回的文件描述符。 len - 文件内容截取的长度。 callback - 回调函数，没有参数。 实例input.txt 文件内容为： 1site:www.runoob.com 接下来我们创建 file.js 文件，代码如下所示： 1234567891011121314151617181920212223242526272829303132333435363738var fs = require("fs");var buf = new Buffer.alloc(1024);console.log("准备打开文件！");fs.open('input.txt', 'r+', function(err, fd) &#123; if (err) &#123; return console.error(err); &#125; console.log("文件打开成功！"); console.log("截取了10字节后的文件内容。"); // 截取文件 fs.ftruncate(fd, 10, function(err)&#123; if (err)&#123; console.log(err); &#125; console.log("文件截取成功。"); console.log("读取相同的文件"); fs.read(fd, buf, 0, buf.length, 0, function(err, bytes)&#123; if (err)&#123; console.log(err); &#125; // 仅输出读取的字节 if(bytes &gt; 0)&#123; console.log(buf.slice(0, bytes).toString()); &#125; // 关闭文件 fs.close(fd, function(err)&#123; if (err)&#123; console.log(err); &#125; console.log("文件关闭成功！"); &#125;); &#125;); &#125;);&#125;); （8）删除文件语法以下为删除文件的语法格式： 1fs.unlink(path, callback) 参数参数使用说明如下： path - 文件路径。 callback - 回调函数，没有参数。 实例input.txt 文件内容为： 1site:www.runoob.com 接下来我们创建 file.js 文件，代码如下所示： 123456789101112var fs = require('fs');console.log('准备删除文件！');fs.unlink('input.txt',function(err) &#123; if (err) &#123; return console.error(err); &#125; console.log('文件删除成功');&#125;)/*准备删除文件！文件删除成功*/ （9）路径 path将传入的路径转换为标准路径，具体讲的话，除了解析路径中的.与..外，还能去掉多余的斜杠。如果有程序需要使用路径作为某些数据的索引，但又允许用户随意输入路径时，就需要使用该方法保证路径的唯一性。 1234567891011const path = require('path'); var cache = &#123;&#125;;function store(key,value)&#123; cache[path.normalize(key)] = value;&#125;store('foo/bar',1);store('foo//baz//../bar',2);console.log(cache);// &#123; 'foo\\bar': 2 &#125; 创建目录语法以下为创建目录的语法格式： 1fs.mkdir(path[, mode], callback) 参数参数使用说明如下： path - 文件路径。 mode - 设置目录权限，默认为 0777。 callback - 回调函数，没有参数。 实例接下来我们创建 file.js 文件，代码如下所示： 123456789var fs = require("fs");console.log("创建目录 /tmp/test/");fs.mkdir("/tmp/test/",function(err)&#123; if (err) &#123; return console.error(err); &#125; console.log("目录创建成功。");&#125;); 以上代码执行结果如下： 123$ node file.js 创建目录 /tmp/test/目录创建成功。 读取目录语法以下为读取目录的语法格式： 1fs.readdir(path, callback) 参数参数使用说明如下： path - 文件路径。 callback - 回调函数，回调函数带有两个参数err, files，err 为错误信息，files 为 目录下的文件数组列表。 实例接下来我们创建 file.js 文件，代码如下所示： 1234567891011var fs = require(&quot;fs&quot;);console.log(&quot;查看 /tmp 目录&quot;);fs.readdir(&quot;/tmp/&quot;,function(err, files)&#123; if (err) &#123; return console.error(err); &#125; files.forEach( function (file)&#123; console.log( file ); &#125;);&#125;); 以上代码执行结果如下： 123456$ node file.js 查看 /tmp 目录input.outoutput.outtesttest.txt 删除目录语法以下为删除目录的语法格式： 1fs.rmdir(path, callback) 参数参数使用说明如下： path - 文件路径。 callback - 回调函数，没有参数。 实例接下来我们创建 file.js 文件，代码如下所示： 1234567891011121314151617var fs = require("fs");// 执行前创建一个空的 /tmp/test 目录console.log("准备删除目录 /tmp/test");fs.rmdir("/tmp/test",function(err)&#123; if (err) &#123; return console.error(err); &#125; console.log("读取 /tmp 目录"); fs.readdir("/tmp/",function(err, files)&#123; if (err) &#123; return console.error(err); &#125; files.forEach( function (file)&#123; console.log( file ); &#125;); &#125;);&#125;); 以上代码执行结果如下： 1234$ node file.js 准备删除目录 /tmp/test读取 /tmp 目录…… 遍历目录递归算法123456789function factorial(n)&#123; if (n === 1)&#123; return 1; &#125;else&#123; return n * factorial(n-1) &#125;&#125;a = factorial(10);console.log(a); 使用递归算法编写的代码虽然简洁，但由于每递归一次就产生一次函数调用，在需要优先考虑性能时，需要把递归算法转换为循环算法，以减少函数调用次数。 遍历算法目录是一个树状结构，在遍历时一般使用深度优先+先序遍历算法。深度优先，意味着到达一个节点后，首先接着遍历子节点而不是邻居节点。先序遍历，意味着首次到达了某节点就算遍历完成，而不是最后一次返回某节点才算数。因此使用这种遍历方式时，下边这棵树的遍历顺序是A &gt; B &gt; D &gt; E &gt; C &gt; F。 12345 A / \ B C / \ \D E F 同步遍历了解了必要的算法后，我们可以简单地实现以下目录遍历函数。 1234567891011function travel(dir, callback) &#123; fs.readdirSync(dir).forEach(function (file) &#123; var pathname = path.join(dir, file); if (fs.statSync(pathname).isDirectory()) &#123; travel(pathname, callback); &#125; else &#123; callback(pathname); &#125; &#125;);&#125; 可以看到，该函数以某个目录作为遍历的起点。遇到一个子目录时，就先接着遍历子目录。遇到一个文件时，就把文件的绝对路径传给回调函数。回调函数拿到文件路径后，就可以做各种判断和处理。因此假设有以下目录： 123456- /home/user/ - foo/ x.js - bar/ y.js z.css 使用以下代码遍历该目录时，得到的输入如下。 12345678travel(&apos;/home/user&apos;, function (pathname) &#123; console.log(pathname);&#125;);------------------------/home/user/foo/x.js/home/user/bar/y.js/home/user/z.css 异步遍历123456789101112131415161718function travel(dir,callback,finish)&#123; fs.readdir(dir,function(err,files)&#123; (function next(i) &#123; if (i&lt;files.length)&#123; var pathname = path.join(dir,files[i]); fs.stat(pathname,function(err,stats)&#123; if(stats.isDirectory())&#123; travel(pathname,callback,function()&#123; next(i+1)'' &#125;); &#125;else&#123; callback(pathname); &#125;; &#125;); &#125;; &#125;); &#125;);&#125;; 这里不详细介绍异步遍历函数的编写技巧，在后续章节中会详细介绍这个。总之我们可以看到异步编程还是蛮复杂的。 （10）文件模块方法参考手册以下为 Node.js 文件模块相同的方法列表： 序号 方法 &amp; 描述 1 fs.rename(oldPath, newPath, callback) 异步 rename().回调函数没有参数，但可能抛出异常。 2 fs.ftruncate(fd, len, callback) 异步 ftruncate().回调函数没有参数，但可能抛出异常。 3 fs.ftruncateSync(fd, len) 同步 ftruncate() 4 fs.truncate(path, len, callback) 异步 truncate().回调函数没有参数，但可能抛出异常。 5 fs.truncateSync(path, len) 同步 truncate() 6 fs.chown(path, uid, gid, callback) 异步 chown().回调函数没有参数，但可能抛出异常。 7 fs.chownSync(path, uid, gid) 同步 chown() 8 fs.fchown(fd, uid, gid, callback) 异步 fchown().回调函数没有参数，但可能抛出异常。 9 fs.fchownSync(fd, uid, gid) 同步 fchown() 10 fs.lchown(path, uid, gid, callback) 异步 lchown().回调函数没有参数，但可能抛出异常。 11 fs.lchownSync(path, uid, gid) 同步 lchown() 12 fs.chmod(path, mode, callback) 异步 chmod().回调函数没有参数，但可能抛出异常。 13 fs.chmodSync(path, mode) 同步 chmod(). 14 fs.fchmod(fd, mode, callback) 异步 fchmod().回调函数没有参数，但可能抛出异常。 15 fs.fchmodSync(fd, mode) 同步 fchmod(). 16 fs.lchmod(path, mode, callback) 异步 lchmod().回调函数没有参数，但可能抛出异常。Only available on Mac OS X. 17 fs.lchmodSync(path, mode) 同步 lchmod(). 18 fs.stat(path, callback) 异步 stat(). 回调函数有两个参数 err, stats，stats 是 fs.Stats 对象。 19 fs.lstat(path, callback) 异步 lstat(). 回调函数有两个参数 err, stats，stats 是 fs.Stats 对象。 20 fs.fstat(fd, callback) 异步 fstat(). 回调函数有两个参数 err, stats，stats 是 fs.Stats 对象。 21 fs.statSync(path) 同步 stat(). 返回 fs.Stats 的实例。 22 fs.lstatSync(path) 同步 lstat(). 返回 fs.Stats 的实例。 23 fs.fstatSync(fd) 同步 fstat(). 返回 fs.Stats 的实例。 24 fs.link(srcpath, dstpath, callback) 异步 link().回调函数没有参数，但可能抛出异常。 25 fs.linkSync(srcpath, dstpath) 同步 link(). 26 fs.symlink(srcpath, dstpath[, type], callback) 异步 symlink().回调函数没有参数，但可能抛出异常。 type 参数可以设置为 ‘dir’, ‘file’, 或 ‘junction’ (默认为 ‘file’) 。 27 fs.symlinkSync(srcpath, dstpath[, type]) 同步 symlink(). 28 fs.readlink(path, callback) 异步 readlink(). 回调函数有两个参数 err, linkString。 29 fs.realpath(path[, cache], callback) 异步 realpath(). 回调函数有两个参数 err, resolvedPath。 30 fs.realpathSync(path[, cache]) 同步 realpath()。返回绝对路径。 31 fs.unlink(path, callback) 异步 unlink().回调函数没有参数，但可能抛出异常。 32 fs.unlinkSync(path) 同步 unlink(). 33 fs.rmdir(path, callback) 异步 rmdir().回调函数没有参数，但可能抛出异常。 34 fs.rmdirSync(path) 同步 rmdir(). 35 fs.mkdir(path[, mode], callback) S异步 mkdir(2).回调函数没有参数，但可能抛出异常。 mode defaults to 0777. 36 fs.mkdirSync(path[, mode]) 同步 mkdir(). 37 fs.readdir(path, callback) 异步 readdir(3). 读取目录的内容。 38 fs.readdirSync(path) 同步 readdir().返回文件数组列表。 39 fs.close(fd, callback) 异步 close().回调函数没有参数，但可能抛出异常。 40 fs.closeSync(fd) 同步 close(). 41 fs.open(path, flags[, mode], callback) 异步打开文件。 42 fs.openSync(path, flags[, mode]) 同步 version of fs.open(). 43 fs.utimes(path, atime, mtime, callback) 44 fs.utimesSync(path, atime, mtime) 修改文件时间戳，文件通过指定的文件路径。 45 fs.futimes(fd, atime, mtime, callback) 46 fs.futimesSync(fd, atime, mtime) 修改文件时间戳，通过文件描述符指定。 47 fs.fsync(fd, callback) 异步 fsync.回调函数没有参数，但可能抛出异常。 48 fs.fsyncSync(fd) 同步 fsync. 49 fs.write(fd, buffer, offset, length[, position], callback) 将缓冲区内容写入到通过文件描述符指定的文件。 50 fs.write(fd, data[, position[, encoding]], callback) 通过文件描述符 fd 写入文件内容。 51 fs.writeSync(fd, buffer, offset, length[, position]) 同步版的 fs.write()。 52 fs.writeSync(fd, data[, position[, encoding]]) 同步版的 fs.write(). 53 fs.read(fd, buffer, offset, length, position, callback) 通过文件描述符 fd 读取文件内容。 54 fs.readSync(fd, buffer, offset, length, position) 同步版的 fs.read. 55 fs.readFile(filename[, options], callback) 异步读取文件内容。 56 fs.readFileSync(filename[, options]) 57 fs.writeFile(filename, data[, options], callback) 异步写入文件内容。 58 fs.writeFileSync(filename, data[, options]) 同步版的 fs.writeFile。 59 fs.appendFile(filename, data[, options], callback) 异步追加文件内容。 60 fs.appendFileSync(filename, data[, options]) The 同步 version of fs.appendFile. 61 fs.watchFile(filename[, options], listener) 查看文件的修改。 62 fs.unwatchFile(filename[, listener]) 停止查看 filename 的修改。 63 fs.watch(filename[, options][, listener]) 查看 filename 的修改，filename 可以是文件或目录。返回 fs.FSWatcher 对象。 64 fs.exists(path, callback) 检测给定的路径是否存在。 65 fs.existsSync(path) 同步版的 fs.exists. 66 fs.access(path[, mode], callback) 测试指定路径用户权限。 67 fs.accessSync(path[, mode]) 同步版的 fs.access。 68 fs.createReadStream(path[, options]) 返回ReadStream 对象。 69 fs.createWriteStream(path[, options]) 返回 WriteStream 对象。 70 fs.symlink(srcpath, dstpath[, type], callback) 异步 symlink().回调函数没有参数，但可能抛出异常。 13、GET/POST请求（1）获取GET请求内容由于GET请求直接被嵌入在路径中，URL是完整的请求路径，包括了?后面的部分，因此你可以手动解析后面的内容作为GET请求的参数。 node.js 中 url 模块中的 parse 函数提供了这个功能。 实例12345678var http = require('http');var url = require('url');var util = require('util');http.createServer(function(req,res)&#123; res.writeHead(200,&#123;'Content-Type':'text/plain;charset=utf-8'&#125;); // 配置请求头 res.end(util.inspect(url.parse(req.url,true)));&#125;).listen(3000) // 配置监听端口号 在浏览器中访问 http://localhost:3000/user?name=菜鸟教程&amp;url=www.runoob.com 然后查看返回结果: 123456789101112&#123; protocol: null, slashes: null, auth: null, host: null, port: null, hostname: null, hash: null, search: &apos;?name=%E8%8F%9C%E9%B8%9F%E6%95%99%E7%A8%8B&amp;url=www.runoob.com&apos;, query: &#123; name: &apos;菜鸟教程&apos;, url: &apos;www.runoob.com&apos; &#125;, pathname: &apos;/user&apos;, path: &apos;/user?name=%E8%8F%9C%E9%B8%9F%E6%95%99%E7%A8%8B&amp;url=www.runoob.com&apos;, href: &apos;/user?name=%E8%8F%9C%E9%B8%9F%E6%95%99%E7%A8%8B&amp;url=www.runoob.com&apos; &#125; 获取URL的参数我们可以使用 url.parse 方法来解析 URL 中的参数，代码如下： 1234567891011121314var http = require('http');var url = require('url');var util = require('util');http.createServer(function(req,res)&#123; res.writeHead(200,&#123;'Content-Type':'text/plain;charset=utf-8'&#125;); // 配置请求头 //解析url参数 var params = url.parse(req.url,true).query; res.write('网站名：' + params.name); res.write('\n'); res.write('网站URL：' + params.url); res.end()&#125;).listen(3000) // 配置监听端口号 在浏览器中访问 http://localhost:3000/user?name=菜鸟教程&amp;url=www.runoob.com 然后查看返回结果: 12网站名：菜鸟教程网站URL：www.runoob.com （2）获取POST请求内容POST 请求的内容全部的都在请求体中，http.ServerRequest 并没有一个属性内容为请求体，原因是等待请求体传输可能是一件耗时的工作。 比如上传文件，而很多时候我们可能并不需要理会请求体的内容，恶意的POST请求会大大消耗服务器的资源，所以 node.js 默认是不会解析请求体的，当你需要的时候，需要手动来做。 基本语法说明： 123456789101112131415161718var http = require('http');var querystring = require('querystring'); http.createServer(function(req, res)&#123; // 定义了一个post变量，用于暂存请求体的信息 var post = ''; // 通过req的data事件监听函数，每当接受到请求体的数据，就累加到post变量中 req.on('data', function(chunk)&#123; post += chunk; &#125;); // 在end事件触发后，通过querystring.parse将post解析为真正的POST请求格式，然后向客户端返回。 req.on('end', function()&#123; post = querystring.parse(post); res.end(util.inspect(post)); &#125;);&#125;).listen(3000); 以下实例表单通过 POST 提交并输出数据： 1234567891011121314151617181920212223242526272829303132var http = require('http');var querystring = require('querystring');var postHTML = '&lt;html&gt;&lt;head&gt;&lt;meta charset="utf-8"&gt;&lt;title&gt;菜鸟教程 Node.js 实例&lt;/title&gt;&lt;/head&gt;' + '&lt;body&gt;' + '&lt;form method="post"&gt;' + '网站名： &lt;input name="name"&gt;&lt;br&gt;' + '网站 URL： &lt;input name="url"&gt;&lt;br&gt;' + '&lt;input type="submit"&gt;' + '&lt;/form&gt;' + '&lt;/body&gt;&lt;/html&gt;';http.createServer(function (req,res) &#123; var body = ''; req.on('data',function (chunk) &#123; body += chunk; &#125;); req.on('end',function ()&#123; // 解析参数 body = querystring.parse(body); res.writeHead(200,&#123;'Content-Type':'text/html;charset=utf8'&#125;); if (body.name &amp;&amp; body.url)&#123; // 输出提交的数据 res.write('网站名:' + body.name); res.write('&lt;br&gt;'); res.write('网站URL：' + body.url); &#125; else &#123; // 输出表单 res.write(postHTML); &#125; res.end(); &#125;);&#125;).listen(3000); 输出结果（127.0.0.1:3000）： 输入： ​ 网站名：菜鸟教程 ​ 网站URL：www.runoob.com 12网站名:菜鸟教程网站URL：www.runoob.com 14、工具模块在 Node.js 模块库中有很多好用的模块。接下来我们为大家介绍几种常用模块的使用： 序号 模块名 &amp; 描述 1 OS 模块 提供基本的系统操作函数。 2 Path 模块 提供了处理和转换文件路径的工具。 3 Net 模块 用于底层的网络通信。提供了服务端和客户端的的操作。 4 DNS 模块 用于解析域名。 5 Domain 模块 简化异步代码的异常处理，可以捕捉处理try catch无法捕捉的。 （1）OS模块Node.js os 模块提供了一些基本的系统操作函数。我们可以通过以下方式引入该模块： 1var os = require("os") 方法 序号 方法 &amp; 描述 1 os.tmpdir() 返回操作系统的默认临时文件夹。 2 os.endianness() 返回 CPU 的字节序，可能的是 “BE” 或 “LE”。 3 os.hostname() 返回操作系统的主机名。 4 os.type() 返回操作系统名 5 os.platform() 返回操作系统名 6 os.arch() 返回操作系统 CPU 架构，可能的值有 “x64”、”arm” 和 “ia32”。 7 os.release() 返回操作系统的发行版本。 8 os.uptime() 返回操作系统运行的时间，以秒为单位。 9 os.loadavg() 返回一个包含 1、5、15 分钟平均负载的数组。 10 os.totalmem() 返回系统内存总量，单位为字节。 11 os.freemem() 返回操作系统空闲内存量，单位是字节。 12 os.cpus() 返回一个对象数组，包含所安装的每个 CPU/内核的信息：型号、速度（单位 MHz）、时间（一个包含 user、nice、sys、idle 和 irq 所使用 CPU/内核毫秒数的对象）。 13 os.networkInterfaces() 获得网络接口列表。 属性 序号 属性 &amp; 描述 1 os.EOL 定义了操作系统的行尾符的常量。 实例创建 main.js 文件，代码如下所示： 12345678910111213141516171819var os = require('os');// CPU的字节库console.log('endianness : ' + os.endianness());// 操作系统名console.log('platform：' + os.platform());// 系统内存总量console.log('total memory：' + os.totalmem() + 'bytes.');//操作系统空闲内存量console.log('free memory：' + os.freemem() + 'bytes.');/*endianness : LEplatform：win32total memory：8580616192bytes.free memory：4759666688bytes. */ （2）Path模块Node.js path 模块提供了一些用于处理文件路径的小工具，我们可以通过以下方式引入该模块： 1var path = require("path") 方法 序号 方法 &amp; 描述 1 path.normalize(p) 规范化路径，注意’..’ 和 ‘.’。 2 path.join([path1][, path2][, …]) 用于连接路径。该方法的主要用途在于，会正确使用当前系统的路径分隔符，Unix系统是”/“，Windows系统是”\”。 3 path.resolve([from …], to) 将 to 参数解析为绝对路径。 4 path.isAbsolute(path) 判断参数 path 是否是绝对路径。 5 path.relative(from, to) 用于将相对路径转为绝对路径。 6 path.dirname(p) 返回路径中代表文件夹的部分，同 Unix 的dirname 命令类似。 7 path.basename(p[, ext]) 返回路径中的最后一部分。同 Unix 命令 bashname 类似。 8 path.extname(p) 返回路径中文件的后缀名，即路径中最后一个’.’之后的部分。如果一个路径中并不包含’.’或该路径只包含一个’.’ 且这个’.’为路径的第一个字符，则此命令返回空字符串。 9 path.parse(pathString) 返回路径字符串的对象。 10 path.format(pathObject) 从对象中返回路径字符串，和 path.parse 相反。 属性 序号 属性 &amp; 描述 1 path.sep 平台的文件路径分隔符，’\‘ 或 ‘/‘。 2 path.delimiter 平台的分隔符, ; or ‘:’. 3 path.posix 提供上述 path 的方法，不过总是以 posix 兼容的方式交互。 4 path.win32 提供上述 path 的方法，不过总是以 win32 兼容的方式交互。 实例创建 main.js 文件，代码如下所示： 1234567891011121314151617181920var path = require('path');// 格式化路径console.log('normalization：' + path.normalize('/test/test1//2slashes/1slash/tab/...'));// 连接路径console.log('joint path：' + path.join('/test','test','2slashes/islash','tab','...'));// 转化为绝对路径console.log('resolve：' + path.resolve('module.js'));// 路径中文件的后缀名console.log('ext name：' + path.extname('main.js'));/*normalization：\test\test1\2slashes\1slash\tab\...joint path：\test\test\2slashes\islash\tab\...resolve：D:\exe\node\exe\module.jsext name：.js*/ （3）Net模块Node.js Net 模块提供了一些用于底层的网络通信的小工具，包含了创建服务器/客户端的方法，我们可以通过以下方式引入该模块： 1var net = require("net") 方法 序号 方法 &amp; 描述 1 net.createServer([options][, connectionListener]) 创建一个 TCP 服务器。参数 connectionListener 自动给 ‘connection’ 事件创建监听器。 2 net.connect(options[, connectionListener]) 返回一个新的 ‘net.Socket’，并连接到指定的地址和端口。 当 socket 建立的时候，将会触发 ‘connect’ 事件。 3 net.createConnection(options[, connectionListener]) 创建一个到端口 port 和 主机 host的 TCP 连接。 host 默认为 ‘localhost’。 4 net.connect(port[, host][, connectListener]) 创建一个端口为 port 和主机为 host的 TCP 连接 。host 默认为 ‘localhost’。参数 connectListener 将会作为监听器添加到 ‘connect’ 事件。返回 ‘net.Socket’。 5 net.createConnection(port[, host][, connectListener]) 创建一个端口为 port 和主机为 host的 TCP 连接 。host 默认为 ‘localhost’。参数 connectListener 将会作为监听器添加到 ‘connect’ 事件。返回 ‘net.Socket’。 6 net.connect(path[, connectListener]) 创建连接到 path 的 unix socket 。参数 connectListener 将会作为监听器添加到 ‘connect’ 事件上。返回 ‘net.Socket’。 7 net.createConnection(path[, connectListener]) 创建连接到 path 的 unix socket 。参数 connectListener 将会作为监听器添加到 ‘connect’ 事件。返回 ‘net.Socket’。 8 net.isIP(input) 检测输入的是否为 IP 地址。 IPV4 返回 4， IPV6 返回 6，其他情况返回 0。 9 net.isIPv4(input) 如果输入的地址为 IPV4， 返回 true，否则返回 false。 10 net.isIPv6(input) 如果输入的地址为 IPV6， 返回 true，否则返回 false。 net.Servernet.Server通常用于创建一个 TCP 或本地服务器。 序号 方法 &amp; 描述 1 server.listen(port[, host][, backlog][, callback]) 监听指定端口 port 和 主机 host ac连接。 默认情况下 host 接受任何 IPv4 地址(INADDR_ANY)的直接连接。端口 port 为 0 时，则会分配一个随机端口。 2 server.listen(path[, callback]) 通过指定 path 的连接，启动一个本地 socket 服务器。 3 server.listen(handle[, callback]) 通过指定句柄连接。 4 server.listen(options[, callback]) options 的属性：端口 port, 主机 host, 和 backlog, 以及可选参数 callback 函数, 他们在一起调用server.listen(port, [host], [backlog], [callback])。还有，参数 path 可以用来指定 UNIX socket。 5 server.close([callback]) 服务器停止接收新的连接，保持现有连接。这是异步函数，当所有连接结束的时候服务器会关闭，并会触发 ‘close’ 事件。 6 server.address() 操作系统返回绑定的地址，协议族名和服务器端口。 7 server.unref() 如果这是事件系统中唯一一个活动的服务器，调用 unref 将允许程序退出。 8 server.ref() 与 unref 相反，如果这是唯一的服务器，在之前被 unref 了的服务器上调用 ref 将不会让程序退出（默认行为）。如果服务器已经被 ref，则再次调用 ref 并不会产生影响。 9 server.getConnections(callback) 异步获取服务器当前活跃连接的数量。当 socket 发送给子进程后才有效；回调函数有 2 个参数 err 和 count。 事件 序号 事件 &amp; 描述 1 listening 当服务器调用 server.listen 绑定后会触发。 2 connection 当新连接创建后会被触发。socket 是 net.Socket实例。 3 close 服务器关闭时会触发。注意，如果存在连接，这个事件不会被触发直到所有的连接关闭。 4 error 发生错误时触发。’close’ 事件将被下列事件直接调用。 net.Socketnet.Socket 对象是 TCP 或 UNIX Socket 的抽象。net.Socket 实例实现了一个双工流接口。 他们可以在用户创建客户端(使用 connect())时使用, 或者由 Node 创建它们，并通过 connection 服务器事件传递给用户。 事件net.Socket 事件有： 序号 事件 &amp; 描述 1 lookup 在解析域名后，但在连接前，触发这个事件。对 UNIX sokcet 不适用。 2 connect 成功建立 socket 连接时触发。 3 data 当接收到数据时触发。 4 end 当 socket 另一端发送 FIN 包时，触发该事件。 5 timeout 当 socket 空闲超时时触发，仅是表明 socket 已经空闲。用户必须手动关闭连接。 6 drain 当写缓存为空得时候触发。可用来控制上传。 7 error 错误发生时触发。 8 close 当 socket 完全关闭时触发。参数 had_error 是布尔值，它表示是否因为传输错误导致 socket 关闭。 属性net.Socket 提供了很多有用的属性，便于控制 socket 交互： 序号 属性 &amp; 描述 1 socket.bufferSize 该属性显示了要写入缓冲区的字节数。 2 socket.remoteAddress 远程的 IP 地址字符串，例如：’74.125.127.100’ or ‘2001:4860:a005::68’。 3 socket.remoteFamily 远程IP协议族字符串，比如 ‘IPv4’ or ‘IPv6’。 4 socket.remotePort 远程端口，数字表示，例如：80 or 21。 5 socket.localAddress 网络连接绑定的本地接口 远程客户端正在连接的本地 IP 地址，字符串表示。例如，如果你在监听’0.0.0.0’而客户端连接在’192.168.1.1’，这个值就会是 ‘192.168.1.1’。 6 socket.localPort 本地端口地址，数字表示。例如：80 or 21。 7 socket.bytesRead 接收到得字节数。 8 socket.bytesWritten 发送的字节数。 方法 序号 方法 &amp; 描述 1 new net.Socket([options]) 构造一个新的 socket 对象。 2 socket.connect(port[, host][, connectListener]) 指定端口 port 和 主机 host，创建 socket 连接 。参数 host 默认为 localhost。通常情况不需要使用 net.createConnection 打开 socket。只有你实现了自己的 socket 时才会用到。 3 socket.connect(path[, connectListener]) 打开指定路径的 unix socket。通常情况不需要使用 net.createConnection 打开 socket。只有你实现了自己的 socket 时才会用到。 4 socket.setEncoding([encoding]) 设置编码 5 socket.write(data[, encoding][, callback]) 在 socket 上发送数据。第二个参数指定了字符串的编码，默认是 UTF8 编码。 6 socket.end([data][, encoding]) 半关闭 socket。例如，它发送一个 FIN 包。可能服务器仍在发送数据。 7 socket.destroy() 确保没有 I/O 活动在这个套接字上。只有在错误发生情况下才需要。（处理错误等等）。 8 socket.pause() 暂停读取数据。就是说，不会再触发 data 事件。对于控制上传非常有用。 9 socket.resume() 调用 pause() 后想恢复读取数据。 10 socket.setTimeout(timeout[, callback]) socket 闲置时间超过 timeout 毫秒后 ，将 socket 设置为超时。 11 socket.setNoDelay([noDelay]) 禁用纳格（Nagle）算法。默认情况下 TCP 连接使用纳格算法，在发送前他们会缓冲数据。将 noDelay 设置为 true 将会在调用 socket.write() 时立即发送数据。noDelay 默认值为 true。 12 socket.setKeepAlive([enable][, initialDelay]) 禁用/启用长连接功能，并在发送第一个在闲置 socket 上的长连接 probe 之前，可选地设定初始延时。默认为 false。 设定 initialDelay （毫秒），来设定收到的最后一个数据包和第一个长连接probe之间的延时。将 initialDelay 设为0，将会保留默认（或者之前）的值。默认值为0. 13 socket.address() 操作系统返回绑定的地址，协议族名和服务器端口。返回的对象有 3 个属性，比如{ port: 12346, family: ‘IPv4’, address: ‘127.0.0.1’ }。 14 socket.unref() 如果这是事件系统中唯一一个活动的服务器，调用 unref 将允许程序退出。如果服务器已被 unref，则再次调用 unref 并不会产生影响。 15 socket.ref() 与 unref 相反，如果这是唯一的服务器，在之前被 unref 了的服务器上调用 ref 将不会让程序退出（默认行为）。如果服务器已经被 ref，则再次调用 ref 并不会产生影响。 实例创建 server.js 文件，代码如下所示： 12345678910111213var net = require('net');var server = net.createServer(function(connection) &#123; console.log('client connected'); connection.on('end',function() &#123; console.log('客户端关闭连接'); &#125;); connection.write('Hello World! \r\n'); connection.pipe(connection);&#125;);server.listen(8080,function() &#123; console.log('server is listening');&#125;); 执行以上服务端代码： 12$ node server.jsserver is listening # 服务已创建并监听 8080 端口 新开一个窗口，创建 client.js 文件，代码如下所示： 1234567891011var net = require('net');var client = net.connect(&#123;port:8080&#125;,function() &#123; console.log('连接到服务器！');&#125;);client.on('data',function(data) &#123; console.log(data.toString()); client.end();&#125;);client.on('end',function()&#123; console.log('断开与服务器的连接');&#125;); 执行以上客户端的代码： 1234连接到服务器！Hello World!断开与服务器的连接 （4）DNS模块Node.js DNS 模块用于解析域名。引入 DNS 模块语法格式如下： 1var dns = require("dns") 方法 序号 方法 &amp; 描述 1 dns.lookup(hostname[, options], callback) 将域名（比如 ‘runoob.com’）解析为第一条找到的记录 A （IPV4）或 AAAA(IPV6)。参数 options可以是一个对象或整数。如果没有提供 options，IP v4 和 v6 地址都可以。如果 options 是整数，则必须是 4 或 6。 2 dns.lookupService(address, port, callback) 使用 getnameinfo 解析传入的地址和端口为域名和服务。 3 dns.resolve(hostname[, rrtype], callback) 将一个域名（如 ‘runoob.com’）解析为一个 rrtype 指定记录类型的数组。 4 dns.resolve4(hostname, callback) 和 dns.resolve() 类似, 仅能查询 IPv4 (A 记录）。 addresses IPv4 地址数组 (比如，[‘74.125.79.104’, ‘74.125.79.105’, ‘74.125.79.106’]）。 5 dns.resolve6(hostname, callback) 和 dns.resolve4() 类似， 仅能查询 IPv6( AAAA 查询） 6 dns.resolveMx(hostname, callback) 和 dns.resolve() 类似, 仅能查询邮件交换(MX 记录)。 7 dns.resolveTxt(hostname, callback) 和 dns.resolve() 类似, 仅能进行文本查询 (TXT 记录）。 addresses 是 2-d 文本记录数组。(比如，[ [‘v=spf1 ip4:0.0.0.0 ‘, ‘~all’ ] ]）。 每个子数组包含一条记录的 TXT 块。根据使用情况可以连接在一起，也可单独使用。 8 dns.resolveSrv(hostname, callback) 和 dns.resolve() 类似, 仅能进行服务记录查询 (SRV 记录）。 addresses 是 hostname可用的 SRV 记录数组。 SRV 记录属性有优先级（priority），权重（weight）, 端口（port）, 和名字（name） (比如，[{‘priority’: 10, ‘weight’: 5, ‘port’: 21223, ‘name’: ‘service.example.com’}, …]）。 9 dns.resolveSoa(hostname, callback) 和 dns.resolve() 类似, 仅能查询权威记录(SOA 记录）。 10 dns.resolveNs(hostname, callback) 和 dns.resolve() 类似, 仅能进行域名服务器记录查询(NS 记录）。 addresses 是域名服务器记录数组（hostname 可以使用） (比如, [‘ns1.example.com’, ‘ns2.example.com’]）。 11 dns.resolveCname(hostname, callback) 和 dns.resolve() 类似, 仅能进行别名记录查询 (CNAME记录)。addresses 是对 hostname 可用的别名记录数组 (比如，, [‘bar.example.com’]）。 12 dns.reverse(ip, callback) 反向解析 IP 地址，指向该 IP 地址的域名数组。 13 dns.getServers() 返回一个用于当前解析的 IP 地址数组的字符串。 14 dns.setServers(servers) 指定一组 IP 地址作为解析服务器。 rrtypes以下列出了 dns.resolve() 方法中有效的 rrtypes值: &#39;A&#39; IPV4 地址, 默认 &#39;AAAA&#39; IPV6 地址 &#39;MX&#39; 邮件交换记录 &#39;TXT&#39; text 记录 &#39;SRV&#39; SRV 记录 &#39;PTR&#39; 用来反向 IP 查找 &#39;NS&#39; 域名服务器记录 &#39;CNAME&#39; 别名记录 &#39;SOA&#39; 授权记录的初始值 错误码每次 DNS 查询都可能返回以下错误码： dns.NODATA: 无数据响应。 dns.FORMERR: 查询格式错误。 dns.SERVFAIL: 常规失败。 dns.NOTFOUND: 没有找到域名。 dns.NOTIMP: 未实现请求的操作。 dns.REFUSED: 拒绝查询。 dns.BADQUERY: 查询格式错误。 dns.BADNAME: 域名格式错误。 dns.BADFAMILY: 地址协议不支持。 dns.BADRESP: 回复格式错误。 dns.CONNREFUSED: 无法连接到 DNS 服务器。 dns.TIMEOUT: 连接 DNS 服务器超时。 dns.EOF: 文件末端。 dns.FILE: 读文件错误。 dns.NOMEM: 内存溢出。 dns.DESTRUCTION: 通道被摧毁。 dns.BADSTR: 字符串格式错误。 dns.BADFLAGS: 非法标识符。 dns.NONAME: 所给主机不是数字。 dns.BADHINTS: 非法HINTS标识符。 dns.NOTINITIALIZED: c c-ares 库尚未初始化。 dns.LOADIPHLPAPI: 加载 iphlpapi.dll 出错。 dns.ADDRGETNETWORKPARAMS: 无法找到 GetNetworkParams 函数。 dns.CANCELLED: 取消 DNS 查询。 实例创建 main.js 文件，代码如下所示： 1234567891011121314var dns = require('dns');dns.lookup('www.github.com',function onLookup(err, address, family) &#123; console.log('ip地址：',address); dns.reverse(address, function(err,hostnames) &#123; if (err) &#123; console.log(err.stack); &#125; console.log('反向解析 ' + address + ": " + JSON.stringify(hostnames)); &#125;);&#125;);/*ip 地址: 192.30.253.113反向解析 192.30.253.113: ["lb-192-30-253-113-iad.github.com"] */ （5）Domain模块Node.js Domain(域) 简化异步代码的异常处理，可以捕捉处理try catch无法捕捉的异常。引入 Domain 模块 语法格式如下： 1var domain = require("domain") domain模块，把处理多个不同的IO的操作作为一个组。注册事件和回调到domain，当发生一个错误事件或抛出一个错误时，domain对象会被通知，不会丢失上下文环境，也不导致程序错误立即退出，与process.on(‘uncaughtException’)不同。 Domain 模块可分为隐式绑定和显式绑定： 隐式绑定: 把在domain上下文中定义的变量，自动绑定到domain对象 显式绑定: 把不是在domain上下文中定义的变量，以代码的方式绑定到domain对象 方法 序号 方法 &amp; 描述 1 domain.run(function) 在域的上下文运行提供的函数，隐式的绑定了所有的事件分发器，计时器和底层请求。 2 domain.add(emitter) 显式的增加事件 3 domain.remove(emitter) 删除事件。 4 domain.bind(callback) 返回的函数是一个对于所提供的回调函数的包装函数。当调用这个返回的函数时，所有被抛出的错误都会被导向到这个域的 error 事件。 5 domain.intercept(callback) 和 domain.bind(callback) 类似。除了捕捉被抛出的错误外，它还会拦截 Error 对象作为参数传递到这个函数。 6 domain.enter() 进入一个异步调用的上下文，绑定到domain。 7 domain.exit() 退出当前的domain，切换到不同的链的异步调用的上下文中。对应domain.enter()。 8 domain.dispose() 释放一个domain对象，让node进程回收这部分资源。 9 domain.create() 返回一个domain对象。 属性 序号 属性 &amp; 描述 1 domain.members 已加入domain对象的域定时器和事件发射器的数组。 实例创建 main.js 文件，代码如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940var EventEmitter = require("events").EventEmitter;var domain = require("domain");var emitter1 = new EventEmitter();// 创建域var domain1 = domain.create();domain1.on('error', function(err)&#123; console.log("domain1 处理这个错误 ("+err.message+")");&#125;);// 显式绑定domain1.add(emitter1);emitter1.on('error',function(err)&#123; console.log("监听器处理此错误 ("+err.message+")");&#125;);emitter1.emit('error',new Error('通过监听器来处理'));emitter1.removeAllListeners('error');emitter1.emit('error',new Error('通过 domain1 处理'));var domain2 = domain.create();domain2.on('error', function(err)&#123; console.log("domain2 处理这个错误 ("+err.message+")");&#125;);// 隐式绑定domain2.run(function()&#123; var emitter2 = new EventEmitter(); emitter2.emit('error',new Error('通过 domain2 处理')); &#125;);domain1.remove(emitter1);emitter1.emit('error', new Error('转换为异常，系统将崩溃!')); 执行以上代码，结果如下所示: 12345678910111213141516监听器处理此错误 (通过监听器来处理)domain1 处理这个错误 (通过 domain1 处理)domain2 处理这个错误 (通过 domain2 处理)events.js:72 throw er; // Unhandled &apos;error&apos; event ^Error: 转换为异常，系统将崩溃! at Object.&lt;anonymous&gt; (/www/node/main.js:40:24) at Module._compile (module.js:456:26) at Object.Module._extensions..js (module.js:474:10) at Module.load (module.js:356:32) at Function.Module._load (module.js:312:12) at Function.Module.runMain (module.js:497:10) at startup (node.js:119:16) at node.js:929:3 15、Node.js Web 模块（1）什么是 Web 服务器？Web服务器一般指网站服务器，是指驻留于因特网上某种类型计算机的程序，Web服务器的基本功能就是提供Web信息浏览服务。它只需支持HTTP协议、HTML文档格式及URL，与客户端的网络浏览器配合。 大多数 web 服务器都支持服务端的脚本语言（php、python、ruby）等，并通过脚本语言从数据库获取数据，将结果返回给客户端浏览器。 目前最主流的三个Web服务器是Apache、Nginx、IIS。 （2）Web 应用架构 Client - 客户端，一般指浏览器，浏览器可以通过 HTTP 协议向服务器请求数据。 Server - 服务端，一般指 Web 服务器，可以接收客户端请求，并向客户端发送响应数据。 Business - 业务层， 通过 Web 服务器处理应用程序，如与数据库交互，逻辑运算，调用外部程序等。 Data - 数据层，一般由数据库组成。 （3）使用 Node 创建 Web 服务器Node.js 提供了 http 模块，http 模块主要用于搭建 HTTP 服务端和客户端，使用 HTTP 服务器或客户端功能必须调用 http 模块，代码如下： 1var http = require('http'); 以下是演示一个最基本的 HTTP 服务器架构(使用 8080 端口) 实例创建 server.js 文件，代码如下所示： 12345678910111213141516171819202122232425262728293031323334var http = require('http');var fs = require('fs');var url = require('url');// 创建服务器http.createServer( function (request, response) &#123; // 解析请求，包括文件名 var pathname = url.parse(request.url).pathname; // 输出请求的文件名 console.log('Request for ' + pathname + " received."); // 从文件系统中读取请求的文件内容 fs.readFile(pathname.substr(1),function (err,data)&#123; if (err)&#123; console.log(err); // HTTP 状态码：404：NOT FOUND // Content Type: text/plain response.writeHead(404,&#123;'Content-Type':'text/html'&#125;); &#125;else&#123; // HTTP 状态码：200：OK // Content Type: text/plain response.writeHead(200,&#123;'Content-Type':'text/html'&#125;); // 响应文件内容 response.write(data.toString()); &#125; // 发送响应数据 response.end(); &#125;);&#125;).listen(8080);// 控制台会输出以下信息console.log('Server running at http:127.0.0.1:8080/'); 接下来我们在该目录下创建一个 index.html 文件，代码如下： index.html 文件 1234567891011&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="utf-8"&gt;&lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;我的第一个标题&lt;/h1&gt; &lt;p&gt;我的第一个段落。&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 执行 server.js 文件： 12$ node server.jsServer running at http://127.0.0.1:8080/ 接着我们在浏览器中打开地址：http://127.0.0.1:8080/index.html，显示如下图所示: 执行 server.js 的控制台输出信息如下： 12Server running at http://127.0.0.1:8080/Request for /index.html received. # 客户端请求信息 （4）使用 Node 创建 Web 客户端Node 创建 Web 客户端需要引入 http 模块 实例创建 client.js 文件，代码如下所示： 123456789101112131415161718192021222324var http = require('http');// 用于请求的选项var options = &#123; host: 'localhost', port: '8080', path: '/index.html'&#125;;// 处理响应的回调函数var callback = function(response)&#123; // 不断更新数据 var body = ''; response.on('data',function(data)&#123; body += data; &#125;); response.on('end',function()&#123; // 数据接收完成 console.log(body); &#125;);&#125;// 向服务端发送请求var req = http.request(options, callback);req.end(); 新开一个终端，执行 client.js 文件，输出结果如下：(注意：此时服务器端server.js也要运行) 1234567891011&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;&gt;&lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;我的第一个标题&lt;/h1&gt; &lt;p&gt;我的第一个段落。&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 执行 server.js 的控制台输出信息如下： 12Server running at http://127.0.0.1:8080/Request for /index.html received. # 客户端请求信息 ‘http’模块提供两种使用方式： 作为服务端使用时，创建一个HTTP服务器，监听HTTP客户端请求并返回响应。 作为客户端使用时，发起一个HTTP客户端请求，获取服务端响应。 首先我们来看看服务端模式下如何工作。 123456// 网络操作var http = require('http');http.createServer(function(request,response)&#123; response.writeHead(200,&#123;'Content-Type':'text-plain'&#125;); response.end('Hello World');&#125;).listen(8142); 首先需要使用.createServer方法创建一个服务器，然后调用.listen方法监听端口。之后，每当来了一个客户端请求，创建服务器时传入的回调函数就被调用一次。可以看出，这是一种事件机制。 HTTP请求本质上是一个数据流，由请求头（headers）和请求体（body）组成。例如以下是一个完整的HTTP请求数据内容。 12345678POST / HTTP/1.1User-Agent: curl/7.26.0Host: localhostAccept: */*Content-Length: 11Content-Type: application/x-www-form-urlencodedHello World 可以看到，空行之上是请求头，之下是请求体。HTTP请求在发送给服务器时，可以认为是按照从头到尾的顺序一个字节一个字节地以数据流方式发送的。而http模块创建的HTTP服务器在接收到完整的请求头后，就会调用回调函数。在回调函数中，除了可以使用request对象访问请求头数据外，还能把request对象当作一个只读数据流来访问请求体数据。 123456789101112131415const buffer = require('buffer');const http = require('http');http.createServer(function(request,response)&#123; var body = []; console.log(request.method); console.log(request.headers); request.on('data',function(chunk)&#123; body.push(chunk); &#125;); request.on('end',function(chunk)&#123; body = buffer.concat(body); console.log(body.toString()); &#125;);&#125;).listen(80); HTTP响应本质上也是一个数据流，同样由响应头（headers）和响应体（body）组成。例如以下是一个完整的HTTP请求数据内容。 1234567HTTP/1.1 200 OKContent-Type: text/plainContent-Length: 11Date: Tue, 05 Nov 2013 05:31:38 GMTConnection: keep-aliveHello World 在回调函数中，除了可以使用response对象来写入响应头数据外，还能把response对象当作一个只写数据流来写入响应体数据。例如在以下例子中，服务端原样将客户端请求的请求体数据返回给客户端。 1234567891011http.createServer(function (request, response) &#123; response.writeHead(200, &#123; 'Content-Type': 'text/plain' &#125;); request.on('data', function (chunk) &#123; response.write(chunk); &#125;); request.on('end', function () &#123; response.end(); &#125;);&#125;).listen(80); 接下来我们看看客户端模式下如何工作。为了发起一个客户端HTTP请求，我们需要指定目标服务器的位置并发送请求头和请求体，以下示例演示了具体做法。 1234567891011121314var options = &#123; hostname: 'www.example.com', port: 80, path: '/upload', method: 'POST', headers: &#123; 'Content-Type': 'application/x-www-form-urlencoded' &#125; &#125;;var request = http.request(options, function (response) &#123;&#125;);request.write('Hello World');request.end(); 可以看到，.request方法创建了一个客户端，并指定请求目标和请求头数据。之后，就可以把request对象当作一个只写数据流来写入请求体数据和结束请求。另外，由于HTTP请求中GET请求是最常见的一种，并且不需要请求体，因此http模块也提供了以下便捷API。 1http.get(&apos;http://www.example.com/&apos;, function (response) &#123;&#125;); 当客户端发送请求并接收到完整的服务端响应头时，就会调用回调函数。在回调函数中，除了可以使用response对象访问响应头数据外，还能把response对象当作一个只读数据流来访问响应体数据。以下是一个例子。 12345678910111213141516171819202122232425http.get(&apos;http://www.example.com/&apos;, function (response) &#123; var body = []; console.log(response.statusCode); console.log(response.headers); response.on(&apos;data&apos;, function (chunk) &#123; body.push(chunk); &#125;); response.on(&apos;end&apos;, function () &#123; body = Buffer.concat(body); console.log(body.toString()); &#125;);&#125;);------------------------------------200&#123; &apos;content-type&apos;: &apos;text/html&apos;, server: &apos;Apache&apos;, &apos;content-length&apos;: &apos;801&apos;, date: &apos;Tue, 05 Nov 2013 06:08:41 GMT&apos;, connection: &apos;keep-alive&apos; &#125;&lt;!DOCTYPE html&gt;... 16、异步编程回调在代码中，异步编程的直接体现就是回调。异步编程依托于回调来实现，但不能说使用了回调后程序就异步化了。我们首先可以看看以下代码。 12345678910111213141516171819202122function heavyCompute(n, callback) &#123; var count = 0, i, j; for (i = n; i &gt; 0; --i) &#123; for (j = n; j &gt; 0; --j) &#123; count += 1; &#125; &#125; callback(count);&#125;heavyCompute(10000, function (count) &#123; console.log(count);&#125;);console.log(&apos;hello&apos;);-- Console ------------------------------100000000hello 可以看到，以上代码中的回调函数仍然先于后续代码执行。JS本身是单线程运行的，不可能在一段代码还未结束运行时去运行别的代码，因此也就不存在异步执行的概念。 但是，如果某个函数做的事情是创建一个别的线程或进程，并与JS主线程并行地做一些事情，并在事情做完后通知JS主线程，那情况又不一样了。我们接着看看以下代码。 123456789setTimeout(function () &#123; console.log(&apos;world&apos;);&#125;, 1000);console.log(&apos;hello&apos;);-- Console ------------------------------helloworld 这次可以看到，回调函数后于后续代码执行了。如同上边所说，JS本身是单线程的，无法异步执行，因此我们可以认为setTimeout这类JS规范之外的由运行环境提供的特殊函数做的事情是创建一个平行线程后立即返回，让JS主进程可以接着执行后续代码，并在收到平行进程的通知后再执行回调函数。除了setTimeout、setInterval这些常见的，这类函数还包括NodeJS提供的诸如fs.readFile之类的异步API。 另外，我们仍然回到JS是单线程运行的这个事实上，这决定了JS在执行完一段代码之前无法执行包括回调函数在内的别的代码。也就是说，即使平行线程完成工作了，通知JS主线程执行回调函数了，回调函数也要等到JS主线程空闲时才能开始执行。以下就是这么一个例子。 123456789101112131415161718192021function heavyCompute(n) &#123; var count = 0, i, j; for (i = n; i &gt; 0; --i) &#123; for (j = n; j &gt; 0; --j) &#123; count += 1; &#125; &#125;&#125;var t = new Date();setTimeout(function () &#123; console.log(new Date() - t);&#125;, 1000);heavyCompute(50000);-- Console ------------------------------8520 可以看到，本来应该在1秒后被调用的回调函数因为JS主线程忙于运行其它代码，实际执行时间被大幅延迟。 代码设计模式异步编程有很多特有的代码设计模式，为了实现同样的功能，使用同步方式和异步方式编写的代码会有很大差异。以下分别介绍一些常见的模式。 函数返回值使用一个函数的输出作为另一个函数的输入是很常见的需求，在同步方式下一般按以下方式编写代码： 12var output = fn1(fn2(&apos;input&apos;));// Do something. 而在异步方式下，由于函数执行结果不是通过返回值，而是通过回调函数传递，因此一般按以下方式编写代码： 12345fn2(&apos;input&apos;, function (output2) &#123; fn1(output2, function (output1) &#123; // Do something. &#125;);&#125;); 可以看到，这种方式就是一个回调函数套一个回调函多，套得太多了很容易写出&gt;形状的代码。 遍历数组在遍历数组时，使用某个函数依次对数据成员做一些处理也是常见的需求。如果函数是同步执行的，一般就会写出以下代码： 12345678var len = arr.length, i = 0;for (; i &lt; len; ++i) &#123; arr[i] = sync(arr[i]);&#125;// All array items have processed. 如果函数是异步执行的，以上代码就无法保证循环结束后所有数组成员都处理完毕了。如果数组成员必须一个接一个串行处理，则一般按照以下方式编写异步代码： 123456789101112(function next(i, len, callback) &#123; if (i &lt; len) &#123; async(arr[i], function (value) &#123; arr[i] = value; next(i + 1, len, callback); &#125;); &#125; else &#123; callback(); &#125;&#125;(0, arr.length, function () &#123; // All array items have processed.&#125;)); 可以看到，以上代码在异步函数执行一次并返回执行结果后才传入下一个数组成员并开始下一轮执行，直到所有数组成员处理完毕后，通过回调的方式触发后续代码的执行。 如果数组成员可以并行处理，但后续代码仍然需要所有数组成员处理完毕后才能执行的话，则异步代码会调整成以下形式： 1234567891011121314(function (i, len, count, callback) &#123; for (; i &lt; len; ++i) &#123; (function (i) &#123; async(arr[i], function (value) &#123; arr[i] = value; if (++count === len) &#123; callback(); &#125; &#125;); &#125;(i)); &#125;&#125;(0, arr.length, 0, function () &#123; // All array items have processed.&#125;)); 可以看到，与异步串行遍历的版本相比，以上代码并行处理所有数组成员，并通过计数器变量来判断什么时候所有数组成员都处理完毕了。 异常处理JS自身提供的异常捕获和处理机制——try..catch..，只能用于同步执行的代码。以下是一个例子。 12345678910111213function sync(fn) &#123; return fn();&#125;try &#123; sync(null); // Do something.&#125; catch (err) &#123; console.log(&apos;Error: %s&apos;, err.message);&#125;-- Console ------------------------------Error: object is not a function 可以看到，异常会沿着代码执行路径一直冒泡，直到遇到第一个try语句时被捕获住。但由于异步函数会打断代码执行路径，异步函数执行过程中以及执行之后产生的异常冒泡到执行路径被打断的位置时，如果一直没有遇到try语句，就作为一个全局异常抛出。以下是一个例子。 12345678910111213141516171819202122function async(fn, callback) &#123; // Code execution path breaks here. setTimeout(function () &#123; callback(fn()); &#125;, 0);&#125;try &#123; async(null, function (data) &#123; // Do something. &#125;);&#125; catch (err) &#123; console.log(&apos;Error: %s&apos;, err.message);&#125;-- Console ------------------------------/home/user/test.js:4 callback(fn()); ^TypeError: object is not a function at null._onTimeout (/home/user/test.js:4:13) at Timer.listOnTimeout [as ontimeout] (timers.js:110:15) 因为代码执行路径被打断了，我们就需要在异常冒泡到断点之前用try语句把异常捕获住，并通过回调函数传递被捕获的异常。于是我们可以像下边这样改造上边的例子。 123456789101112131415161718192021function async(fn, callback) &#123; // Code execution path breaks here. setTimeout(function () &#123; try &#123; callback(null, fn()); &#125; catch (err) &#123; callback(err); &#125; &#125;, 0);&#125;async(null, function (err, data) &#123; if (err) &#123; console.log(&apos;Error: %s&apos;, err.message); &#125; else &#123; // Do something. &#125;&#125;);-- Console ------------------------------Error: object is not a function 可以看到，异常再次被捕获住了。在NodeJS中，几乎所有异步API都按照以上方式设计，回调函数中第一个参数都是err。因此我们在编写自己的异步函数时，也可以按照这种方式来处理异常，与NodeJS的设计风格保持一致。 有了异常处理方式后，我们接着可以想一想一般我们是怎么写代码的。基本上，我们的代码都是做一些事情，然后调用一个函数，然后再做一些事情，然后再调用一个函数，如此循环。如果我们写的是同步代码，只需要在代码入口点写一个try语句就能捕获所有冒泡上来的异常，示例如下。 1234567891011121314function main() &#123; // Do something. syncA(); // Do something. syncB(); // Do something. syncC();&#125;try &#123; main();&#125; catch (err) &#123; // Deal with exception.&#125; 但是，如果我们写的是异步代码，就只有呵呵了。由于每次异步函数调用都会打断代码执行路径，只能通过回调函数来传递异常，于是我们就需要在每个回调函数里判断是否有异常发生，于是只用三次异步函数调用，就会产生下边这种代码。 12345678910111213141516171819202122232425262728293031function main(callback) &#123; // Do something. asyncA(function (err, data) &#123; if (err) &#123; callback(err); &#125; else &#123; // Do something asyncB(function (err, data) &#123; if (err) &#123; callback(err); &#125; else &#123; // Do something asyncC(function (err, data) &#123; if (err) &#123; callback(err); &#125; else &#123; // Do something callback(null); &#125; &#125;); &#125; &#125;); &#125; &#125;);&#125;main(function (err) &#123; if (err) &#123; // Deal with exception. &#125;&#125;); 可以看到，回调函数已经让代码变得复杂了，而异步方式下对异常的处理更加剧了代码的复杂度。如果NodeJS的最大卖点最后变成这个样子，那就没人愿意用NodeJS了，因此接下来会介绍NodeJS提供的一些解决方案。 域（Domain） 官方文档： http://nodejs.org/api/domain.html 推荐网站：https://nqdeng.github.io/7-days-nodejs/#1 NodeJS提供了domain模块，可以简化异步代码的异常处理。在介绍该模块之前，我们需要首先理解“域”的概念。简单的讲，一个域就是一个JS运行环境，在一个运行环境中，如果一个异常没有被捕获，将作为一个全局异常被抛出。NodeJS通过process对象提供了捕获全局异常的方法，示例代码如下 12345678910process.on(&apos;uncaughtException&apos;, function (err) &#123; console.log(&apos;Error: %s&apos;, err.message);&#125;);setTimeout(function (fn) &#123; fn();&#125;);-- Console ------------------------------Error: undefined is not a function 虽然全局异常有个地方可以捕获了，但是对于大多数异常，我们希望尽早捕获，并根据结果决定代码的执行路径。我们用以下HTTP服务器代码作为例子： 12345678910111213141516171819202122232425262728293031323334353637function async(request, callback) &#123; // Do something. asyncA(request, function (err, data) &#123; if (err) &#123; callback(err); &#125; else &#123; // Do something asyncB(request, function (err, data) &#123; if (err) &#123; callback(err); &#125; else &#123; // Do something asyncC(request, function (err, data) &#123; if (err) &#123; callback(err); &#125; else &#123; // Do something callback(null, data); &#125; &#125;); &#125; &#125;); &#125; &#125;);&#125;http.createServer(function (request, response) &#123; async(request, function (err, data) &#123; if (err) &#123; response.writeHead(500); response.end(); &#125; else &#123; response.writeHead(200); response.end(data); &#125; &#125;);&#125;); 以上代码将请求对象交给异步函数处理后，再根据处理结果返回响应。这里采用了使用回调函数传递异常的方案，因此async函数内部如果再多几个异步函数调用的话，代码就变成上边这副鬼样子了。为了让代码好看点，我们可以在每处理一个请求时，使用domain模块创建一个子域（JS子运行环境）。在子域内运行的代码可以随意抛出异常，而这些异常可以通过子域对象的error事件统一捕获。于是以上代码可以做如下改造： 1234567891011121314151617181920212223242526272829function async(request, callback) &#123; // Do something. asyncA(request, function (data) &#123; // Do something asyncB(request, function (data) &#123; // Do something asyncC(request, function (data) &#123; // Do something callback(data); &#125;); &#125;); &#125;);&#125;http.createServer(function (request, response) &#123; var d = domain.create(); d.on(&apos;error&apos;, function () &#123; response.writeHead(500); response.end(); &#125;); d.run(function () &#123; async(request, function (data) &#123; response.writeHead(200); response.end(data); &#125;); &#125;);&#125;); 可以看到，我们使用.create方法创建了一个子域对象，并通过.run方法进入需要在子域中运行的代码的入口点。而位于子域中的异步函数回调函数由于不再需要捕获异常，代码一下子瘦身很多。 陷阱无论是通过process对象的uncaughtException事件捕获到全局异常，还是通过子域对象的error事件捕获到了子域异常，在NodeJS官方文档里都强烈建议处理完异常后立即重启程序，而不是让程序继续运行。按照官方文档的说法，发生异常后的程序处于一个不确定的运行状态，如果不立即退出的话，程序可能会发生严重内存泄漏，也可能表现得很奇怪。 但这里需要澄清一些事实。JS本身的throw..try..catch异常处理机制并不会导致内存泄漏，也不会让程序的执行结果出乎意料，但NodeJS并不是存粹的JS。NodeJS里大量的API内部是用C/C++实现的，因此NodeJS程序的运行过程中，代码执行路径穿梭于JS引擎内部和外部，而JS的异常抛出机制可能会打断正常的代码执行流程，导致C/C++部分的代码表现异常，进而导致内存泄漏等问题。 因此，使用uncaughtException或domain捕获异常，代码执行路径里涉及到了C/C++部分的代码时，如果不能确定是否会导致内存泄漏等问题，最好在处理完异常后重启程序比较妥当。而使用try语句捕获异常时一般捕获到的都是JS本身的异常，不用担心上诉问题。 小结本章介绍了JS异步编程相关的知识，总结起来有以下几点： 不掌握异步编程就不算学会NodeJS。 异步编程依托于回调来实现，而使用回调不一定就是异步编程。 异步编程下的函数间数据传递、数组遍历和异常处理与同步编程有很大差别。 使用domain模块简化异步代码的异常处理，并小心陷阱。 17、大示例学习讲究的是学以致用和融会贯通。至此我们已经分别介绍了NodeJS的很多知识点，本章作为最后一章，将完整地介绍一个使用NodeJS开发Web服务器的示例。 需求我们要开发的是一个简单的静态文件合并服务器，该服务器需要支持类似以下格式的JS或CSS文件合并请求。 1http://assets.example.com/foo/??bar.js,baz.js 在以上URL中，??是一个分隔符，之前是需要合并的多个文件的URL的公共部分，之后是使用,分隔的差异部分。因此服务器处理这个URL时，返回的是以下两个文件按顺序合并后的内容。 12/foo/bar.js/foo/baz.js 另外，服务器也需要能支持类似以下格式的普通的JS或CSS文件请求。 1http://assets.example.com/foo/bar.js 以上就是整个需求。 第一次迭代快速迭代是一种不错的开发方式，因此我们在第一次迭代时先实现服务器的基本功能。 设计简单分析了需求之后，我们大致会得到以下的设计方案。 123 +---------+ +-----------+ +----------+request --&gt;| parse |--&gt;| combine |--&gt;| output |--&gt; response +---------+ +-----------+ +----------+ 也就是说，服务器会首先分析URL，得到请求的文件的路径和类型（MIME）。然后，服务器会读取请求的文件，并按顺序合并文件内容。最后，服务器返回响应，完成对一次请求的处理。 另外，服务器在读取文件时需要有个根目录，并且服务器监听的HTTP端口最好也不要写死在代码里，因此服务器需要是可配置的。 实现根据以上设计，我们写出了第一版代码如下。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970var fs = require(&apos;fs&apos;), path = require(&apos;path&apos;), http = require(&apos;http&apos;);var MIME = &#123; &apos;.css&apos;: &apos;text/css&apos;, &apos;.js&apos;: &apos;application/javascript&apos;&#125;;function combineFiles(pathnames, callback) &#123; var output = []; (function next(i, len) &#123; if (i &lt; len) &#123; fs.readFile(pathnames[i], function (err, data) &#123; if (err) &#123; callback(err); &#125; else &#123; output.push(data); next(i + 1, len); &#125; &#125;); &#125; else &#123; callback(null, Buffer.concat(output)); &#125; &#125;(0, pathnames.length));&#125;function main(argv) &#123; var config = JSON.parse(fs.readFileSync(argv[0], &apos;utf-8&apos;)), root = config.root || &apos;.&apos;, port = config.port || 80; http.createServer(function (request, response) &#123; var urlInfo = parseURL(root, request.url); combineFiles(urlInfo.pathnames, function (err, data) &#123; if (err) &#123; response.writeHead(404); response.end(err.message); &#125; else &#123; response.writeHead(200, &#123; &apos;Content-Type&apos;: urlInfo.mime &#125;); response.end(data); &#125; &#125;); &#125;).listen(port);&#125;function parseURL(root, url) &#123; var base, pathnames, parts; if (url.indexOf(&apos;??&apos;) === -1) &#123; url = url.replace(&apos;/&apos;, &apos;/??&apos;); &#125; parts = url.split(&apos;??&apos;); base = parts[0]; pathnames = parts[1].split(&apos;,&apos;).map(function (value) &#123; return path.join(root, base, value); &#125;); return &#123; mime: MIME[path.extname(pathnames[0])] || &apos;text/plain&apos;, pathnames: pathnames &#125;;&#125;main(process.argv.slice(2)); 以上代码完整实现了服务器所需的功能，并且有以下几点值得注意： 使用命令行参数传递JSON配置文件路径，入口函数负责读取配置并创建服务器。 入口函数完整描述了程序的运行逻辑，其中解析URL和合并文件的具体实现封装在其它两个函数里。 解析URL时先将普通URL转换为了文件合并URL，使得两种URL的处理方式可以一致。 合并文件时使用异步API读取文件，避免服务器因等待磁盘IO而发生阻塞。 我们可以把以上代码保存为server.js，之后就可以通过node server.js config.json命令启动程序，于是我们的第一版静态文件合并服务器就顺利完工了。 另外，以上代码存在一个不那么明显的逻辑缺陷。例如，使用以下URL请求服务器时会有惊喜。 1http://assets.example.com/foo/bar.js,foo/baz.js 经过分析之后我们会发现问题出在/被自动替换/??这个行为上，而这个问题我们可以到第二次迭代时再解决。 第二次迭代在第一次迭代之后，我们已经有了一个可工作的版本，满足了功能需求。接下来我们需要从性能的角度出发，看看代码还有哪些改进余地。 设计把map方法换成for循环或许会更快一些，但第一版代码最大的性能问题存在于从读取文件到输出响应的过程当中。我们以处理/??a.js,b.js,c.js这个请求为例，看看整个处理过程中耗时在哪儿。 12345678 发送请求 等待服务端响应 接收响应---------+----------------------+-------------&gt; -- 解析请求 ------ 读取a.js ------ 读取b.js ------ 读取c.js -- 合并数据 -- 输出响应 可以看到，第一版代码依次把请求的文件读取到内存中之后，再合并数据和输出响应。这会导致以下两个问题： 当请求的文件比较多比较大时，串行读取文件会比较耗时，从而拉长了服务端响应等待时间。 由于每次响应输出的数据都需要先完整地缓存在内存里，当服务器请求并发数较大时，会有较大的内存开销。 对于第一个问题，很容易想到把读取文件的方式从串行改为并行。但是别这样做，因为对于机械磁盘而言，因为只有一个磁头，尝试并行读取文件只会造成磁头频繁抖动，反而降低IO效率。而对于固态硬盘，虽然的确存在多个并行IO通道，但是对于服务器并行处理的多个请求而言，硬盘已经在做并行IO了，对单个请求采用并行IO无异于拆东墙补西墙。因此，正确的做法不是改用并行IO，而是一边读取文件一边输出响应，把响应输出时机提前至读取第一个文件的时刻。这样调整后，整个请求处理过程变成下边这样。 12345678发送请求 等待服务端响应 接收响应---------+----+-------------------------------&gt; -- 解析请求 -- 检查文件是否存在 -- 输出响应头 ------ 读取和输出a.js ------ 读取和输出b.js ------ 读取和输出c.js 按上述方式解决第一个问题后，因为服务器不需要完整地缓存每个请求的输出数据了，第二个问题也迎刃而解。 实现根据以上设计，第二版代码按以下方式调整了部分函数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354function main(argv) &#123; var config = JSON.parse(fs.readFileSync(argv[0], &apos;utf-8&apos;)), root = config.root || &apos;.&apos;, port = config.port || 80; http.createServer(function (request, response) &#123; var urlInfo = parseURL(root, request.url); validateFiles(urlInfo.pathnames, function (err, pathnames) &#123; if (err) &#123; response.writeHead(404); response.end(err.message); &#125; else &#123; response.writeHead(200, &#123; &apos;Content-Type&apos;: urlInfo.mime &#125;); outputFiles(pathnames, response); &#125; &#125;); &#125;).listen(port);&#125;function outputFiles(pathnames, writer) &#123; (function next(i, len) &#123; if (i &lt; len) &#123; var reader = fs.createReadStream(pathnames[i]); reader.pipe(writer, &#123; end: false &#125;); reader.on(&apos;end&apos;, function() &#123; next(i + 1, len); &#125;); &#125; else &#123; writer.end(); &#125; &#125;(0, pathnames.length));&#125;function validateFiles(pathnames, callback) &#123; (function next(i, len) &#123; if (i &lt; len) &#123; fs.stat(pathnames[i], function (err, stats) &#123; if (err) &#123; callback(err); &#125; else if (!stats.isFile()) &#123; callback(new Error()); &#125; else &#123; next(i + 1, len); &#125; &#125;); &#125; else &#123; callback(null, pathnames); &#125; &#125;(0, pathnames.length));&#125; 可以看到，第二版代码在检查了请求的所有文件是否有效之后，立即就输出了响应头，并接着一边按顺序读取文件一边输出响应内容。并且，在读取文件时，第二版代码直接使用了只读数据流来简化代码。 第三次迭代第二次迭代之后，服务器本身的功能和性能已经得到了初步满足。接下来我们需要从稳定性的角度重新审视一下代码，看看还需要做些什么。 设计从工程角度上讲，没有绝对可靠的系统。即使第二次迭代的代码经过反复检查后能确保没有bug，也很难说是否会因为NodeJS本身，或者是操作系统本身，甚至是硬件本身导致我们的服务器程序在某一天挂掉。因此一般生产环境下的服务器程序都配有一个守护进程，在服务挂掉的时候立即重启服务。一般守护进程的代码会远比服务进程的代码简单，从概率上可以保证守护进程更难挂掉。如果再做得严谨一些，甚至守护进程自身可以在自己挂掉时重启自己，从而实现双保险。 因此在本次迭代时，我们先利用NodeJS的进程管理机制，将守护进程作为父进程，将服务器程序作为子进程，并让父进程监控子进程的运行状态，在其异常退出时重启子进程。 实现根据以上设计，我们编写了守护进程需要的代码。 12345678910111213141516171819202122var cp = require(&apos;child_process&apos;);var worker;function spawn(server, config) &#123; worker = cp.spawn(&apos;node&apos;, [ server, config ]); worker.on(&apos;exit&apos;, function (code) &#123; if (code !== 0) &#123; spawn(server, config); &#125; &#125;);&#125;function main(argv) &#123; spawn(&apos;server.js&apos;, argv[0]); process.on(&apos;SIGTERM&apos;, function () &#123; worker.kill(); process.exit(0); &#125;);&#125;main(process.argv.slice(2)); 此外，服务器代码本身的入口函数也要做以下调整。 12345678910111213141516function main(argv) &#123; var config = JSON.parse(fs.readFileSync(argv[0], &apos;utf-8&apos;)), root = config.root || &apos;.&apos;, port = config.port || 80, server; server = http.createServer(function (request, response) &#123; ... &#125;).listen(port); process.on(&apos;SIGTERM&apos;, function () &#123; server.close(function () &#123; process.exit(0); &#125;); &#125;);&#125; 我们可以把守护进程的代码保存为daemon.js，之后我们可以通过node daemon.js config.json启动服务，而守护进程会进一步启动和监控服务器进程。此外，为了能够正常终止服务，我们让守护进程在接收到SIGTERM信号时终止服务器进程。而在服务器进程这一端，同样在收到SIGTERM信号时先停掉HTTP服务再正常退出。至此，我们的服务器程序就靠谱很多了。 第四次迭代在我们解决了服务器本身的功能、性能和可靠性的问题后，接着我们需要考虑一下代码部署的问题，以及服务器控制的问题。 设计一般而言，程序在服务器上有一个固定的部署目录，每次程序有更新后，都重新发布到部署目录里。而一旦完成部署后，一般也可以通过固定的服务控制脚本启动和停止服务。因此我们的服务器程序部署目录可以做如下设计。 123456789- deploy/ - bin/ startws.sh killws.sh + conf/ config.json + lib/ daemon.js server.js 在以上目录结构中，我们分类存放了服务控制脚本、配置文件和服务器代码。 实现按以上目录结构分别存放对应的文件之后，接下来我们看看控制脚本怎么写。首先是start.sh。 123456#!/bin/shif [ ! -f &quot;pid&quot; ]then node ../lib/daemon.js ../conf/config.json &amp; echo $! &gt; pidfi 然后是killws.sh。 123456#!/bin/shif [ -f &quot;pid&quot; ]then kill $(tr -d &apos;\r\n&apos; &lt; pid) rm pidfi 于是这样我们就有了一个简单的代码部署目录和服务控制脚本，我们的服务器程序就可以上线工作了。 后续迭代我们的服务器程序正式上线工作后，我们接下来或许会发现还有很多可以改进的点。比如服务器程序在合并JS文件时可以自动在JS文件之间插入一个;来避免一些语法问题，比如服务器程序需要提供日志来统计访问量，比如服务器程序需要能充分利用多核CPU，等等。而此时的你，在学习了这么久NodeJS之后，应该已经知道该怎么做了。 小结本章将之前零散介绍的知识点串了起来，完整地演示了一个使用NodeJS开发程序的例子，至此我们的课程就全部结束了。以下是对新诞生的NodeJSer的一些建议。 要熟悉官方API文档。并不是说要熟悉到能记住每个API的名称和用法，而是要熟悉NodeJS提供了哪些功能，一旦需要时知道查询API文档的哪块地方。 要先设计再实现。在开发一个程序前首先要有一个全局的设计，不一定要很周全，但要足够能写出一些代码。 要实现后再设计。在写了一些代码，有了一些具体的东西后，一定会发现一些之前忽略掉的细节。这时再反过来改进之前的设计，为第二轮迭代做准备。 要充分利用三方包。NodeJS有一个庞大的生态圈，在写代码之前先看看有没有现成的三方包能节省不少时间。 不要迷信三方包。任何事情做过头了就不好了，三方包也是一样。三方包是一个黑盒，每多使用一个三方包，就为程序增加了一份潜在风险。并且三方包很难恰好只提供程序需要的功能，每多使用一个三方包，就让程序更加臃肿一些。因此在决定使用某个三方包之前，最好三思而后行。]]></content>
      <categories>
        <category>javascript</category>
        <category>nodejs</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>javascript</tag>
        <tag>nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[js google代码规范]]></title>
    <url>%2F2018%2F09%2F02%2Fjavascript%2Fnodejs%2F01%E3%80%81JS_Google%E4%BB%A3%E7%A0%81%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[文件名文件名须全部小写，下划线（_）和短横线（-）源文件采用 UTF-8 编码。 非空代码块：K&amp;R 风格 左花括号不另起新行 左花括号后紧跟换行 右花括号前需要换行 如果右花括号结束了语句，或者它是函数、类、类中的方法的结束括号，则其后面需要换行。如果后面紧跟的是 else，catch 或 while，或逗号，分号以及右括号，则不需要跟一个换行。 代码块中的编进：+2 个空格新开代码块中代码需要加 2 个空格的缩进 数组字面量：类代码块（block-like）12345const c = [0, 1, 2];someMethod(foo, [ 0, 1, 2,], bar); Switch 语句一如其他代码块一样，switch 语句中每个条件块也是增加 2 个空格的缩进。每个 switch 标签后新起一行，加缩进，就像是创建了一个带花括号的代码块一样。每个标签开始时缩进又恢复，相当于只有 switch 标签中的内容是当作代码块来处理的。 break 关键字后面的空行是可选的。 123456789101112switch (animal) &#123; case Animal.BANDERSNATCH: handleBandersnatch(); break; case Animal.JABBERWOCK: handleJabberwock(); break; default: throw new Error('Unknown animal');&#125; 声明语句一个声明占一行每个声明语句后都跟换行。 分号是必需的语句后需用分号结束。不能依赖于编辑器的自动分号插入功能。 最大列宽：80何处该换行换行的的准则是：尽量在优先级高的语法层面（higher syntactic level）进行 123currentEstimate = calc(currentEstimate + x * currentEstimate) / 2.0f; 语法优先级从高到低依次为：赋值，除法，函数调用，参数，数字常量。 换行后后续行至少有 4 个空格的缩进当发生换行时，第一行后面跟着的其他行至少缩进 4 个空格，除非满足代码块的缩进规则，另说。 换行后后续跟随多行时，缩进可适当大于 4 个空格。通常，语法中低优先级的后续行以 4 的倍数进行缩进，如果只有两行并且处于同一优先级，则保持一样的缩进即可。 空格垂直方向的空格以下场景需要有一个空行： 类或对象中的方法间a. 例外的情形：对象中属性间的空行是可选的。如果有的话，一般是用来将属性进行分组。 方法体中，尽量少地使用空行来进行代码的分隔。函数体开始和结束都不要加空行。3.类或对象中首个方法前及最后一个方法后的空行，既不提倡也不反对。 适用此风格中其他条目的规定（e.g.3.4 goog.require statements）。 连续多个空行不是必需的，但了不鼓励这么做。 水平方向的空格水平方向的空格依位置为定，有三种大的分类：行首（一行的开始），行尾（一行的结束）以及行间（一行中除去行首及行尾的部分）。行首的空格（i.e. 缩进）无处不在。行尾的空格是禁止的。 除了 Javascript 本身及其他规则的要求，还有字面量，注释，JSDoc 等需要的空格外，单个的 ASCII 类型的空格在以下情形中也是需要的。 将关键字（譬如 if，for，catch）与括号（(）分隔。 将关键字（else，catch）与闭合括号（}） 分隔。 对于左花括号有两种例外：a. 作为函数首个参数的对象之前，数组中首个对象元素 （e.g. foo({a: [{c: d}]})）b. 在模板表达式中，因为模板语法的限制不能加空格（e.g. abc${1 + 2}def）。 二元，三元操作符的两边 逗号或分号后。但其前面是不允许有空格的。 对象字面量中冒号后面。 双斜线（//）两边。这里可以使用多个空格，但也不是必需的。 JSDoc 注释及其两边e.g. 简写的类型声明 this.foo = /** @type {number} */ (bar); 或类型转换（cast）function(/** string */ foo) {。 函数参数推荐将函数的所有参数放在与函数名同一行的位置。如果这样会导致列宽超限，那参数应该以一种易读的方式进行换行。为了节省空间，尽量超过宽度限制时才进行换行，换行后每个参数一行以提高可读性。换行后缩进为 4 个空格。与括号对齐是可以的，但不推荐。以下是觉的参数换行示例： 12345678910111213141516171819202122232425// Arguments start on a new line, indented four spaces. Preferred when the// arguments don't fit on the same line with the function name (or the keyword// "function") but fit entirely on the second line. Works with very long// function names, survives renaming without reindenting, low on space.doSomething( descriptiveArgumentOne, descriptiveArgumentTwo, descriptiveArgumentThree) &#123; // …&#125;// If the argument list is longer, wrap at 80. Uses less vertical space,// but violates the rectangle rule and is thus not recommended.doSomething(veryDescriptiveArgumentNumberOne, veryDescriptiveArgumentTwo, tableModelEventHandlerProxy, artichokeDescriptorAdapterIterator) &#123; // …&#125;// Four-space, one argument per line. Works with long function names,// survives renaming, and emphasizes each argument.doSomething( veryDescriptiveArgumentNumberOne, veryDescriptiveArgumentTwo, tableModelEventHandlerProxy, artichokeDescriptorAdapterIterator) &#123; // …&#125; 注释块状注释块状注释与被注释代码保持相同缩进。/* ... */ 和 // 都是。对于多行的 /* ... */ 注释，后续注释行以 * 开头且与上一行缩进保持一致。参数的注释紧随参数之后，用于在函数名或参数名无法完全表达其意思的情况。 注意区分 JSDoc（/** ... */），上面不注释不能写成这样。 本地变量的声明使用 const 和 let所有本地变量使用 const 或 let 来声明。默认使用 const，除非该变量需要重新赋值。杜绝使用 var。 一次声明一个变量一次只声明一个本地变量，let a = 1, b = 2; 这样的做法是禁止的。 需要时才声明，声明后尽快初始化本地变量不要全部一次性声明在代码块的开头。声明在该变量每一次需要被使用的地方，以减少其影响范围。 尽量标明类型JSDoc 类型注释可以添加在声明语句上面，或者内联到变量名前面。 1234const /** !Array&lt;number&gt; */ data = [];/** @type &#123;!Array&lt;number&gt;&#125; */const data = []; 数组字面量尾部加逗号始终在元素后面带上一个结束的逗号。 属性引号保持一致(字典键的类型一致)对象属性分为不包含引号或 Symbols 的 structs 类型和包含引号或运算属性的 dicts 类型。两种类型不要混合使用，应该保持一致。 标识符类型的命名规则包名包名使用小写开头的驼峰命名 lowerCamelCase 类名类，接口，record (这是什么？) 以及 typedef names (类型定义符) 使用大写开头的驼峰 UpperCamelCase。 未被导出的类只本地使用，并没有用 @private 标识，所以命名上不需要以下划线结尾。 类型名称通常为名词或名词短语。比如，Request，ImmutableList，或者 VisibilityMode。此外，接口名有时会是一个形容词或形容短语（比如 Readable）。 方法名方法名使用小写开头的驼峰。私有方法需以下划线结尾。 方法名一般为动词或动词短语。比如 sendMessage 或者 stop_。属性的 Getter 或 Setter 不是必需的，如果有的话，也是小写驼峰命名且需要类似这样 getFoo(对于布尔值使用 isFoo 或 hasFoo 形式)，setFoo(value)。 注：私有属性或方法以下划线开头才是主流吧 单元测试代码中的方法名会出现用下划线来分隔组件形式。一种典型的形式是这样的test&lt;MethodUnderTest&gt;_&lt;state&gt;，例如 testPop_emptyStack。对于这种测试代码中的方法，命名上没有统一的要求 枚举枚举使用大写开头的驼峰，和类相似，一般一个单数形式的名词。枚举中的元素写成 CONSTANT_CASE。 常量常量写成 CONSTANT_CASE：所有字母使用大写，以下划线分隔单词。私有静态属性可以用内部变量代替，所以不会有使用私有枚举的情况，也就无需将常量以下划线结尾来命名。 常量的定义每个常量都是 @const 标识的静态属性或 模块内部通过 const 声明的变量，但并不是所有 @const 标识的静态属性或 const 声明的变量都是常理。需要常量时，先想清楚该对象是否真的不可变。例如，如果该对象中做生意状态可被改变，显然不适合作为常量。只是想着不去改变它的值是不够的，我们要求它需要从本质上来说应该一成不变。 非常量字段非常量字段（静态或其他）使用小写开头的驼峰，如果是私有的私有的则以下划线结尾。 一般是名词或名词短语。例如 computedValues，index_。 参数参数使用小写开头的驼峰。即使参数需要一个构造器来初始化时，也是这一规则。 公有方法的参数名不能只使用一个字母。 例外：如果三方库需要，参数名可以用 $ 开头。此例外不适用于其他标识符（e.g. 本地变量或属性）。 本地变量本地变量使用小写开头的驼峰，除非是上面介绍过的本地常量。函数中的常量命名仍然使用小写开头的驼峰lowerCamelCase。即使该变量指向的是构造器也使用 lowerCamelCase。 模板参数/Template parameter names模板参数力求简洁，用一个单词，一个字母表示，全部使用大写，例如 TYPE，THIS。 驼峰：定义有时将一个英文短语转成驼峰有很多形式，例如首字母进行缩略，IPv6 以及 iOS 这种都有出现。为保证代码可控，本规范规定出如下规则。 将短语移除撇号转成 ASCII 表示。例如 Müller&#39;s algorithm 表示成 Muellers algorithm。 将上述结果拆分成单词，以空格或其他不发音符号（中横线）进行分隔。a. 推荐的做法：如果其中包含一个已经常用的驼峰翻译，直接提取出来（e.g. AdWords 会成为 ad words）。需要注意的是 iOS 本身并不是个驼峰形式，它不属性任何形式，所以它不适用本条规则。 将所有字母转成小写，然后将以下情况中的首字母大写：a. 每个单词的首字母，这样便得到了大写开头的驼峰b. 除首个单词的其他所有单词的首字母，这样得到小写开头的驼峰 将上述结果合并。 过程中原来名称中的大小写均被忽略。 示例： Prose form Correct Incorrect XML HTTP request XmlHttpRequest XMLHTTPRequest new customer ID newCustomerId newCustomerID inner stopwatch innerStopwatch innerStopWatch supports IPv6 on iOS? supportsIpv6OnIos supportsIPv6OnIOS ouTube importer YouTubeImporter YoutubeImporter* *这种情况可接受，但不推荐。 注释：一些英文词汇通过中横线连接的方式是有歧义的，比如 “nonempty” 和 “non-empty” 都是正确写法，所以方法名 checkNonempty checkNonEmpty 都算正确。 JSDocJSDoc 使用在了所有的类，字段以及方法上。 通用形式JSDoc 基本的形式如下： 123456/** * Multiple lines of JSDoc text are written here, * wrapped normally. * @param &#123;number&#125; arg A number to do something to. */function doSomething(arg) &#123; … &#125; 或者这种单行的形式： 12/** @const @private &#123;!Foo&#125; A short bit of JSDoc. */this.foo_ = foo; 如果单行形式长到需要折行，则需要切换到多行模式而不是使用单行形式。 有许多工具会对 JSDoc 文档进行解析以提取出有效的信息对代码进行检查和优化。所以这些注释需要好好写。 MarkdownJSDoc 支持 Markdown，所以必要时可包含 HTML。 工具会自动提取 JSDoc 的内容，其中自己书写的格式会被忽略。比如如果你写成下面这个样子： 123456/** * Computes weight based on three factors: * items sent * items received * last timestamp */ 最终提取出来是这样的： 1Computes weight based on three factors: items sent items received last timestamp 取而代之，我们应该按 markdown 的语法来格式化： 123456/** * Computes weight based on three factors: * - items sent * - items received * - last timestamp */ JSDoc tags本规则可使用 JSDoc tags 的一个子集。详细列表见 9.1 JSDoc tag reference。大部分 tags 独占一行。 错误的示例： 12345/** * The "param" tag must occupy its own line and may not be combined. * @param &#123;number&#125; left @param &#123;number&#125; right */function add(left, right) &#123; ... &#125; 简单的 tag 无需额外数据（比如 @private，@const，@final，@export），可以合并到一行。 123456789101112131415161718/** * Place more complex annotations (like "implements" and "template") * on their own lines. Multiple simple tags (like "export" and "final") * may be combined in one line. * @export @final * @implements &#123;Iterable&lt;TYPE&gt;&#125; * @template TYPE */class MyClass &#123; /** * @param &#123;!ObjType&#125; obj Some object. * @param &#123;number=&#125; num An optional number. */ constructor(obj, num = 42) &#123; /** @private @const &#123;!Array&lt;!ObjType|number&gt;&#125; */ this.data_ = [obj, num]; &#125;&#125; 关于合并及合并后的顺序没有明确的规范，代码中保持一致即可。 详细的类型信息可参见 Annotating JavaScript for the Closure Compiler 和 Types in the Closure Type System。 换行换行之后的 tag 块使用四个空格进行缩进。 12345678910/** * Illustrates line wrapping for long param/return descriptions. * @param &#123;string&#125; foo This is a param with a description too long to fit in * one line. * @return &#123;number&#125; This returns something that has a description too long to * fit in one line. */exports.method = function(foo) &#123; return 5;&#125;; @fileoverview 换行时不缩进。 文件头部注释一个文件可以在头部有个总览。包括版权信息，作者以及默认可选的可见信息/visibility level等。文件中包含多个类时，头部这个总览显得很有必要。它可以帮助别人快速了解该文件的内容。如果写了，则应该有一个描述字段简单介绍文件中的内容以及一些依赖，或者其他信息。换行后不缩进。 示例： 12345/** * @fileoverview Description of file, its uses and information * about its dependencies. * @package */ 类的注释类，接口以及 records 需要有描述，参数，实现的接口以及可见性或其他适当的 tags 注释。类的描述需要告诉读者类的作用及何时使用该类，以及其他一些可以帮助别人正确使用该类的有用信息。构造器上的文本描述可省略。@constructor 和 @extends 不与 class 一起使用，除非该类是用来声明接口 @interface 或者扩展一个泛型类。 123456789101112131415161718192021222324/** * A fancier event target that does cool things. * @implements &#123;Iterable&lt;string&gt;&#125; */class MyFancyTarget extends EventTarget &#123; /** * @param &#123;string&#125; arg1 An argument that makes this more interesting. * @param &#123;!Array&lt;number&gt;&#125; arg2 List of numbers to be processed. */ constructor(arg1, arg2) &#123; // ... &#125;&#125;;/** * Records are also helpful. * @extends &#123;Iterator&lt;TYPE&gt;&#125; * @record * @template TYPE */class Listable &#123; /** @return &#123;TYPE&#125; The next item in line to be returned. */ next() &#123;&#125;&#125; 枚举和 typedef 注释枚举和 typedef 需要写文档。仅有的枚举和 typedef 其文档的描述不能为空。枚举中单个元素的文档可直接写在元素的前面一行。 1234567891011121314151617/** * A useful type union, which is reused often. * @typedef &#123;!Bandersnatch|!BandersnatchType&#125; */let CoolUnionType;/** * Types of bandersnatches. * @enum &#123;string&#125; */const BandersnatchType = &#123; /** This kind is really frumious. */ FRUMIOUS: 'frumious', /** The less-frumious kind. */ MANXOME: 'manxome',&#125;; Typedefs 可方便地用于定义 records 类型，或 unions 的别名，复杂函数，或者 泛型类型。Typedefs 不适合用来定义字段很多的 records，因为其不支持对每个字段进行文档书写，也不适合用于模板或递归引用中。对于大型 records 使用 @record。 方法与函数参数和返回类型需要写文档。必要时 this 也需要在文档中说明。方法，参数及返回的描述在方法的其他 JSDoc 中或方法签名中有表述，那么也是可以省略的。方法的描述应使用第三人称。如果方法重载父类中的方法，需要使用 @override 标识。重载方法需要包含所有的参数 @param 以及 @return 如果类型有变的话，如果没变也可省略。 1234567891011121314151617181920212223/** This is a class. */class SomeClass extends SomeBaseClass &#123; /** * Operates on an instance of MyClass and returns something. * @param &#123;!MyClass&#125; obj An object that for some reason needs detailed * explanation that spans multiple lines. * @param &#123;!OtherClass&#125; obviousOtherClass * @return &#123;boolean&#125; Whether something occurred. */ someMethod(obj, obviousOtherClass) &#123; ... &#125; /** @override */ overriddenMethod(param) &#123; ... &#125;&#125;/** * Demonstrates how top-level functions follow the same rules. This one * makes an array. * @param &#123;TYPE&#125; arg * @return &#123;!Array&lt;TYPE&gt;&#125; * @template TYPE */function makeArray(arg) &#123; ... &#125; 匿名函数不需要写 JSDoc，但在自动推荐参数类型有困难时，也可手动指定一下类型。 12345promise.then( (/** !Array&lt;number|string&gt; */ items) =&gt; &#123; doSomethingWith(items); return /** @type &#123;string&#125; */ (items[0]); &#125;); 属性属性的类型需要加文档。对于私有属性如果其命名已经很好地提示了其类型，则描述可省略。 公有的常量与属性是一样的文档写法。当 @const 类型的属性从一个表达式初始化时，是能明显看出其类型的，此时其显式的类型指定可省略。 小贴士：@const 属性的类型从构造器的参数进行初始化时，其类型是“显然”的，因为参数上面声明了类型，或者从一个函数调用进行初始化，因为函数返回值的类型也是声明了的。对于非常量或者它的值来源不是很明朗的情况下才需要指定其类型。 1234567891011121314151617181920212223/** My class. */class MyClass &#123; /** @param &#123;string=&#125; someString */ constructor(someString = 'default string') &#123; /** @private @const */ this.someString_ = someString; /** @private @const &#123;!OtherType&#125; */ this.someOtherThing_ = functionThatReturnsAThing(); /** * Maximum number of things per pane. * @type &#123;number&#125; */ this.someProperty = 4; &#125;&#125;/** * The number of times we'll try before giving up. * @const */MyClass.RETRY_COUNT = 33; 警告如何处理警告解决之前，先明白警告的意思。如果不清楚，可以询问。 一旦了解之后，可进行如下操作： 首先，修复或找替代方案。 尝试针对该警告进行修复，或者换种方式实现相同的功能同时规避掉警告。 或者，检查看是否是一个误报。 如果确认代码没问题，警告是误报，写下相应注释并添加 @suppress标识。 或者，留下 TODO 注释。 这是最次的做法。这种做法相当于直接忽略警告，直到时机成熟再解决。 将警告控制在最小范围将警告控制在最小合理的作用域范围，通常一个本地变量或很小的方法范围内。然后可以将这个变量或方法单独提取出来。 示例： 12345/** @suppress &#123;uselessCode&#125; Unrecognized 'use asm' declaration */function fn() &#123; 'use asm'; return 0;&#125; 一个类中大量的 @suppress 也好过编译时报大量的警告。 已有代码的重新格式化更新已有代码时，遵循以下原则： 没必要更新所有老代码以满足本规范。需要在成本与代码一致性之间找个平衡点。规范不断在演变，花大成本更新老代码需要折衷。然而，如果老文件大部分都被修改了的话，那可以顺便将其全部改为符合现在的规范。 注意控制改动范围。如果你发现需要投入大量精力去更新代码而影响了当前需求的进展，考虑将这些老代码的更新另起一个分支。 新增代码：遵循本规范全新创建的文件应该全部遵循本规范，某些包中其他类型文件有其他规范的另说。 向一个不是遵循本规范而写的文件添加新代码时，推荐先重新格式化当前文件 如果重新格式化完不成，那么新加的代码应该与老代码尽量保持一致，但不要滥用规范。]]></content>
      <categories>
        <category>javascript</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git基础]]></title>
    <url>%2F2018%2F08%2F20%2Fgit%2Fgit%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[对于一个新建的项目，我们首先要在项目目录中执行git init来初始化版本系统，之后可以通经过git status 来查看当前状态 Unttacked files：尚未被追踪的文件 Changes to committed：已经提交到暂存区，但尚未提交到版本管理中，对未被跟踪的文件执行git add 文件名命令后的状态。 Changes not staged for commit：已经被追踪的文件发生了修改，但修改还未被添加到暂存区。 nothing to commit, working directory clean：没有文件被修改或者被追踪。 本地提交的代码可以推送到远端，但是需要先设置远端的仓库地址： git remote add origin git@github.com 代码推送到远端的master分支上 git push -u origin master 其他命令： 修改已提交的comment 我们推荐一个commit只做一件事，比如修改一个bug，完成一个小功能点，但有时会出现已经提交了commit，又发现一个小问题，而修复后作为独立的commit不合适，此时可以使用 git commit --amend 把新的改动加入到刚才的commit中 修改历史comment 有时候还会有需要变更历史的需求，比如提交了多个相似的commit，需要将其合并为一个，git rebase -i HEAD~3命令，这个命令通过交互模式（-i的作用）的方式来进行调整，最后的HEAD~3指明我们想修改的最近三次commit，执行命令后会出现下图 这里我们可以调整注释，合并两个commit（使用squash）或者舍弃某个commit（直接去掉某个commit） 需要注意的是：即使不修改任何内容，进入这个模式后，也会重写这几个commit（重新生成commit号）。我们可以通过删除所有的非注释内容来正常退出。 ​ 这种操作不可在共用的分支上做，尤其是对于已经推送到远端的commit。因为变更完成后只有通过git push -f 才能重新提交。如果是公用分支，会导致其他开发人员“崩溃”（代码冲突）。 git blame审查代码 其实大部分IDE都已经内置了这个功能。 通过git blame &lt;文件名&gt;方式来查看每一行代码是由哪个开发人员编写的。 远端和本地 通过git remote add origin &lt;远端仓库地址&gt;来增加远端仓库，其中，origin是远端仓库默认的名称 然后通过git push -u origin master来推送本地master分支的代码到远端，并同时设置本地master关联远端的master分支。 git fetch 本地库的master的commitID不变，但是与git关联的仓库中的commitID变成了2，这时候我们本地相当于存储了两个代码的版本号，这时候我们要通过git merge去合并 git pull 将本地的代码更新至远程仓库里面最新的代码版本 同时提交到多个远程仓库 首先通过git remote -v 查看远程仓库信息 git remote set-url --add origin + url 添加仓库信息 git remote -v就可以看到这个链接了 提交，git push origin --all]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB创建用户]]></title>
    <url>%2F2018%2F03%2F15%2Fdb%2Fmongodb%2F04%E3%80%81%E5%88%9B%E5%BB%BA%E7%94%A8%E6%88%B7%2F</url>
    <content type="text"><![CDATA[创建用户 使用哪个库，创建的用户就属于哪个库,当然，这个仅限于当前库的role1234567use admin- db.createUser(&#123;user: &apos;admin&apos;, pwd: &apos;Button2015&apos;, roles: [&#123;role:&apos;read&apos;, db: &apos;admin&apos;&#125;]&#125;)use data- db.createUser(&#123;user: &apos;read&apos;, pwd: &apos;Button2015&apos;, roles: [&#123;role:&apos;read&apos;, db: &apos;data&apos;&#125;]&#125;) role 数据库用户角色：read、readWrite; 数据库管理角色：dbAdmin、dbOwner、userAdmin； 集群管理角色：clusterAdmin、clusterManager、clusterMonitor、hostManager； 备份恢复角色：backup、restore； 所有数据库角色：readAnyDatabase、readWriteAnyDatabase、userAdminAnyDatabase、dbAdminAnyDatabase 超级用户角色：root// 这里还有几个角色间接或直接提供了系统超级用户的访问（dbOwner 、userAdmin、userAdminAnyDatabase） 内部角色：__system 删除用户 1db.dropUser(&apos;username&apos;) 完成后需要重启数据库，并且加上 —auth参数，否则不用密码也能进入]]></content>
      <categories>
        <category>数据库</category>
        <category>mongoDB</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>数据库</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB集群]]></title>
    <url>%2F2018%2F03%2F15%2Fdb%2Fmongodb%2F03%E3%80%81mongodb%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[目的：使用三个docker虚拟容器创建mongoDB集群1、拉取镜像可配置阿里云的镜像加速器12345678&gt; sudo mkdir -p /etc/docker&gt; sudo tee /etc/docker/daemon.json &lt;&lt;-&apos;EOF&apos;&#123; &quot;registry-mirrors&quot;: [&quot;https://test.mirror.aliyuncs.com&quot;]&#125;EOF&gt; sudo systemctl daemon-reload&gt; sudo systemctl restart docker 2、创建docker-compose文件注意，docker-compose的文件名需要是docker-compose.yml 这里采用的mongo4.0的镜像，每个容器最大内存限制为4G 12345678910111213141516171819202122232425262728293031323334353637383940414243version: &apos;2&apos;services: db0: image: mongo:4.0 restart: always mem_limit: 4G volumes: - /workspace/mongo-alpha/db0:/data/db - /workspace/mongo-alpha/common:/data/common environment: TZ: Asia/Shanghai ports: - &quot;27018:27017&quot; command: mongod --replSet rs0 links: - db1 - db2 db1: image: mongo:4.0 restart: always mem_limit: 4G volumes: - /workspace/mongo-alpha/db1:/data/db - /workspace/mongo-alpha/common:/data/common environment: TZ: Asia/Shanghai ports: - &quot;27019:27017&quot; command: mongod --replSet rs0 db2: image: mongo:4.0 restart: always mem_limit: 4G volumes: - /workspace/mongo-alpha/db2:/data/db - /workspace/mongo-alpha/common:/data/common environment: TZ: Asia/Shanghai ports: - &quot;27020:27017&quot; command: mongod --replSet rs0 3、创建容器在docker-compost.yml的相同路径下，执行docker-compose up -d，这样使用docker ps就可以看到有三个容器启动，选择一个容器进入docker exec -it containerid bash 执行mongo 123456789101112131415161718192021&gt; use admin&gt; config = &#123; &quot;_id&quot; : &quot;rs0&quot;, &quot;members&quot; : [ &#123; &quot;_id&quot; : 0, &quot;host&quot; : &quot;192.168.1.44:27018&quot;, &quot;priority&quot;:2 # 为权重 &#125;, &#123; &quot;_id&quot; : 1, &quot;host&quot; : &quot;192.168.1.44:27019&quot; &#125;, &#123; &quot;_id&quot; : 2, &quot;host&quot; : &quot;192.168.1.44:27020&quot; &#125; ] &#125; &gt; rs.initiate(config) 此时可以使用rs.status()查看集群信息，rs.isMaster()查看当前节点是否是主节点 创建用户 要操作哪个库，就在哪个库下创建用户 123use admin- db.createUser(&#123;user: &apos;user&apos;, pwd: &apos;pwd&apos;, roles: [&#123;role:&apos;root&apos;, db: &apos;admin&apos;&#125;]&#125;) role 数据库用户角色：read、readWrite; 数据库管理角色：dbAdmin、dbOwner、userAdmin； 集群管理角色：clusterAdmin、clusterManager、clusterMonitor、hostManager； 备份恢复角色：backup、restore； 所有数据库角色：readAnyDatabase、readWriteAnyDatabase、userAdminAnyDatabase、dbAdminAnyDatabase 超级用户角色：root// 这里还有几个角色间接或直接提供了系统超级用户的访问（dbOwner 、userAdmin、userAdminAnyDatabase） 内部角色：__system 删除用户db.dropUser(&#39;username&#39;) 查看用户show users 12345678910read：允许用户读取指定数据库readWrite：允许用户读写指定数据库dbAdmin：允许用户在指定数据库中执行管理函数，如索引创建、删除，查看统计或访问system.profileuserAdmin：允许用户向system.users集合写入，可以找指定数据库里创建、删除和管理用户clusterAdmin：只在admin数据库中可用，赋予用户所有分片和复制集相关函数的管理权限。readAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的读权限readWriteAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的读写权限userAdminAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的userAdmin权限dbAdminAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的dbAdmin权限。root：只在admin数据库中可用。超级账号，超级权限 openssl rand -base64 725 &gt; /data/common/mongodb-keyfile生成密钥，长度大小要求在6-1024之间 配置权限，更改所有者和所属组 chmod 600 mongodb-keyfile chown 999:999 mongodb-keyfile 注：docker容器的所属组为999 4、配置登录重新配置docker-compose.yml 12345678910111213141516171819202122232425262728293031323334353637383940414243version: &apos;2&apos;services: db0: image: mongo:4.0 restart: always mem_limit: 4G volumes: - /workspace/mongo-alpha/db0:/data/db - /workspace/mongo-alpha/common:/data/common environment: TZ: Asia/Shanghai ports: - &quot;27018:27017&quot; command: mongod --replSet rs0 --auth --keyFile /data/common/mongodb-keyfile links: - db1 - db2 db1: image: mongo:4.0 restart: always mem_limit: 4G volumes: - /workspace/mongo-alpha/db1:/data/db - /workspace/mongo-alpha/common:/data/common environment: TZ: Asia/Shanghai ports: - &quot;27019:27017&quot; command: mongod --replSet rs0 --auth --keyFile /data/common/mongodb-keyfile db2: image: mongo:4.0 restart: always mem_limit: 4G volumes: - /workspace/mongo-alpha/db2:/data/db - /workspace/mongo-alpha/common:/data/common environment: TZ: Asia/Shanghai ports: - &quot;27020:27017&quot; command: mongod --replSet rs0 --auth --keyFile /data/common/mongodb-keyfile 不加--auth ，数据库就可以不通过密码登录，当忘记密码时，可以使用。 此时要先将容器停止，docker-compose stop，再使用docker-compose up 新建并启动镜像 5、连接mongodb://用户名:密码@集群1，集群2，集群3/要操作的数据库?authSource=用户具有的权限库&amp;replicaSet=分片名 mongodb://user:pwd@192.168.1.44:27018,192.168.1.44:27019,192.168.1.44:27020/data?authSource=admin&amp;replicaSet=rs0 这是在admin库下创建的一个root权限的用户，authsource为admin，要操作data表，分片名为rs0 附：数据备份与还原 备份 mongodump -h $DB_HOST -u $DB_USER -p $DB_PASS --authenticationDatabase admin -d $DB_NAME -o $OP_DIR --forceTableScan mongodump -h dbhost -d dbname -o dbdirectory-h：MongDB所在服务器地址，例如：127.0.0.1，当然也可以指定端口号：127.0.0.1:27017-d：需要备份的数据库实例，例如：test-o：备份的数据存放位置，例如：/root/data/dump，当然该目录需要提前建立，在备份完成后，系统自动在dump目录下建立一个test目录，这个目录里面存放该数据库实例的备份数据。 -u 用户名 -p 密码 —authenticationDatabase 登录的用户验证的数据库 —forceTableScan 不使用任何索引的情况下扫描数据 还原 mongorestore -u xx-p xx --authenticationDatabase admin -d data . mongorestore 默认是追加， 可以加—drop清空后恢复 附： --batchSize=num 可以选择每次操作的文档数量，来限制mongo操作时使用的内存大小 --gzip 可选的。适用于mongodump &gt; = 3.2，启用备份文件的内联压缩。 --oplog 在备份过程中，保留oplog，可以在恢复数据的时候进行操作重现，保证在备份过程中新的操作也可以被保留 任何replset成员都需要！此参数使“mongodump”在备份过程中捕获oplog更改日志，以保持一致的时间点。 恢复：mongorestore -u xx-p xx --authenticationDatabase admin . --oplogReplay 它的实际作用是在导出的同时生成一个oplog.bson文件，存放在你开始进行dump到dump结束之间所有的oplog。用图形来说明下oplog.bson的覆盖范围： 注意：mongodump时，—oplog只能在master节点（主从结构）或者副本集的成员中执行。也就是说，mongodump —oplog不能在主从结构的slave上执行]]></content>
      <categories>
        <category>数据库</category>
        <category>mongoDB</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>服务器</tag>
        <tag>数据库</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB使用固定集合]]></title>
    <url>%2F2018%2F01%2F26%2Fdb%2Fmongodb%2F02%E3%80%81%E4%BD%BF%E7%94%A8%E5%9B%BA%E5%AE%9A%E9%9B%86%E5%90%88%2F</url>
    <content type="text"><![CDATA[MongoDB中的固定集合：大小是固定的，类似于循环队列，如果没有空间了，最老的文档会被删除以释放空间，新插入的会占据这块空间。1、固定集合（oplog）oplog是一个典型的固定集合，因为其大小的显示，可以用来短暂的恢复部分数据。需要使用显示的创建方式db.createCollection(&#39;my_collection&#39;, {&quot;capped&quot;:true, &quot;size&quot;: 100000})此命令创建一个名为my_collection，大小为100000字节的固定集合。 此外还可以定义集合中的文档数量db.createCollection(&quot;my_collection2&quot;, {&quot;capped&quot;: true, “size”:100000, &quot;max&quot;: 100}); 固定集合创建之后就不能改变，如果需要修改固定集合的属性，只能将它删除之后再重建 1为固定集合指定文档数量限制时，必须同时指定固定集合的大小，不管先达到哪一个限制，之后插入的新文档就会把最老的文档挤出集合：固定集合的文档数量不能超过文档数量限制，固定集合的大小也不能超过大小限制。 创建固定集合时，还有另一个选项，可以将已有的某个常规集合转换为固定集合，可以使用convertToCapped命令实现。 db.runCommand({&#39;convertToCapped&#39;: &#39;test&#39;, &#39;size&#39;: 10000}); 2、自然排序返回的结果的顺序就是文档在磁盘上的顺序。使用{&#39;$natural&#39;: 1}进行排序 3、循环游标当循环游标的结果集被取光后，游标不会被关闭。 当有新文档插入到集合中时，循环游标就会继续取到结果。 通常用于当文档被插入到工作队列（固定集合）时对新插入的文档进行处理，如果超过10分钟没有新的结果，循环游标就会被释放，因此，当游标被关闭时自动重新执行查询是非常重要的。 4、没有_id索引的集合在调用createCollection创建集合时，指定autoIndexId选项为false。_id索引必须是唯一索引，不同于其他索引，_id索引一旦创建就无法删除，如果创建的_id索引不合规范，就只能删除集合再重新创建了。]]></content>
      <categories>
        <category>数据库</category>
        <category>mongoDB</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>数据库</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB 索引]]></title>
    <url>%2F2018%2F01%2F25%2Fdb%2Fmongodb%2F01%E3%80%81mongo%E5%BB%BA%E7%AB%8B%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[1、MongoDB 索引数据库前台建立索引要比后台建立索引耗时少，但在建立索引期间会锁定数据库，从而导致其他操作无法进行数据读写。db.getCollection(&#39;entity&#39;).ensureIndex({&#39;nick&#39;: 1}, {&#39;background&#39;: true}) 在后台建立索引期间会定期释放写锁，从而保证其他操作的运行。这意味着后台建立索引耗时更长，尤其是在频繁进行写入的服务器上，但是后台服务器在建立索引期间，可以继续为其他客户端提供服务 索引通常能够极大的提高查询的效率，如果没有索引，MongoDB在读取数据时必须扫描集合中的每个文件并选取那些符合查询条件的记录。 这种扫描全集合的查询效率是非常低的，特别在处理大量的数据时，查询可以要花费几十秒甚至几分钟，这对网站的性能是非常致命的。 索引是特殊的数据结构，索引存储在一个易于遍历读取的数据集合中，索引是对数据库表中一列或多列的值进行排序的一种结构 createIndex() 方法MongoDB使用 createIndex() 方法来创建索引。 注意在 3.0.0 版本前创建索引方法为 db.collection.ensureIndex()，之后的版本使用了 db.collection.createIndex() 方法，ensureIndex() 还能用，但只是 createIndex() 的别名。 语法createIndex()方法基本语法格式如下所示： 1&gt;db.collection.createIndex(keys, options) 语法中 Key 值为你要创建的索引字段，1 为指定按升序创建索引，如果你想按降序来创建索引指定为 -1 即可。 实例12&gt;db.col.createIndex(&#123;&quot;title&quot;:1&#125;)&gt; createIndex() 方法中你也可以设置使用多个字段创建索引（关系型数据库中称作复合索引）。 12&gt;db.col.createIndex(&#123;&quot;title&quot;:1,&quot;description&quot;:-1&#125;)&gt; createIndex() 接收可选参数，可选参数列表如下： Parameter Type Description background Boolean 建索引过程会阻塞其它数据库操作，background可指定以后台方式创建索引，即增加 “background” 可选参数。 “background” 默认值为false。 unique Boolean 建立的索引是否唯一。指定为true创建唯一索引。默认值为false. name string 索引的名称。如果未指定，MongoDB的通过连接索引的字段名和排序顺序生成一个索引名称。 dropDups Boolean 3.0+版本已废弃。在建立唯一索引时是否删除重复记录,指定 true 创建唯一索引。默认值为 false. sparse Boolean 对文档中不存在的字段数据不启用索引；这个参数需要特别注意，如果设置为true的话，在索引字段中不会查询出不包含对应字段的文档.。默认值为 false. expireAfterSeconds integer 指定一个以秒为单位的数值，完成 TTL设定，设定集合的生存时间。 v index version 索引的版本号。默认的索引版本取决于mongod创建索引时运行的版本。 weights document 索引权重值，数值在 1 到 99,999 之间，表示该索引相对于其他索引字段的得分权重。 default_language string 对于文本索引，该参数决定了停用词及词干和词器的规则的列表。 默认为英语 language_override string 对于文本索引，该参数指定了包含在文档中的字段名，语言覆盖默认的language，默认值为 language. 实例在后台创建索引： 1db.values.createIndex(&#123;open: 1, close: 1&#125;, &#123;background: true&#125;) 通过在创建索引时加 background:true 的选项，让创建工作在后台执行 2、MongoDB 索引限制 额外开销每个索引占据一定的存储空间，在进行插入，更新和删除操作时也需要对索引进行操作。所以，如果你很少对集合进行读取操作，建议不使用索引。 内存(RAM)使用由于索引是存储在内存(RAM)中,你应该确保该索引的大小不超过内存的限制。 如果索引的大小大于内存的限制，MongoDB会删除一些索引，这将导致性能下降。 查询限制索引不能被以下的查询使用： 正则表达式及非操作符，如 $nin, $not, 等。 算术运算符，如 $mod, 等。 $where 子句 所以，检测你的语句是否使用索引是一个好的习惯，可以用explain来查看。 索引键限制从2.6版本开始，如果现有的索引字段的值超过索引键的限制，MongoDB中不会创建索引。 插入文档超过索引键限制如果文档的索引字段值超过了索引键的限制，MongoDB不会将任何文档转换成索引的集合。与mongorestore和mongoimport工具类似。 最大范围 集合中索引不能超过64个 索引名的长度不能超过128个字符 一个复合索引最多可以有31个字段 3、pymongo 索引相关操作 pymongo.collection.Collection.create_index (Python method, in collection – Collection level operations) pymongo.collection.Collection.create_indexes (Python method, in collection – Collection level operations) pymongo.collection.Collection.drop_index (Python method, in collection – Collection level operations) pymongo.collection.Collection.drop_indexes (Python method, in collection – Collection level operations) pymongo.collection.Collection.ensure_index (Python method, in collection – Collection level operations) pymongo.collection.Collection.index_information (Python method, in collection – Collection level operations) pymongo.collection.Collection.list_indexes (Python method, in collection – Collection level operations) pymongo.collection.Collection.reindex (Python method, in collection – Collection level operations) 附mongo-express相关配置 db.currentOp()查看当前正在进行的操作]]></content>
      <categories>
        <category>数据库</category>
        <category>mongoDB</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>数据库</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python虚拟环境]]></title>
    <url>%2F2017%2F10%2F10%2Fpython%2Fbase%2Fpython%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[​ 在实际开发过程中，我们可能需要维护多个项目，这些项目的依赖又不同，如果不同的依赖都直接安装在系统环境中，那么可能会出现冲突。因此，就有了这个方案——虚拟环境。 Python3.3之后版本自带venv模块 第三方工具virtualenv 第三方工具pipenv 1. Python3.3之后版本自带venv模块 创建虚拟环境 python3.6 -m venv project-env 进入虚拟环境目录 cd project-env 激活虚拟环境 source bin/acrivite 安装环境 pip install package 退出虚拟环境 deactivate 2. virtualenv用法 安装到系统目录（需要root权限） pip install virtualenv 安装到用户目录 pip install virtualenv --user 接下来就可以直接使用了 virtualenv project-env 其余的和venv相同 virtualenv的增强包如果要使用virtualenv的增强包的话 pip install virtualenvwrapper 需要修改home目录(macOS下是/Users/用户名/)下的.bashrc文件。如果是oh-my-zsh用户，则需要修改.zshrc，最后一行添加如下内容 12export WORKON_HOME=$HOME/.virtualenvssource /usr/local/bin/virtualenwrapper.sh 如果使用--user的方式安装的话，应该是哟经下面的方式启动virtualenvwrapper： source $HOME/.local/bin/virtualenvwrapper.sh 配置好之后，通过mkvirtualenv project-env来创建环境，使用workon project-env 来激活虚拟环境]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
</search>
